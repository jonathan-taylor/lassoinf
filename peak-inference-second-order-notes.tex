\documentclass{article}

\input{../preamble.sty}
\usepackage{lmodern}

\newcommand{\ag}[1]{{\bf{{\red{[{AG: #1}]}}}}}
\newcommand{\InnerProduct}[2]{\langle #1,#2 \rangle}
\newcommand{\Norm}[1]{\|#1\|}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\br}{{\bf r}}
\newcommand{\bs}{{\bf s}}
\newcommand{\bt}{{\boldsymbol t}}
\newcommand{\bH}{{\boldsymbol H}}
\newcommand{\Newton}{\mathrm{Newton}}
\newcommand{\DummyField}{{\tt f}}
\newcommand{\DummyGradient}{{\bf g}}
\newcommand{\InformationMatrix}{{\boldsymbol J}}
\newcommand{\lin}{\mathrm{lin}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\newcommand{\Exp}{\mathrm{Exp}}

\newcommand{\RandomField}{Y}
\newcommand{\bLambda}{{\boldsymbol \Lambda}}
\newcommand{\bGamma}{{\boldsymbol \Gamma}}
\newcommand{\Err}{\mathrm{Err}}
\newcommand{\bQ}{{\boldsymbol Q}}
\newcommand{\bJ}{{\boldsymbol J}}
\newcommand{\bV}{{\boldsymbol V}}
\newcommand{\bI}{{\boldsymbol I}}
\newcommand{\bC}{{\boldsymbol C}}
\newcommand{\convweak}{\overset{d}{\to}}

\newcommand{\appropto}{\mathrel{\vcenter{
			\offinterlineskip\halign{\hfil$##$\cr
				\propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

\renewcommand{\thealgorithm}{\arabic{algorithm}}

\title{ {\bf Inference for Local Maxima of a Gaussian Random Field} \\ Second-order calculations}

\begin{document}
	
	\maketitle
	\RaggedRight
	
	Consider a generic signal-plus-noise random field
	\begin{equation}
		\label{eqn:signal-plus-noise}
		Y_t = \mu_t + \epsilon_t, \quad {\rm for} ~ t \in \mc{T} \subseteq \R^d,
	\end{equation}
	where $\epsilon \sim N(0,C)$ is a centered Gaussian process with constant variance: $C(t,t) = 1$ for all $t \in \mc{T}$. Throughout, we will treat the covariance function $C$ as known. We are interested in conducting inference for the peaks of $\mu$,
	\begin{equation}
		\label{eqn:true-peaks}
		T^* = \{t: \nabla \mu_t = 0, \nabla^2 \mu_t < 0\}.
	\end{equation}
	It seems natural to base inference for $T^*$ on the observed peaks of $Y$, call them
	\begin{equation}
		\label{eqn:observed-peaks}
		\wh{T} = \{t: \nabla Y_t = 0, \nabla^2 Y_t \preceq 0\}.
	\end{equation}
	However, depending on SNR type considerations, there may be many spurious local maxima in $\wh{T}$, i.e. peaks that fall in null regions of $\mc{T}$. (This will be defined more precisely later on.) In such cases it may be useful to screen out some of the points in $\wh{T}$, for instance by restricting attention to only those peaks which surpass a threshold $u$:
	\begin{equation}
		\label{eqn:observed-peaks}
		\wh{T}_u = \wh{T} \cap \{t: Y_t \geq u\}.
	\end{equation}
	In a high-curvature asymptotic regime, we have carried out first-order calculations which show that for sufficiently large $u$ ``most'' observed peaks $\wh{T}_u$ are not spurious. (Again, to be made more precise later on.) Moreover, for consistent $\hat{t} \in \wh{T}_u$, i.e $\hat{t} \overset{P}{\to} t^*$ for some $t^{*} \in T^*$, $\hat{t}$ has asymptotic distribution
	$G_{t^*}(y)^{1/2}(\hat{t} - t^*) \overset{d}{\to} N(0, I)$, where
	\begin{equation}
		\label{eqn:goldilocks}
		G_{t}(y) := -\nabla^2 \mu_{t} \Lambda_t^{-1} H_t(y)
	\end{equation}
	is the Goldilocks form of the variance. We have also shown that $Y_{\hat{t}}$ converges to a truncated Gaussian distribution with mean parameter $\mu_{t^*}$.
	
	These notes carry out second-order calculations. These flesh out the conditions under which the Goldilocks form of the variance is more accurate than the usual sandwich form, and the extent of the improvement. They also considers inference for the height parameter $\mu_{t^*}$, and shows that it is possible to obtain more precise inferences than simply using a truncated Gaussian distribution with mean parameter $\mu_{t^*}$. Finally, they extend these second-order calculations to an inferential procedure that uses randomization in order to increase power and ease construction of pivotal quantities.
	
	\subsection{Technical Roadmap}
	Our analysis will start by invoking the Kac-Rice formula -- which computes the expected number of critical points satisfying appropriate side conditions -- to compute an exact expression for the intensity function $\rho(y,t)$ of a point process the counts the number of critical points located at $t$ of height $y$. Taylor expansion of this intensity function yields an accurate approximation under our high-curvature asymptotics. Finally, we show that with high probability the relevant number of such critical points is either $0$ or $1$; thus our approximation to the intensity function (suitably normalized) in fact yields an approximate joint density for $(\hat{t},Y_{\hat{t}})$.
	
	Section~? carries out a parallel analysis in the randomized setting.
	
	\ag{Statements that are not yet fully justified will be marked in red.}
	
	\section{Preliminaries}
	We begin by reviewing the Kac-Rice theorem for the expected number of critical point of a random field subject to side condition, then introduce some notation and formalize our assumptions.
	
	\subsection{Kac-Rice}
	\ag{TO COME}
	
	\subsection{Asymptotic setup and notation}
	
	\paragraph{Asymptotic setup.}
	Throughout, we will assume our data are a sequence of curves $Y_n = \mu_n + \epsilon_n$, each observed from the signal plus noise model~\eqref{eqn:signal-plus-noise}. Both the signal $\mu_n$ and the threshold $u_n$ may vary with $n$; however, the noise covariance $C$, parameter space $\mc{T}$, and dimension $d$ will remain fixed in $n$. Obviously the intensity function $\rho_n$ will also depend on $n$, and we will derive an approximation to $\rho_n$ that is increasingly accurate as $n \to \infty$. To state the accuracy of approximation we will use formal $O(\cdot)$ notation: for sequences $(a_n)$ and $(b_n)$, we write $a_n = O(b_n)$ if $\limsup a_n/b_n < \infty$. We will let $c_0,c_1,\ldots$ (and $C_0,C_1,\ldots$) denote small (and large)  constants that do not depend on $n$, and $c$ (and $C$) denote small (and large) constants that do not depend on $n$ and may change from line to line.
	
	The index $n$ is simply a bit of syntactic sugar to make our asymptotic statements easier to state and parse. The only real asymptotic condition necessary is that the curvature grow, as formalized in Assumption~\ref{asmp:curvature-asymptotics} later. As such, when it is convenient to do so we will feel free to suppress subscripts in $n$, writing $Y \equiv Y_n, \mu \equiv \mu_n$ and so on. Correspondingly, when we write $a = O(b)$ it should be understood that $a \equiv a_n, b \equiv b_n$ are sequences in $n$.
	
	\paragraph{Covariance and array notation.}
	We introduce the following notation for the covariance of derivatives of $Y$:
	\begin{align*}
		C_{10}(t,t') & := \Cov[\nabla Y_t, Y_{t'}] \in \Rd, \quad  C_{01}(t,t') := \Cov[Y_{t'},\nabla Y_t] \in \R^{d}\\
		C_{11}(t,t') & := \Cov[\nabla Y_t,\nabla Y_{t'}] \in \R^{d \times d}, \quad C_{02}(t,t') := \Cov[Y_t,\nabla^2 Y_{t'}], C_{20}(t,t') := \Cov[\nabla^2 Y_t,Y_{t'}] \\
		C_{21}(t,t') & := \Cov[\nabla^2 Y_t, \nabla Y_{t'}] \in \R^{d \times d \times d}, \quad C_{12}(t,t') :=  \Cov[\nabla Y_t, \nabla^2 Y_{t'}] \in \R^{d \times d \times d},\\
		C_{22}(t,t') & := \Cov[\nabla^2 Y_t, \nabla^2 Y_t'] \in \R^{d \times d \times d \times d}.
	\end{align*}
	The elements of $C_{21}$ (and $C_{12}$) and $C_{22}$ are arranged in the predictable way: 
	$$
	(C_{21}(t,t'))_{ijk} = \Cov\Big[\frac{d^2}{dt_i dt_j}Y_t, \frac{d}{dt_k}Y_t\Big], \quad C_{22}(t,t')_{ijkl} = \Cov\Big[\frac{d^2}{dt_i dt_j}Y_t, \frac{d^2}{dt_k dt_l}Y_t\Big]
	$$
	We will denote $\Lambda_t := C_{11}(t,t)$. By the constant variance assumption (i) $C_{10}(t,t) = 0$, implying $Y_t$ and $\nabla Y_t$ are independent, and (ii) $\Lambda_t = -C_{20}(t)$. For any array $A \in \R^{d \times d \times d}$ and vector $x \in \Rd$, we denote $A(x) \in \R^{d \times d}$ and $A(x^2) \in \Rd$ to be the matrix and vector (respectively) with entries
	\begin{equation}
		\label{eqn:array-notaiton}
		(A(x))_{ij} := \sum_{k = 1}^{d} A_{ijk} v_k, \quad (A(x^2))_{i} = \sum_{j,k = 1}^{d} A_{ijk} v_j v_k.
	\end{equation}
	Finally, we will denote $\Gamma_t(x) = C_{21}(t,t)(\Lambda_t^{-1}x)$.
	
	\subsection{Assumptions}
	We separate our assumptions into two categories: assumptions placed on the noise $\epsilon$ and assumptions placed on the signal $\mu$.
	
	\paragraph{Assumptions on the noise.}
	Throughout we make the following assumptions on the covariance function of the noise. 
	\begin{assumption}
		\label{asmp:noise-stationary-c2}
		The process $Y$ has constant variance: $C(t,t) = 1$. The covariance kernel satisfies $C(t,\cdot) \in C^{4 + \alpha}(\Rd)$ for every $t \in \mc{T}$. 
	\end{assumption}
	\begin{assumption}
		\label{asmp:noise-non-degenerate}
		The matrix $\Var[\nabla Y_t] := \Lambda_t$ is uniformly lower bounded over $t \in \mc{T}$: there exists a constant $c > 0$ such that $\inf_{t \in \mc{T}} \lambda_{\min}(\Lambda_t) \geq c$ for all $n \in \mathbb{N}$.
	\end{assumption}
	Assumption~\ref{asmp:noise-stationary-c2} implies that $\epsilon$ is almost surely twice-differentiable \ag{Need to check this}, while Assumption~\ref{asmp:noise-non-degenerate} rules out degeneracies in the distribution of $\nabla \epsilon_t$. 
	
	\paragraph{Assumptions on the signal.}
	Next, we place some assumptions on the signal. Our first assumption is that the signal is smooth around both true peaks and null regions. We have already defined true peaks $T^{\ast}$ in~\eqref{eqn:true-peaks}. We define the null region $T_0 := \{t: \exists \varepsilon_0 > 0: \mu_{t'} = 0 \; \forall s \in B(t,\varepsilon_0)\}$. Let $T_0^{\ast} = T^{\ast} \cup T_0$.
	\begin{assumption}
		\label{asmp:signal-holder}
		There exists $\delta_0 > 0$ such that the mean $\mu \in C^{4}(B(T_0^{\ast},\delta_0))$ for all $n \in \mathbb{N}$.
	\end{assumption}
	Our second assumption rules out signal peaks lying too near the boundary of the parameter space $\mc{T}$. For boundary points, the formula for the intensity function given by the Kac-Rice theorem  will be different, and as written, our asymptotics will not apply.
	
	\begin{assumption}
		\label{asmp:signal-boundary}
		There exists $\delta_1 > 0$ such that ${\rm dist}(T^{\ast},\partial \mc{T}) > \delta_1$.
	\end{assumption}
	
	\ag{Probably could let $\delta_0,\delta_1 \to 0$ at some rate, but for now, this seems fine. We will handle the case where Assumption~\ref{asmp:signal-boundary} is violated separately.}
	
	\subsection{High-curvature asymptotics}
	We will work under the following high-curvature asymptotics. Recall the definitions of $\Lambda_t$ and $\Gamma_t$ from above. For $v \in \R$, define
	\begin{equation}
		\label{eqn:deterministic-equivalent-hessian}
		H_{t}(v) := -\nabla^2{\mu}_{t} + (v - \mu_{t})\Lambda_t + \Gamma_t(\nabla \mu_t),
	\end{equation}
	In words $H_t(v)$ is simply the expectation of the negative Hessian $-\nabla^2 Y_t$ conditional on $Y_t = v$ and $\nabla Y_t = 0$. Later, we will show later that at peaks $\hat{t} \in \wh{T}_u$ for which $\hat{t} \overset{P}{\to} t^* \in T^*$, the observed negative Hessian $-\nabla^2 Y_{\hat{t}}$ concentrates (after appropriate rescaling) around $H_{t^*}(u_{t^*}^{+})$ where $u_{t^*}^{+} := \max\{\mu_{t^*},u\}$. Thus the following assumption, which asserts that the minimum eigenvalue of $H_{t^*}(u_{t^*}^{+})$ is growing, corresponds to placing a high-curvature assumption on $Y$.
	\begin{assumption}
		\label{asmp:curvature-asymptotics}
		Define $\delta_{t}(v) := \{\lambda_{\min}(H_{t}(v))\}^{-1}$. Assume
		\begin{equation}
			\label{eqn:high-curvature-asymptotics}
			\inf_{t^* \in T^*} \delta_{t^*}(u_{t^*}^{+}) \to 0, \quad \sup_{t^* \in T^*} \sup_{t \in B(t^*,\delta_0)}\|\mu_{t}^{(k)}\| \cdot \delta_{t^*}(u_{t^*}^{+}) = O(1) \;{\rm for}\; k = 3,4.
		\end{equation}
	\end{assumption}

	\section{Asymptotic distribution after selection}
	\label{sec:asymptotic-distribution}
	
	We begin with inference without randomization, based on the distributional limit of points $\hat{t} \in \wh{T}_u$.
	
	\subsection{Kac-Rice}
	Consider the following point process associated to $Y_t$: for a scalar $v \in \R$ and a given $A \subseteq \mc{T}$,
	$$
	N_{v}(A) := N\{t \in A: \nabla Y_t = 0, \nabla^2 Y_t \preceq 0, Y_t \geq v\}.
	$$
	Under \ag{appropriate smoothness conditions}, the Kac-Rice theorem gives the following representation for the expectation of this counting process:
	\begin{equation}
		\label{eqn:kac-rice}
		\E[N_v(A)] = \int_{v}^{\infty} \int_{A} \rho(y,t) \,dt \,dy, \quad \rho(y,t) := \E[\det(-\nabla^2 Y_t) \cdot \1(\nabla^2 Y_t \preceq 0)|Y_t = y,\nabla Y_t = 0] \cdot p_{Y_t,\nabla Y_t}(y,0).
	\end{equation}
	Here $p_{Y_t,\nabla Y_t}(y,0)$ is the Gaussian density of $(Y_{t},\nabla Y_t)$ evaluated at $(y,0)$. Recall that the constant variance assumption implies $\Cov[Y_t,\nabla Y_t] = 0$, and thus $Y_t$ and $\nabla Y_t$ are independent, so $p_{Y_t,\nabla Y_t}(y,0) = p_{Y_t}(y) \cdot p_{\nabla Y_t}(0)$. 
	
	\subsection{Asymptotic expansion of intensity}
	We'll start by computing an approximation to the intensity function $\rho$ defined in~\eqref{eqn:kac-rice}, that is highly accurate under the high-curvature asymptotics of Assumption~\ref{asmp:curvature-asymptotics} at points $t = t^{\ast} + h$ such that $h = o(1)$. We will use this to develop approximations to the joint distribution of $(\hat{t},Y_{\hat{t}})$, the marginal distribution of $Y_{\hat{t}}$, and the conditional distribution of $\hat{t}$ given $Y_{\hat{t}}$. 
	
	\paragraph{Determinant term.}
	The expectation of the determinant of a Gaussian random matrix, multiplied by the indicator that the matrix is positive semi-definite, is in general difficult to compute. However, in our high-curvature asymptotic setup, the probability that $\nabla^2 Y_t$ is negative definite is exponentially close to $1$. We are thus left with the conditional expectation of a determinant of a Gaussian matrix which can be effectively approximated near peaks $t^*$, as detailed by the following Lemma.
	\begin{lemma}
		\label{lem:approximation-peak-intensity}
		Under Assumptions~\ref{asmp:noise-stationary-c2}-\ref{asmp:curvature-asymptotics}, at points $t,y$ such that $t - t^* := h = o(1)$ and $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$,
		\begin{equation}
			\label{eqn:approximate-intensity-1}
			\E[\det(-\nabla^2 Y_t) \cdot \1(\nabla^2 Y_t \preceq 0)|Y_t = y,\nabla Y_t = 0] =  \Big(1 + O\big(h^2 \vee \{\delta_{t^*}(y)\}^2 \vee \exp(-c_1 \{\delta_{t^*}(y)\}^{-2})\big)\Big)\det(H_{t^*}(y))\cdot\big(1 + d_{t^*}(y,h)\big),
		\end{equation}
		where $d_{t^*}(y,h) := \tr\big(\{H_{t^*}(y)\}^{-1} \dot{H}_{t^*}(y)(h)\big)$ and
		\begin{equation}
			\label{eqn:hessian-deterministic-equivalent-derivative}
			\dot{H}_{t^*}(y)(h) = \nabla^3 \mu_{t^*}(h) + (y - \mu_{t^*})\cdot \dot{\Lambda}_{t^*}(h + \Lambda_{t^*}^{-1} \nabla^2 \mu_{t^*}h).
		\end{equation}
		Moreover, $d_{t^*}(y,h)$ is linear in $h$ and $d_{t^*}(y,h) = O(h)$.
	\end{lemma}
	The proof of Lemma~\ref{lem:approximation-peak-intensity} is given in Section~\ref{subsec:pf-approximation-peak-intensity}. The restriction that $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$ is made so that $\nabla^2 Y_t$ is negative definite with very high probability,  conditional on $Y_t = y$.
	
	\paragraph{Density term.}
	Standard asymptotic analysis (carried out in Section~\ref{subsec:asymptotic-analysis-density}) shows that under the assumptions of Lemma~\ref{lem:approximation-peak-intensity}, the log-density of both $Y_t$ and $\nabla Y_t$ are locally well-approximated by quadratics: if $h^2 \delta_{t^*}^{-1} \to 0, |y - \mu_{t^*}| \delta_{t^*}^{-1}h^3 \to 0$ then
	\begin{align}
		p_{Y_t}(y) & = \frac{1}{\sqrt{2\pi}} \cdot \exp\Big(-\frac{1}{2}\Big\{(y - \mu_{t^*})^2 - (y - \mu_{t^*}) \cdot h'\nabla^2 \mu_{t^*} h\Big\} \Big) \cdot (1 + O(|y - \mu_{t^*}| \delta_{t^*}^{-1} h^3) + O(\delta_{t^*}^{-2}h^4)) \label{eqn:asymptotic-density-height}\\
		p_{\nabla Y_t}(0) & = \frac{(1 - R(h)/2 - d(h)/2)}{\sqrt{(2\pi)^d\det(\Lambda_{t^*})}}\exp\bigg(-\frac{1}{2}h' \nabla^2 \mu_{t^*} \Lambda_{t^*}^{-1}  \nabla^2 \mu_{t^*} h \bigg) \cdot (1 + O(\delta_{t^*}^{-2}h^4) + O(h^2)), \label{eqn:asymptotic-density-gradient}
	\end{align}
	where 
	\begin{align}
		d(h) & = \tr(\Lambda_{t^*}^{-1} \dot{\Lambda}_{t^*}(h)), \label{eqn:asymptotic-intensity-linear-term}\\
		R(h) & = h' \nabla^2 \mu_{t^*} \Lambda_{t^*}^{-1} \{\nabla^3\mu_{t^*}(h^2)\} +  h'\nabla^2 \mu_{t^*} \Lambda_{t^*}^{-1}\{\dot{\Lambda}_{t^*}(\Lambda_{t^*}^{-1}h)\} h \label{eqn:asymptotic-intensity-cubic-term}
	\end{align}
	are linear and cubic in $h$ respectively, of order $d(h) = O(h), R(h) = O(\delta_{t^*}^{-2}h^3)$. 
	
	Combining~\eqref{eqn:approximate-intensity-1}-\eqref{eqn:asymptotic-density-gradient} yields our first main result, an approximation to the intensity $\rho(y,t)$.
	\begin{theorem}
		\label{thm:approximate-joint-intensity}
		Under Assumptions~\ref{asmp:noise-stationary-c2}-\ref{asmp:curvature-asymptotics}, at points $t = t^{\ast} + h, y = \mu_{t^*} + \Delta$ such that $h^2 \delta_{t^*}^{-1} \to 0, \Delta \delta_{t^*}^{-1}h^3 \to 0$ and $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$,
		\begin{align*}
			\rho(y,t) & = \Big(1 + O(h^2) + O(\delta_{t^*}^2) + O(\Delta h^2) + O(\Delta\delta_{t^*}^2)\Big)\cdot\bar{\rho}(t,y),
		\end{align*}
		where
		\begin{equation}
			\label{eqn:approximate-joint-intensity}
			\bar{\rho}(t,y) := \frac{\det(H_{t^*}(y))\Big(1 + d_{t^*}(y,h) - R(h)/2 - d(h)/2\Big)}{\sqrt{(2\pi)^{d + 1}\det(\Lambda_{t^*})}}\cdot\exp\Big(-\frac{(y - \mu_{t^*})^2}{2}\Big) \cdot \exp\bigg(-\frac{h' G_{t^*}(y)h}{2}\bigg),
		\end{equation}
		and $G_{t^*}(y)$ is defined in~\eqref{eqn:goldilocks}.
	\end{theorem}
	To get a sense of the accuracy of the approximation, consider $y = u_{t^*}^{+} + O(1)$ and $h = O(\delta_{t^*}(y))$; these are ``typical'' values of $Y_{\hat{t}},\hat{t} - t^*$ under selection. For such choices, the relative error in approximating $\rho(y,t)$ by $\bar{\rho}(t,y)$ is $O(\delta_{t^*}^2 + |u_{t^*}^{+} - \mu_{t^*}| \delta_{t^*}^2)$. When there is negligible or moderate selection pressure (think $u \leq \mu_{t^*} + O(1))$ the relative error is $O(\delta_{t^*}^2)$, and Theorem~\ref{thm:approximate-joint-intensity} gives a \emph{second-order} accurate approximation to the joint intensity. But even under severe selection pressure (think $u = 2\cdot \mu_{t^*}$, say, where $2$ can be replaced by any constant greater than $1$) the relative error is $O(\delta_{t^*})$ and so the approximation is still accurate.
	
	% \ag{Compare Theorem~\ref{thm:approximate-joint-intensity} to what's known in the literature, for the null case.}
	
	The exponentiated quadratic functional form of the limiting intensity~\eqref{eqn:approximate-joint-intensity} suggests that, in some sense, observed thresholded peaks are asymptotically Normally distributed around signal peaks $t^{\ast}$, with the height of observed peaks following a truncated Gaussian distribution. In order to make proper sense of this statement, by computing a conditional \emph{density} of observed peak location and height near true peaks, we will need to show that each signal peak ``generates'' either zero or one nearby thresholded process peaks with high probability. That is the subject of the next section.
	
	\subsection{Peak uniqueness}
	\label{subsec:expected-number-process-peaks}
	
	Under the conditions of Theorem~\ref{thm:approximate-joint-intensity} it can be shown that the field $Y$ will be approximately locally quadratic around $\hat{t} \in \hat{T}_u$\ag{Make reference to something more concrete to this effect in the stationary case.}, and consequently any thresholded peak $\hat{t} \in \wh{T}_u$ ``near'' $t^{\ast} \in T_0^{\ast}$ will be unique with high probability. The following theorem makes this precise, by relating the probability of observing a single peak near $t^*$ to the expected number of peaks near $t^*$ . Nearness here corresponds to membership in a ball centered at $t^*$, of radius $D_{t^*} := C_0 \sqrt{\log(1/\delta_{t^*})} \delta_{t^*}$, where $C_0$ is a global constant to be specified later. We abbreviate $B_{t^*} = B(t^{\ast},D_{t^*})$. 
	
	\begin{theorem}
		\label{thm:no-more-than-one-process-peak-per-signal-peak}
		Under Assumptions~\ref{asmp:noise-stationary-c2}-\ref{asmp:curvature-asymptotics}, there exists a constant $c_2 > 0$ such that for any $t_n^{\ast} \in T_0^{\ast}$, any $A \subseteq B_{t^*}$ and any $v \geq u$:
		\begin{equation}
			\label{eqn:no-more-than-one-peak-1}
			\begin{aligned}
				\P\big(N_{v}(A) = 1, N_{u}(B_{t^*}) = 1\big) = \big(1 - o(\exp(-c_2 \delta_{t^*}^{-2}))\big) \cdot \E[N_{v}(A)]
			\end{aligned}
		\end{equation}
	\end{theorem}
	The proof of Theorem~\ref{thm:no-more-than-one-process-peak-per-signal-peak} is given in Section~\ref{subsec:pf-no-more-than-one-process-peak-per-signal-peak}.
	
	\subsection{Distribution of height and location after selection}
	Suppose we are told that there is in fact exactly one peak of height at least $u$ in the neighborhood of a given signal peak $t^{\ast}$, in the sense that $N_{u}(B_{t^{\ast}}) = 1$. Denote this unique point by $\hat{t}$.\footnote{This means $\hat{t}$ is only defined on the event $N_{u}(B) = 1$.} The results of the previous two sections can be used to derive an approximation to the joint distribution of $\hat{t}, Y_{\hat{t}}$ conditional on $N_u(B_{t^*}) = 1$. To see how, notice that this joint distribution is characterized by
	\begin{equation}
		\label{eqn:distribution-after-selection}
		\Q_{B,u}(A,v) := \P(\hat{t} \in A, Y_{\hat{t}} \geq v|N_{u}(B) = 1) = \frac{\P(N_{v}(A) = 1, N_{u}(B) = 1)}{\P(N_{v}(B) = 1)}, \quad A \subseteq B, v \geq u.
	\end{equation}
	Let $p(t,v)$ denote the joint density of $\hat{t},Y_{\hat{t}}$ under the law $\Q_{B,u}$. If the conditions of Theorem~\ref{thm:no-more-than-one-process-peak-per-signal-peak} are satisfied, then by that theorem \ag{Need to be careful about rigorously justifying limits}
	\begin{equation}
	\label{eqn:approximate-distribution-after-selection-1}
		\begin{aligned}
			p(t,y)
			& = \lim_{\epsilon \to 0}\frac{1}{|\epsilon|^{d + 1} \nu_d}\big(1 - o(\exp(-c_2\delta^{-2})\big) \cdot \frac{\int_{y}^{y + \epsilon} \int_{B_{\epsilon}}  \rho(t',y') \,dt' \,dy'}{\int_{u}^{\infty} \int_{B} \rho(t',y') \,dy' \,dt'} = \big(1 - o(\exp(-c_2\delta_{t^*}^{-2})\big) \cdot \frac{\rho(t,y)}{\E[N_u(B)]}.
		\end{aligned}
	\end{equation}
	Substituting $\bar{\rho}$ for $\rho$ and computing an approximation to $\E[N_u(B)]$ by marginalizing over $t,y$ leads to an approximation to the joint density $p(t,y)$. Furthermore, integrating this approximation over $B$ gives an approximation to the marginal density of $Y_{\hat{t}}$. From there an approximation to the conditional density of $\hat{t}$ given $Y_{\hat{t}}$ follows from the definition of conditional probability. To ease notation, in the statement of the following theorem we drop all subscripts in $t^*$.
	\begin{theorem}
		\label{thm:approximate-joint-distribution}
		Under the conditions of Theorem~\ref{thm:approximate-joint-intensity}, the joint density of $\hat{t},Y_{\hat{t}}$ at $t = t^* + h,y = \mu + \Delta$ is
		\begin{equation}
			\label{eqn:approximate-joint-density}
			\begin{aligned}
				p(t,y) & = \Big(1 + O(h^2) + O(\delta^2) + O(\Delta h^2) + O(\Delta\delta^2)\Big) \cdot \bar{p}(t,y), \quad {\rm where} \\
				\bar{p}(t,y) & := c(u) \cdot\Big(1 + d(y,h) - R(h)/2 - d(h)/2\Big) \cdot \det(H(y)) \cdot \exp\Big(-\frac{(y - \mu)^2}{2}\Big) \cdot \exp\bigg(-\frac{h' G(y)h}{2}\bigg),
			\end{aligned}
		\end{equation}
		and the constant of proportionality is
		\begin{equation}
			\label{eqn:approximate-joint-density-constant-of-proportionality}
			c(u) = \frac{1}{\sqrt{(2\pi)^{d + 1}}} \exp\bigg(\frac{(u^+ - \mu)\tr(\{H(u^{+})\}^{-1}\Lambda)}{2}\bigg)
			\frac{1}{\Psi(u - \mu - \frac{1}{2}\tr(\{H(u^{+})\}^{-1}\Lambda))}.
		\end{equation}
		Moreover, the marginal density of $Y_{\hat{t}}$ is 
		\begin{equation}
			\label{eqn:approximate-marginal-density-height}
			\begin{aligned}
				p(y) & = \big(1 + O(\delta^2) + O(\Delta \delta^2) + O(|y - u|^2 \delta^2)\big) \cdot \bar{p}(y), \quad {\rm where} \\ 
				\bar{p}(y) & := \frac{1}{\sqrt{2\pi}}\frac{\exp\Big(-\big(y - \mu - \frac{1}{2}\tr(\{H(u^+)\}^{-1}\Lambda)\big)^2/2\Big) }{\Psi(y - \mu - \frac{1}{2}\tr(\{H(u^+)\}^{-1}\Lambda))}.
			\end{aligned}
		\end{equation}
		Finally, the conditional density of $\hat{t}$ given $Y_{\hat{t}} = y$ is 
		\begin{equation}
			\label{eqn:approximate-conditional-density-location}
			\begin{aligned}
				p(t|y) & = \big(1 + O(h^2) + O(\delta^2) + O(\Delta h^2) + O(\Delta\delta^2) + O(|y - u|^2 \delta^2)\big) \cdot \bar{p}(t|y), \quad {\rm where} \\
				\bar{p}(t|y) & := \Big(1 + d(y,h) - d(h)/2 - R(h)/2\Big) \cdot \frac{\sqrt{\det(G(y))}}{\sqrt{(2\pi)^d}} \cdot \exp\Big(-\frac{1}{2}h'G(y) h\Big).
			\end{aligned}
		\end{equation}
	\end{theorem}
	We can draw the following conclusions from Theorem~\ref{thm:approximate-joint-distribution}:
	\begin{itemize}
		\item Takeaways for height: 
		\begin{enumerate}
			\item Peak height is truncated Gaussian, with mean parameter $\mu_{t^*}$ plus the correction term $\frac{1}{2}\tr(\{H(u^+)\}^{-1}\Lambda)$. We are seeing two types of winner's curse bias, one due to selection pressure, the other due to using a local maximum as point estimate.
			\item The truncated Gaussian approximation with corrected mean is always second order accurate.
			\item In traditional asymptotics (i.e. first-order asymptotics with no selection) we ignore the second kind of winner's curse bias because it is asymptotically negligible. This is not second-order accurate, and under strong selection pressure it is not even first-order accurate.
		\end{enumerate}
		\item Takeaways for location: 
		\begin{enumerate}
			\item Conditional on height, location is asymptotically unbiased, Normally distributed, with asymptotic variance = Goldilocks. 
			\item Under stationarity + non-skewness conditions, Goldilocks is second-order accurate. \ag{Will need to elaborate}
			\item The usual sandwich form (with either expected or observed Hessian) is not second-order accurate, and is not even first-order accurate under strong selection pressure.
		\end{enumerate}
	\end{itemize}

	\subsection{Pivots}
	\ag{TO COME}
	
	\subsection{Confidence regions and error guarantees}
	\ag{TO COME}
	
	\section{Asymptotic distribution after randomized selection}
	\label{sec:asymptotic-distribution-after-randomized-selection}
	
	\ag{This section is still wip. In particular I'm quite confident the conclusion of Theorem~\ref{thm:approximate-randomized-joint-intensity} is correct, but need to figure out what the right error term is.}
	
	\subsection{Randomized peak inference using one-step estimators}
	Letting $\omega \sim N(0,\gamma \cdot C)$ for $\gamma > 0$, we split the field $Y$ into selection and inference parts:
	\begin{equation*}
		Y^s := Y + \omega, \quad Y^i = Y - \frac{1}{\gamma} \omega.
	\end{equation*}
	We will use thresholded peaks of the selection field,
	\begin{equation*}
		\wh{T}_u^s := \{t: \nabla Y_t^s = 0, \nabla^2 Y_t^s \preceq 0, Y_t^s \geq u\},
	\end{equation*}
	to identify promising regions of parameter space. We will then produce inferences for peak parameters (i.e. location and height of peaks) in these promising regions, using the full data $Y$. While there are (many) different ways one could imagine doing this, our inferences will be based on one-step estimators. That is, if a given point $t \in \Rd$ is selected from the data, meaning $t \in \wh{T}_u^s$, our corresponding point estimates for peak location and height will be
	\begin{align*}
		\hat{t}_{{\rm one-step}}(t) 
		& := 
		t + [-\nabla^2 Y_t]^{-1} \nabla Y_t \\
		\hat{\mu}_{{\rm one-step}}(t) 
		& := 
		Y_t + \frac{1}{2}\nabla Y_t'[-\nabla^2 Y_t]^{-1} \nabla Y_t.
	\end{align*}
	In order to produce valid inferences for parameters of interest, we will need to understand the distribution of these one-step estimators. We will do this through asymptotic expansion of the intensity function of a certain point process which can be tied to the distribution of one-step estimators.
	
	\subsection{Kac-Rice for randomized selection}
	Our ultimate objective is a conditional law of estimates of $t^*, \mu_{t^*}$ that depend on the full data $Y$, where the conditioning event depends on $Y^s$. Thus, we will need to understand the joint distribution of \emph{pairs} of critical points $\hat{t} \in \wh{T}, \hat{t}^s \in \wh{T}_u^{s}$. Our approach will parallel the non-randomized case: we relate this distribution to the intensity function of a point process that can be computed via the Kac-Rice formula, then approximate this intensity function at points $t$ near $t^*$.
	
	For ${\tt t} = (t,t') \in \mc{T} \times \mc{T}$ let $U_{{\tt t}} = (Y_{t}, \nabla Y_{t'})$ and $V_{{\tt t}} = (\nabla Y_{t},\nabla Y_{t'})$. The following process counts pairs of critical points $\hat{t} \in \wh{T}_v, \hat{t}^s \in \wh{T}_u^{s}$:
	\begin{equation*}
		N_{v,u}(A,B) = N\{{\tt t} \in A \times B: V_{{\tt t}} = 0, JV_{{\tt t}} \preceq 0, U_{{\tt t}} \geq (v,u)\},
	\end{equation*}
	where $JV_{{\tt t}}$ is the Jacobian of $V_{{\tt t}}$, a block diagonal matrix with blocks $\nabla^2 Y_t$ and $\nabla^2 Y_{t'}$. Under \ag{appropriate smoothness and non-degeneracy conditions}, the \emph{Kac-Rice theorem} gives a formula for $\E[N_{v,u}(A,B)]$:
	\begin{equation}
		\begin{aligned}
			\label{eqn:kac-rice-pairs}
			\E\Big[N_{v,u}(A,B)\Big] & = \int \rho({\tt t},{\tt y}) \,d{\tt t} \,d{\tt y}, \quad 
			\rho({\tt t},{\tt y}) := \E[\det(-JV_{{\tt t}}) \cdot \1(J V_{{\tt t}} \preceq 0)|V_{{\tt t}} = 0, U_{{\tt t}} = {\tt y}]  \cdot p_{V_{{\tt t}},U_{{\tt t}}}(0,{\tt y}),
		\end{aligned}
	\end{equation}
	where the region of integration is $A \times B \times [u,\infty) \times [v,\infty)$, and $p_{V_{{\tt t}},U_{{\tt t}}}(0,{\tt y})$ is the Gaussian density of $V_{{\tt t}},U_{{\tt t}}$ evaluated at $0,{\tt y}$. 
	
	\subsection{Asymptotic intensity after randomized selection}
	By Fubini, we can equivalently write the expectation of the counting process $N_{v,u}(A,B_{t^*})$ as 
	\begin{equation*}
		\E[N_{v,u}(A,B_{t^*})] = \int_{A} \int_{v}^{\infty} \rho^{B_{t^*},u}(t,y) \,dt \,dy, \quad {\rm where} \; \rho^{B_{t^*},u}(t,y) := \int_{B_{t^*}} \int_{u}^{\infty} \rho({\tt t},{\tt y}) \,dt' \,dy'.
	\end{equation*}
	We will eventually see that $\rho^{B_{t^*},u}(t,y)$ corresponds to the joint density of $\hat{t},Y_{\hat{t}}$ of peaks $\hat{t} \in \wh{T}$ after observing a randomized peak $\hat{t}^s \in \wh{T}_u^s$ (up to a normalizing constant and exponentially ignorable error). The following theorem gives an approximation to $\rho^{B_{t^*},u}(t,y)$ that is accurate at points $t = t^* + h, y = u_{\gamma} + \Delta$, where $\pi = \frac{\gamma}{1 - \gamma}$ is the fraction of information reserved for inference, and $u_{\gamma}^{+} = \pi \mu_{t^*} + (1 - \pi) u^{+}$ is the conditional expectation $\E[Y_{t^*}|Y_{t^*}^s = u^{+},\nabla Y_{t^*} = 0]$. 
	\begin{theorem}
		\label{thm:approximate-randomized-joint-intensity}
		Under Assumptions~\ref{asmp:noise-stationary-c2}-\ref{asmp:curvature-asymptotics}, at points $t = t^* + h, y = u_{\gamma} + \Delta$ with $h = o(1)$ and $\Delta \geq - o(\delta_{t^*}^{-1})$,
		\begin{equation*}
		\begin{aligned}
			\rho^{B_{t^*},u}(t,y) 
			& = (1 + \red{O(?)}) \cdot \bar{\rho}^{B_{t^*},u}(t,y), \quad {\rm where} \\
			\bar{\rho}^{B_{t^*},u}(t,y)
			& := \Psi\bigg(\frac{u - y - \gamma\tr(\{H_{t^*}(u_{\gamma}^{+})\}^{-1}\Lambda_{t^*})}{\sqrt{\gamma}}\bigg) \cdot \bar{\rho}(t,y).
		\end{aligned}
		\end{equation*}
	\end{theorem}

	\subsection{The conditional distribution of interest}
	We will now give an expression for the distribution of local maxima of $Y$ around $t^*$, conditional on the event that $Y^s$ has a unique local maximum in a large-deviation neighborhood of $t^*$ of height at least $u$, and $Y$ has a unique local maximum (at any height) in a large-deviation neighborhood of $t^*$. In math: for $B \subseteq \mc{T}$, let $N_{u}^s(B) := N(\wh{T}_u^s \cap B)$ count the number of local maxima of $Y^s$ in $B$ that have height at least $u$ and $N_v(B) := N(\wh{T}_v \cap B)$ count the number of local maxima of $Y$ in $B$.  Define $B_{t^*}(u) := B(t^*, C\delta \log(1/\delta))$ where $\delta = \delta_{t^*}(u)$: this is the large-deviation neighborhood mentioned above. We will typically shorten this to $B_{t^*}$. The precise meaning of the statement ``$Y$ has a unique local maximum (at any height) in a large-deviation neighborhood of $t^*$'' is that $N(B_{t^*}) = 1$. Under this event, we denote the unique local maximum of $Y$ in $B_{t^*}(u)$ by $\hat{t}$. We are interested in the following conditional distribution of $\hat{t}, Y_{\hat{t}}$:
	\begin{equation*}
		\P\Big(\hat{t} \in A, Y_{\hat{t}} \geq v|N_{u}^s(B_{t^*}) = 1, N(B_{t^*}) = 1\Big) = \frac{\P(N_v(A) = 1,N_{u}^s(B_{t^*}) = 1, N(B_{t^*}) = 1)}{\P(N_u^s(B_{t^*}) = 1,N(B_{t^*}) = 1)}.
	\end{equation*}
	\ag{I guess, but it remains to prove, that the following approximations are exponentially good:}
	\begin{equation*}
		\P\Big(N_v(A) = 1,N_{u}^s(B_{t^*}) = 1, N(B_{t^*}) = 1\Big) \approx \E[N_{v,u}(A,B_{t^*})], \quad \P\Big(N_u^s(B_{t^*}) = 1,N(B_{t^*}) = 1\Big) \approx \E[N_u^s(B_{t^*})].
	\end{equation*}

	\section{Technical preliminaries}
	
	\subsection{Asymptotics of signal, covariance}
	Consider a pair of points $t,  t'$ such that both $\|t' - t\| = o(1)$ and $\|t - t^*\| = o(1)$ for some $t^* \in T_0^*$. For simplicity of notation, we write $\eta = t' - t$ and $h = t - t^*$, and abbreviate $\delta_{t^*} := \delta_{t^*}(\mu_{t^*})$.  
	
	By Assumption~\ref{asmp:curvature-asymptotics}, the Taylor expansions of $\mu_{t'}, \nabla \mu_{t'}, \nabla^2 \mu_{t'}$ around $t'= t$ read
	\begin{equation}
		\begin{aligned}
			\label{eqn:signal-taylor-expansion}
			|\mu_{t'} - \mu_{t} - \nabla \mu_t'\eta - \frac{1}{2}\eta^{\top}\nabla^2\mu_{t}\eta| 
			& = O(\delta_{t^{\ast}}^{-1}\eta^3) \\
			\|\nabla \mu_{t'} - \nabla \mu_t - \nabla^2 \mu_{t^*} \eta - \frac{1}{2}\nabla^3\mu_{t^*}(\eta^2)\| 
			& = O(\delta_{t^{\ast}}^{-1}\eta^{3}) \\
			\|\nabla^2 \mu_{t'} - \nabla^2 \mu_{t}\| 
			& = O(\delta_{t^{\ast}}^{-1} \eta),
		\end{aligned}
	\end{equation}
	where above $\nabla^3 \mu_{t} := \frac{d}{dt}\nabla^2 \mu_t$. Taking $t = t^*, t' = t$ in~\eqref{eqn:signal-taylor-expansion}, we can read off the following immediate consequences:
	\begin{align}
		\label{eqn:signal-1st-and-2nd-derivative-control}
		|\mu_t - \mu_{t^*}| = O(\delta_{t^*}^{-1} \|t - t^{*}\|^2), \quad \|\nabla \mu_{t}\| = O( \delta_{t^{\ast}}^{-1} \|t - t^*\|), \quad
		\|\nabla^2 \mu_{t}\| = O(\delta_{t^{\ast}}^{-1}), \quad |\mu_{t'} - \mu_t| = O(\eta^2 \delta_{t^*}^{-1} \vee h^2 \delta_{t^*}^{-1}).
	\end{align}
	As for the covariance function, by Assumption~\ref{asmp:noise-stationary-c2} we have the following Taylor expansions:
	\begin{equation}
		\label{eqn:covariance-taylor-expansion}
		\begin{aligned}
			C_{10}(t',t) & = -\Lambda_t \eta + O(\eta^2) \\
			C_{11}(t',t) & = \Lambda_t + (C_{12}(t,t)(\eta))' + O(\eta^2) \\
			C_{11}(t,t') & = \Lambda_t + C_{12}(t,t)(\eta) + O(\eta^2) \\
			\Lambda_{t'} & = \Lambda_t + C_{12}(t,t)(\eta) + (C_{12}(t,t)(\eta))' + O(\eta^2),
		\end{aligned}
	\end{equation}
	where we have used the fact that
	$$
	\Lambda_{t} = \frac{d}{dt'} C_{10}(t,t') \Big|_{t' = t}, \quad C_{12}(t,t) = \frac{d}{dt'} C_{11}(t,t')  \Big|_{t' = t}.
	$$ 
	By algebra, it is further the case that for any $x \in \Rd$,
	\begin{equation}
		\begin{aligned}
			\label{eqn:covariance-taylor-expansion-2}
			C_{11}(t',t)x = \Lambda_tx + C_{21}(t,t)(x)\eta + O(\eta^2\|x\|).
		\end{aligned}
	\end{equation}
	Finally, by Assumptions~\ref{asmp:noise-stationary-c2} and~\ref{asmp:noise-non-degenerate} we have that:
	\begin{equation}
		\label{eqn:variance-control}
		\|\dot{\Lambda}_t\|, \|\Lambda_{t}^{-1}\|_F = O(1), \quad \|\Lambda_{t'} - \Lambda_{t}\|_F = O(\eta).
	\end{equation}
	
	\subsection{Asymptotics of Hessian}
	\label{subsec:hessian-asymptotics}
	We will make use of the following projection/residual decomposition of the Hessian $\nabla^2 Y_t$:
	\begin{equation}
		\begin{aligned}
			-\nabla^2 Y_t 
			& = 
			\Cov\big[-\nabla^2 Y_t; (Y_t, \nabla Y_t)\big]\big(\big\{\Var(Y_t, \nabla Y_t)\big\}^{-1} (Y_t, \nabla Y_t)'\big) + R(-\nabla^2 Y_t; (Y_t, \nabla Y_t)) \\
			& = 
			Y_t \Lambda_t + C_{21}(t,t)(\Lambda_t^{-1} \nabla Y_t) + R(-\nabla^2 Y_t; (Y_t, \nabla Y_t)),
		\end{aligned}
	\end{equation}
	where $R(\nabla^2 Y_t; (Y_t, \nabla Y_t))$ is a residual term independent of $(Y_t, \nabla Y_t)$. We will sometimes find it convenient to rewrite this as
	\begin{equation}
		\label{eqn:hessian-decomposition}
		-\nabla^2 Y_t = -\nabla^2 \mu_t + (Y_t - \mu_t) \Lambda_t + C_{21}(t,t)\big(\Lambda_t^{-1} (\nabla Y_t - \nabla \mu_t)\big) + R(-\nabla^2 \epsilon_t; (\epsilon_t, \nabla \epsilon_t)),
	\end{equation}
	where $R(-\nabla^2 \epsilon_t; (\epsilon_t, \nabla \epsilon_t)) = -\nabla^2 \epsilon_t + \epsilon_t \Lambda_t + \dot{\Lambda}_t( \Lambda_t^{-1} \nabla \epsilon_t)$ is independent of $\epsilon_t,\nabla \epsilon_t$ with mean zero and variance
	\begin{equation}
		\label{eqn:hessian-decomposition-residual-variance}
		\Var[R(\nabla^2 \epsilon_t; (\epsilon_t, \nabla \epsilon_t))] := \sigma^2 \kappa_t.
	\end{equation} 
	The decomposition in~\eqref{eqn:hessian-decomposition} clearly shows that $H_t(y) = \E[-\nabla^2 Y_t|Y_t = y,\nabla Y_t = 0]$, as claimed.
	
	Under Assumptions~\ref{asmp:noise-stationary-c2} and~\ref{asmp:signal-holder} we have that $H_t(y)$ -- viewed as a function of $t$ with $y$ fixed -- is twice continuously differentiable in $B_{t^*}$. If additionally Assumption~\ref{asmp:curvature-asymptotics} then $H_t(y)$ further satisfies the following Lipschitz-like property:
	\begin{align}
		\|H_t(y) - H_{t^*}(y)\| 
		& \leq -(\nabla^2 \mu_{t^*} - \nabla^2 \mu_{t}) + |y - \mu_t|\cdot \|\Lambda_t - \Lambda_{t^*}\| + |\mu_t - \mu_{t^*}| \cdot \|\Lambda_{t^*}\| + \|\dot{\Lambda}_t\| \|\Lambda_t^{-1}\| \|\nabla \mu_t\| \nonumber \\
		& = O(h\delta_{t^*}^{-1} \vee h|y - \mu_{t^*}|), \label{eqn:deterministic-hessian-continuity}
	\end{align} 
	with the second implication following from~\eqref{eqn:signal-1st-and-2nd-derivative-control} and~\eqref{eqn:variance-control}. Under the additional assumption that $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$, the minimum eigenvalue of the deterministic equivalent is bounded below:
	\begin{equation}
	\label{eqn:deterministic-hessian-min-eigenvalue}
	\{\delta_{t^*}(y)\}^{-1} \geq (1 - o(1)) \cdot \max\{\delta_{t^*}^{-1}, |y - \mu_{t^*}|\},
	\end{equation}
	and we therefore additionally have the relative error bounds
	\begin{equation}
		\label{eqn:deterministic-hessian-norm}
		\frac{\|H_t(y) - H_{t^*}(y)\|}{\|H_{t^*}(y)\|} = O(h), \quad \|H_t(y)\| = \|H_{t^*}(y)\| (1 + O(h)), \quad \delta_{t}(y) = \delta_{t^*}(y) (1 + O(h)).
	\end{equation}
	A direct implication of the first statement above is
	\begin{equation}
		\label{eqn:deterministic-hessian-pointwise}
		\max_{i,j} |(H_t(y))_{ij}| = O(\{\delta_{t^*}(y)\}^{-1}).
	\end{equation}
	\paragraph{Taylor expansion of determinant.}
	\ag{The remainder terms need to be checked and better justified.}
	Under Assumptions~\ref{asmp:noise-stationary-c2}-\ref{asmp:curvature-asymptotics} and assuming additionally $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$, Taylor expansion of the function $t' \mapsto \det(H_{t'}(y))$ around $t' = t$ yields
	\begin{equation}
		\label{eqn:taylor-expansion-det}
		\det(H_{t'}(y)) = \det(H_{t}(y)) \cdot \Big(1 + \tr\big(\{H_{t}(y)\}^{-1} \dot{H}_{t}(y)(\eta)\big) + O(\eta^2)\Big).
	\end{equation}
	Suppose additionally that $|y - u_{t^*}^{+}|\delta_{t^*}(u_{t^*}^+) \to 0$. Then Taylor expansion of the function $y \mapsto \det(H_{t^*}(y))$ around $y = u_{t^*}^{+}$ yields
	\begin{equation}
		\label{eqn:taylor-expansion-dety}
		\det(H_{t^{*}}(y)) = \det(H_{t^{*}}(u_{t^*}^{+})) \cdot \Big(1 + (y - u_{t^*}^{+}) \tr\big(\{H_{t^*}(u_{t^*}^{+})\}^{-1}\Lambda_{t^*}\big) + O(|y - u_{t^*}^{+}|^2\delta_{t^*}(u_{t^*}^+)^2)\Big).
	\end{equation}
	

	\paragraph{Moments of determinant.}
	We will need some control on the moments of $\det(-\nabla^2 Y_{t})$ conditional on $Y_{t} = y, \nabla Y_{t} = 0$, under the conditions of Lemma~\ref{lem:approximation-peak-intensity}. First, we have that for all $p \geq 1$, 
	\begin{equation}
		\label{eqn:deterministic-hessian-determinant-moments}
		\E\big[|\det(-\nabla^2 Y_t)|^p|Y_t = y,\nabla Y_t = 0\big] = O\big(\|H_t(y)\|^{pd} \vee 1\big) = O\big(\|H_{t^*}(y)\|^{pd} \vee 1 \vee h^{d}(\delta_{t^*}^{-p} \vee |y - \mu_{t^*}|^p)) = O\big(\delta_{t^*}^{-pd} \vee |y - \mu_{t^*}|^{pd}),
	\end{equation}
	with the first implication following by Lemma~\ref{lem:determinant-moments}, the second by~\eqref{eqn:deterministic-hessian-continuity}, and the third since $\|H_{t^*}(y)\| = O(\delta_{t^*}^{-1} \vee |y - \mu_{t^*}|)$. If $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$, then we may use~\eqref{eqn:deterministic-hessian-min-eigenvalue}, which lower bounds the minimum eigenvalue~$\{\delta_{t^*}(y)\}^{-1}$, to further deduce
	\begin{equation*}
		\label{eqn:deterministic-hessian-determinant-moments-2}
		\E\big[|\det(-\nabla^2 Y_t)|^p|Y_t = y,\nabla Y_t = 0\big] = O(\{\delta_{t^*}(y)\}^{-p}).
	\end{equation*}
	\paragraph{Tail behavior.}
	Finally, we will want to bound the probability that $\nabla^2 Y_t$ is not negative-definite, conditional on $\nabla Y_t = 0$ and $Y_t = y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$. We will show that there exists some $c_0 > 0$ for which
	\begin{equation}
		\label{eqn:deterministic-hessian-negative-definite}
		\P\Big(\nabla^2 Y_t \preceq 0|Y_t = y,\nabla Y_t = 0\Big) = 1 - O(\exp(-c_0\{\delta_{t^*}(y)\}^{-2})).
	\end{equation}
	From the variational representation of eigenvalue we have $\nabla^2 Y_t \not\preceq 0$ if and only if $\sup_{s \in \S^{d - 1}} s'\nabla^2 Y_ts > 0$. However, for each $s \in \S^{d - 1}$ and $\varepsilon > 0$, for all $n$ sufficiently large,
	\begin{equation}
		\E[s'\nabla^2 Y_t s|Y_t = y,\nabla Y_t = 0] = s'H_t(y)s = -s' H_{t^*}(y)s + O(h\{\delta_{t^*}(y)\}^{-1}) \leq -(1 - \varepsilon)\{\delta_{t^*}(y)\}^{-1},
	\end{equation}
	with the second implication following from~\eqref{eqn:deterministic-hessian-continuity} and~\eqref{eqn:deterministic-hessian-min-eigenvalue}. Equation~\eqref{eqn:deterministic-hessian-negative-definite} then follows from the Borell-TIS inequality~\eqref{eqn:borell-tis}, keeping in mind that 
	$$
	\sup_{s \in \S^{d - 1}} \Var[s'\nabla^2 Y_t s|Y_t = y,\nabla Y_t = 0] = \sigma^2 \cdot \sup_{s \in \S^{d - 1}}\kappa_{t}(s^2) = O(1).
	$$
	
	\subsection{Asymptotics of randomized Kac-Rice Hessian}
	\label{subsec:randomized-hessian-asymptotics}
	
	To approximate the determinant term in randomized Kac-Rice, we will need some results on the asymptotic behavior of $JV_{{\tt t}}$ conditional on $U_{{\tt t}} = {\tt y}, V_{{\tt t}} = 0$. Again we will consider points $t,  t'$ such that -- writing $\eta = t' - t$ and $h = t - t^*$ -- both $\|\eta\| = o(1)$ and $\|h\| = o(1)$ for some $t^* \in T_0^*$. 
	
	\paragraph{Decomposition into projection and residual.}
	For notational convenience define $W_{{\tt t}} = (U_{{\tt t}}, V_{{\tt t}}) \in \R^{2 + 2d}$. We are interested in the asymptotic distribution of the Jacobian of $V_{\tt t}$ given $W_{{\tt t}}$. To do so, it will be useful to decompose the Jacobian into its projection onto $W_{{\tt t}}$ and a residual term:
	\begin{equation}
		\begin{aligned}
			-\nabla^2 Y_t 
			& = 
			\Cov\big[-\nabla^2 Y_t, W_{{\tt t}}\big]\Big(\big\{\Var(W_{{\tt t}})\big\}^{-1} W_{{\tt t}}\Big) + R(-\nabla^2 Y_t; W_{{\tt t}}),
		\end{aligned}
	\end{equation}
	where $R(\nabla^2 Y_t; W_{{\tt t}})$ is independent of $W_{{\tt t}}$. We will sometimes find it convenient to rewrite this as
	\begin{equation}
		\label{eqn:hessian-decomposition}
		-\nabla^2 Y_t = -\nabla^2 \mu_t +	\Cov\big[-\nabla^2 Y_t, W_{{\tt t}}\big]\Big(\big\{\Var(W_{{\tt t}})\big\}^{-1} (W_{{\tt t}} - \E[W_{{\tt t}}])\Big) + \underbrace{R(-\nabla^2 \epsilon_t; (W_{{\tt t}} - \E[W_{{\tt t}}]))}_{:= R_t},
	\end{equation}
	where $R_t$ has mean-zero and variance $O(1)$.
	
	\paragraph{Conditional distribution of $\nabla^2 Y_t,\nabla^2 Y_{t'}^s$.}
	We start with the conditional expectation, which by definition is 
	\begin{equation*}
		\E\big[\nabla^2 Y_t|U_{\tt t} = {\tt y}, V_{\tt t} = 0\big] = \E\big[\nabla^2 Y_t\big] + \Cov\big(\nabla^2 Y_t,W_{\tt t}\big) \Big(\{\Var[W_{\tt t}]\}^{-1}(W_{{\tt t}}  -\E[W_{{\tt t}}])\Big).
	\end{equation*}
	The covariance term above is a $d \times d \times (2 + 2d)$ array, consisting of the following sub-arrays:
	\begin{align*}
		\Cov\big(\nabla^2 Y_t,W_{\tt t}\big)_{\cdot \cdot 1} & = -\Lambda_t \\
		\Cov\big(\nabla^2 Y_t,W_{\tt t}\big)_{\cdot \cdot 2} & = C_{20}(t,t') = -\Lambda_t + O(\eta) \\
		\Cov\big(\nabla^2 Y_t,W_{\tt t}\big)_{\cdot \cdot 3:(d + 2)} & = C_{21}(t,t) \\
		\Cov\big(\nabla^2 Y_t,W_{\tt t}\big)_{\cdot \cdot (d + 3):(2d + 2)} & = C_{21}(t,t') = C_{21}(t,t) + O(\eta),	
	\end{align*}
	with the asymptotic statements following from~\eqref{eqn:covariance-taylor-expansion}. The variance term is 
	\begin{equation*}
		\Var[W_{\tt t}] 
		= 
		\begin{bmatrix}
			1 & C(t,t') & 0 & C_{01}(t,t')' \\
			C(t',t) & (1 + \gamma) & C_{10}(t,t')' & 0 \\
			0 & C_{10}(t,t') & \Lambda_t & C_{11}(t,t') \\
			C_{01}(t,t') & 0 & C_{11}(t',t) & (1 + \gamma) \Lambda_{t'}
		\end{bmatrix}
		= 
		\begin{bmatrix}
			1 & 1 & 0 & 0 \\
			1 & (1 + \gamma) & 0 & 0 \\
			0 & 0 & \Lambda_t & \Lambda_{t} \\
			0 & 0 & \Lambda_t & (1 + \gamma) \Lambda_{t}
		\end{bmatrix}
		+ O(\eta) 
		:= \bar{V}[W_{{\tt t}}] + O(\eta),
	\end{equation*}
	with the asymptotic statements again following from~\eqref{eqn:covariance-taylor-expansion}. Hence,
	\begin{align*}
		\{\Var[W_t]\}^{-1}\E[W_t] 
		& = 
		\frac{1}{\gamma} 
		\begin{bmatrix}
			1 + \gamma & -1 & 0 & 0 \\
			-1 & 1 & 0 & 0 \\
			0 & 0 & (1 + \gamma)\Lambda_t^{-1} & -\Lambda_{t}^{-1} \\
			0 & 0 & -\Lambda_t^{-1} & \Lambda_{t}^{-1}
		\end{bmatrix}
		\begin{bmatrix}
			y - \mu_t \\
			y' - \mu_{t'} \\
			0 - \nabla \mu_t \\
			0 - \nabla \mu_{t'}
		\end{bmatrix}
		+ O\big(\eta|y - \mu_{t}| \vee \eta|y' - \mu_{t'}| \vee h^2 \delta_{t^*}^{-1}\big) \\
		& = 
		\frac{1}{\gamma} 
		\begin{bmatrix}
			1 + \gamma & -1 & 0 & 0 \\
			-1 & 1 & 0 & 0 \\
			0 & 0 & (1 + \gamma)\Lambda_t^{-1} & -\Lambda_{t}^{-1} \\
			0 & 0 & -\Lambda_t^{-1} & \Lambda_{t}^{-1}
		\end{bmatrix}
		\begin{bmatrix}
			y - \mu_t \\
			y' - \mu_t \\
			0 - \nabla \mu_t \\
			0 - \nabla \mu_{t}
		\end{bmatrix}
		+ O\big(\eta|y - \mu_{t}| \vee \eta|y' - y| \vee \eta^2 \delta_{t^*}^{-1} \vee h^2 \delta_{t^*}^{-1}\big) \\
		& = 
		\frac{1}{\gamma}
		\begin{bmatrix}
			\gamma(y - \mu_t) + (y - y') \\
			-(y - y') \\
			-\gamma \Lambda_t^{-1} \\
			0
		\end{bmatrix}
		+ O\big(\eta|y - \mu_{t}| \vee \eta|y' - y| \vee \eta^2 \delta_{t^*}^{-1} \vee h^2 \delta_{t^*}^{-1}\big) \\
		& = \frac{1}{\gamma}
		\begin{bmatrix}
			\gamma(y - \mu_t) + (y - y') \\
			-(y - y') \\
			-\gamma \Lambda_t^{-1} \\
			0
		\end{bmatrix}
		+ O\big(\eta|y - \mu_{t^*}| \vee \eta|y' - y| \vee \eta^2 \delta_{t^*}^{-1} \vee h^2 \delta_{t^*}^{-1}\big).
	\end{align*}
	In the first line above we have used~\eqref{eqn:matrix-inverse-taylor-expansion} to control the error introduced in approximating $\Var[W_{{\tt t}}]^{-1}$ by $\bar{V}[W_{{\tt t}}]^{-1}$, noting that the hypothesis of~\eqref{eqn:matrix-inverse-taylor-expansion} applies because $\|\bar{V}[W_{{\tt t}}]^{-1}\|_F = O(1)$. The second line follows from~\eqref{eqn:signal-1st-difference-bound}, the third line is basic algebra, and in the fourth line we have used the triangle inequality and~\eqref{eqn:signal-1st-and-2nd-derivative-control} to obtain $
	|y - \mu_t| \leq |y - \mu_{t^*}| + |\mu_t - \mu_{t^*}| = |y - \mu_{t^*}| + O(h^2\delta_{t^*}^{-1})$. Combining these approximations leads to the following expression for the conditional expectation of the Hessian:
	\begin{equation}
		\label{eqn:randomized-kac-rice-hessian}
		\begin{aligned}
			\E\Big[\nabla^2 Y_t|U_{\tt t} = {\tt y}, V_{\tt t} = 0\Big] 
			& = \nabla^2 \mu_t - (y - \mu_t) \Lambda_t - \Gamma_t(\nabla \mu_t) + O\big(\eta|y - \mu_{t^*}| \vee \eta|y' - y| \vee \eta^2 \delta_{t^*}^{-1} \vee h^2 \delta_{t^*}^{-1}\big) \\
			& = -H_t(y) + O\big(\eta|y - \mu_{t^*}| \vee \eta|y' - y| \vee \eta^2 \delta_{t^*}^{-1} \vee h^2 \delta_{t^*}^{-1}\big) \\
			& := -H_t(y) + O\big(\psi(\eta,h,y,y')\big).
		\end{aligned}
	\end{equation}
	Similar steps show that
	\begin{equation}
		\label{eqn:randomized-kac-rice-hessian-2}
		\E\Big[\nabla^2 Y_{t'}^s|U_{\tt t} = {\tt y}, V_{\tt t} = 0\Big] = -H_{t'}(y') + O\big(\psi(\eta,h,y,y')\big).
	\end{equation}
	These can be stated as relative error bounds under certain conditions on $y,y'$. Namely, if $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$, then $\delta_{t}(y) = O(\delta_{t^*}(y)) = O(\delta_{t^*})$ and so
	\begin{equation*}
		\frac{\Big\|\E\Big[-\nabla^2 Y_t|U_{\tt t} = {\tt y}, V_{\tt t} = 0\Big]  - H_t(y)\Big\|}{\|H_{t}(y)\|} = O\Big(\eta \delta_{t^*}(y)(|y - \mu_{t^*}| \vee |y - y'|) + \eta^2 + h^2\Big),
	\end{equation*}
	Likewise if $y' \geq y - o(\delta_{t^*}^{-1})$ then $\delta_{t'}(y') = O(\delta_{t^*}(y)) = O(\delta_{t^*})$ and
	\begin{equation*}
		\frac{\Big\|\E\Big[-\nabla^2 Y_{t'}^s|U_{\tt t} = {\tt y}, V_{\tt t} = 0\Big] - H_{t'}(y')\Big\|}{\|H_{t'}(y')\|} = O\Big(\eta \delta_{t^*}(y)(|y - \mu_{t^*}| \vee |y - y'|) + \eta^2 + h^2\Big).
	\end{equation*}
	As for the conditional variance, we will only need that 
	\begin{equation}
		\label{eqn:randomized-kac-rice-hessian-variance}
		\sup_{s \in \S^{d - 1}} \Var[s'\nabla^2 Y_t s|U_{{\tt t}} = {\tt y},V_{{\tt t}} = 0] = O(1).
	\end{equation}
	
	\paragraph{Determinant of conditional expectation of $JV_{{\tt t}}$.} Observe that $JV_{\tt t}$ is block diagonal, with blocks $\nabla^2Y_{t}$ and $\nabla^2 Y_{t'}^s$. Denoting $J_{{\tt t}}({\tt y}) := \E[JV_{{\tt t}}|U_{\tt t} = {\tt y}, V_{\tt t} = 0]$, our previous derivations imply that if $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1}), y \leq \mu_{t^*} + O(\delta_{t^*}^{-1})$ and $y' \geq \mu_{t^*} - o(\delta_{t^*}^{-1}), |y' - y| = O(\delta_{t^*}^{-1})$, then
	\begin{align}
		\det(J_{\tt t}({\tt y})) 
		& = \det\Big(\E\Big[-\nabla^2 Y_t|U_{\tt t} = {\tt y}, V_{\tt t} = 0\Big]\Big) \cdot \det\Big(\E\Big[-\nabla^2 Y_{t'}^s|U_{\tt t} = {\tt y}, V_{\tt t} = 0\Big]\Big) \nonumber \\
		& = \det(H_t(y)) \cdot \det(H_{t'}(y')) \cdot \Big(1 + O\Big(\eta \delta_{t^*}(y)(|y - \mu_{t^*}| \vee |y - y'|) + \eta^2 + h^2\Big)\Big) \label{eqn:randomized-kac-rice-det-jacobian}
	\end{align}
	
	\paragraph{Moments and tail behavior of determinant.}
	First, we have that for all $p \geq 1$, 
	\begin{equation}
		\begin{aligned}
			\label{eqn:asymptotics-hessian-determinant-moments}
			\E\big[|\det(-\nabla^2 Y_t)|^p|U_{{\tt t}} = {\tt y},\nabla Y_{{\tt t}} = 0\big] 
			& = O\Big(\|\E[-\nabla^2 Y_t|U_{{\tt t}} = {\tt y},\nabla Y_{{\tt t}} = 0]\|^p \vee 1\Big) \\
			& = O\big(\|H_t(y)\|^{pd} \vee \psi(\eta,h,y,y')^{pd} \vee 1\big) \\
			& = O\big(\{\delta_{t^*}(y)\}^{-p} \vee \psi(\eta,h,y,y')^{pd} \vee 1\big),
		\end{aligned}
	\end{equation}
	with the first line following from Lemma~\ref{lem:determinant-moments} and~\eqref{eqn:randomized-kac-rice-hessian-variance}, the second line from~\eqref{eqn:randomized-kac-rice-hessian}, and the final line from~\eqref{eqn:deterministic-hessian-pointwise}. If further $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$, then the first term is dominant:
	\begin{equation}
		\label{eqn:asymptotics-hessian-determinant-moments-2}
		\E\big[|\det(-\nabla^2 Y_t)|^p|U_{{\tt t}} = {\tt y},\nabla Y_{{\tt t}} = 0\big] = O(\{\delta_{t^*}(y)\}^{-p}).
	\end{equation}
	As for the tail behavior, suppose $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$. Then there exists some $c_0 > 0$ for which
	\begin{equation}
		\label{eqn:asymptotics-hessian-negative-definite}
		\P\Big(\nabla^2 Y_t \preceq  0|U_{{\tt t}} = {\tt y}, V_{{\tt t}} = 0\Big) = 1 - O(\exp(-c_0\{\delta_{t^*}(y)\}^{-2})).
	\end{equation}
	To see why, note that from the variational representation of eigenvalue we have $\nabla^2 Y_t \not\succ 0$ if and only if $\sup_{s \in \S^{d - 1}} s'\nabla^2 Y_ts > 0$. However, for each $s \in \S^{d - 1}$ and $\varepsilon > 0$,
	\begin{align*}
		\E[s'\nabla^2 Y_t s|Y_t = y,\nabla Y_t = 0] 
		& = s'\E[\nabla^2 Y_t |U_{{\tt t}} = {\tt y}, V_{{\tt t}} = 0]s \\
		& = -(1 + O(\eta \vee h^2)) s' H_t(y)s \\
		& = -(1 + O(\eta \vee h^2)) s' H_{t^*}(y)s \\
		& \leq  -(1 + O(\eta \vee h^2)) \{\delta_{{t^*}}(y)\}^{-1},
	\end{align*}
	with the first line being linearity of expectation, the second line following from~\eqref{eqn:randomized-kac-rice-hessian}, the third line from~\eqref{eqn:deterministic-hessian-continuity}, and the inequality by definition of $\delta_{t^*}(y)$. Note that the restriction $y \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$ ensures $\{\delta_{t^*}(y)\}^{-1} \to \infty$. Thus, Equation~\eqref{eqn:asymptotics-hessian-negative-definite} follows from the Borell-TIS inequality~\eqref{eqn:borell-tis} and the bound on the conditional variance in~\eqref{eqn:randomized-kac-rice-hessian-variance}. 
	
	Similar reasoning shows that equivalent results hold for $\nabla^2 Y_{t'}^s$, so that if $y' \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$:
	\begin{equation}
		\label{eqn:randomized-kac-rice-hessian-moments-2}
		\begin{aligned}
			\E\big[|\det(-\nabla^2 Y_{t'}^s)|^p|U_{{\tt t}} = {\tt y},\nabla Y_{{\tt t}} = 0\big] 
			& = 
			O(\{\delta_{t^*}(y')\}^{-p}),  \\
			\P\Big(\nabla^2 Y_{t'}^s \preceq  0|U_{{\tt t}} = {\tt y}, V_{{\tt t}} = 0\Big) 
			& = 
			1 - O(\exp(-c_0\{\delta_{t^*}(y')\}^{-2})).
		\end{aligned}
	\end{equation}
	
	\section{Proofs for Section~\ref{sec:asymptotic-distribution}}
	
	\subsection{Proof of Lemma~\ref{lem:approximation-peak-intensity}}
	\label{subsec:pf-approximation-peak-intensity}
	We begin by verifying that the negative definite indicator is exponentially ignorable:
	\begin{equation}
		\label{pf:approximation-peak-intensity-1}
		\begin{aligned}
			& \bigg|\E\Big[\det(-\nabla^2 Y_t) \cdot \1(\nabla Y_t \preceq 0)|Y_t = y,\nabla Y_t = 0\Big] - \E\Big[\det(-\nabla^2 Y_t)|Y_t = y,\nabla Y_t = 0\Big]\bigg| \\
			& \quad = \bigg|\E\Big[\det(-\nabla^2 Y_t) \cdot \1(\nabla Y_t \preceq 0)|Y_t = y,\nabla Y_t = 0\Big]\bigg| \\
			& \quad \leq \Big\{\E\Big[|\det(\nabla^2 Y_t)|^{p}|Y_t = y,\nabla Y_t = 0\Big]\Big\}^{1/p} \cdot \Big\{\P\Big(\nabla^2 Y_t \preceq 0|Y_t = y,\nabla Y_t = 0\Big)\Big\}^{1/q} \\
			& \quad = O\Big(\{\delta_{t^*}(y)\}^{-d} \exp(-c_1 \{\delta_{t^*}(y)\}^{-2})\Big).
		\end{aligned}
	\end{equation}
	The inequality above is H\"{o}lder's, and holds for any conjugate exponents $1/p + 1/q = 1$. The final implication combines the upper bound on the moments of $\det(\nabla^2 Y_t)$ from~\eqref{eqn:deterministic-hessian-determinant-moments-2}, and the lower bound on the probability that $\nabla^2 Y_t$ is negative definite from~\eqref{eqn:deterministic-hessian-negative-definite}; in this statement $c_1 = c_0/q$. 
	
	Next we use the decomposition in~\eqref{eqn:hessian-decomposition} to write
	\begin{equation*}
		\E\Big[\det(-\nabla^2 Y_t)|Y_t = y,\nabla Y_t = 0\Big] = \E\Big[\det(H_t(y) + R_t)\Big],
	\end{equation*}
	where we have abbreviated $R_t = R(-\nabla^2 \epsilon_t;(\epsilon_t,\nabla \epsilon_t))$. Applying the representation of determinant in~\eqref{eqn:representation-determinant} and using linearity of expectation gives
	\begin{align*}
		\E[\det(H_t(y) + R_t)] = \sum_{k = 0}^{d} \sum_{\mc{P}} \omega(p) \sum_{s \in \mc{S}_k} \E[(R_t)_{sp_{s}}] (H_t(y))_{s^cp_{s^c}} =: \sum_{k = 0}^{d} D_k.
	\end{align*}
	Obviously $D_0 = \det(H_t(y))$. Since $R_t$ is a mean-zero Gaussian, it follows that $\E[(R_t)_{s^cp_{s^c}}] = 0$ for any $s \in \mc{S}_k$ with $k$ odd, so that the contributions of all such terms vanish in the expected determinant, and in particular $D_1 = 0$. On the other hand, it follows from~\eqref{eqn:deterministic-hessian-pointwise} that
	$(H_t(y))_{sp_s} = O(\{\delta_{t^*}(y)\}^{-|s|})$; since $\E[(R_t)_{sp_s}] = O(1)$, we conclude that $D_k = O(\{\delta_{t^*}(y)\}^{-|d - k|})$ for all $k = 2,\ldots,d$. In summary,
	\begin{equation*}
		\E[\det(H_t(y) + R_t)] = \det(H_t(y))(1 +  O(\{\delta_{t^*}(y)\}^2)). 
	\end{equation*}
	Finally, Taylor expansion of $H_t(y)$ around $t = t^*$ as in~\eqref{eqn:taylor-expansion-det} yields
	\begin{equation*}
		\det(H_t(y)) = \det(H_{t^*}(y))\big(1 + \tr(\{H_t(y)\}^{-1} \dot{H}_t(y)(h)) + O(h^2)\big).
	\end{equation*}
	which completes the proof of the Lemma.
	
	\subsection{Derivation of~\eqref{eqn:asymptotic-density-height} and~\eqref{eqn:asymptotic-density-gradient}}
	\label{subsec:asymptotic-analysis-density}
	
	\paragraph{Density of height.}
	The density of the height is
	\begin{equation}
		\label{eqn:density-height}
		p_{Y_t}(y) = \frac{1}{\sqrt{2\pi}} \cdot \exp\Big(-\frac{1}{2\sigma^2} (y - \mu_t)^2\Big).
	\end{equation}
	It follows from~\eqref{eqn:signal-taylor-expansion} that
	\begin{equation*}
		(y - \mu_t)^2 = (y - \mu_{t^*})^2 - h'\{(y - \mu_{t^*})\nabla^2\mu_{t^*}\}h + O(\Delta \delta_{t^*}^{-1}h^3) + O(\delta_{t^*}^{-2}h^4),	
	\end{equation*}
	and plugging this back into~\eqref{eqn:density-height} implies~\eqref{eqn:asymptotic-density-height}. 
	
	\paragraph{Density of gradient.}
	The density of the gradient evaluated at $0$ is
	\begin{equation}
		\label{eqn:density-gradient}
		p_{\nabla Y_t}(0) = \frac{1}{\sqrt{(2\pi)^d\det(\Lambda_t)}} \exp\Big(-\frac{1}{2\sigma^2} \nabla \mu_t' \Lambda_t^{-1}\nabla\mu_t\Big).
	\end{equation}
	The first-order Taylor expansion of $\Lambda_t^{-1}$ around $t = t^*$ is
	\begin{equation*}
		\Lambda_t^{-1} = \Lambda_{t^*}^{-1} - \Lambda_{t^*}^{-1}\dot{\Lambda}_{t^*}(\Lambda_{t^*}h) + O(h^2),
	\end{equation*}
	and along with the Taylor expansion of $\nabla \mu_t$ in~\eqref{eqn:signal-taylor-expansion} this gives
	\begin{align*}
		\nabla \mu_{t}' \Lambda_t^{-1} \nabla \mu_t 
		& = (\nabla^2 \mu_{t^*}h + \frac{1}{2}\nabla^3 \mu_{t^*}(h^2))'(\Lambda_{t^*}^{-1} - \Lambda_{t^*}^{-1}\dot{\Lambda}_{t^*}(\Lambda_{t^*}^{-1}h)) (\nabla^2 \mu_{t^*}h + \frac{1}{2}\nabla^3 \mu_{t^*}(h_n^2)) + O(\delta_{t^*}^{-1}h^3) \\
		& = h' \nabla^2 \mu_{t^*} \Lambda_{t^*}^{-1}  \nabla^2 \mu_{t^*} h + R(h) + O(\delta_{t^*}^{-1}h^3),
	\end{align*}
	where
	\begin{equation*}
		R(h) = h' \nabla^2 \mu_{t^*} \Lambda_{t^*}^{-1} \{\nabla^3\mu_{t^*}(h^2)\} +  h'\nabla^2 \mu_{t^*} \Lambda_{t^*}^{-1}\{\dot{\Lambda}_{t^*}(\Lambda_{t^*}^{-1}h)\} h
	\end{equation*}
	is cubic in $h$ and is $O(\delta_{t^*}^{-1}h^2)$. First-order Taylor expansion of $\det(\Lambda_t)$ around $t = t^*$ yields
	\begin{align*}
		(\det(\Lambda_t))^{-1/2} 
		& = \Big(\det(\Lambda_{t^*})(1 + \tr(\Lambda_{t^*}^{-1}\dot{\Lambda}_2(t^*)(h)) + O(h^2))\Big)^{-1/2} \\
		& = \{\det(\Lambda_{t^*})\}^{-1/2}\Big(1 - \frac{1}{2}\tr(\Lambda_{t^*}^{-1}\dot{\Lambda}_2(t^*)(h)) + O(h^2)\Big).
	\end{align*}
	with the second line using $(1 + x)^{-1/2} = 1 - x/2 + O(x^2)$. Plugging these asymptotic expansions back into~\eqref{eqn:density-gradient} yields
	\begin{equation*}
		\begin{aligned}
			p_{\nabla Y_t}(0) 
			& = \frac{\Big(1 - \frac{1}{2}\tr(\Lambda_{t^*}^{-1}\dot{\Lambda}_2(t^*)(h))\Big)}{\sqrt{(2\pi)^d\det(\Lambda_{t^*})}}\exp\bigg(-\frac{1}{2\sigma^2}\Big\{h' \{\nabla^2 \mu_{t^*} \Lambda_{t^*}^{-1}  \nabla^2 \mu_{t^*}\} h + R(h)\Big\}- \frac{d(h)}{2}\bigg) \cdot (1 + O(\delta_{t^*}^{-1}h^3) + O(h^2)) \\
			& = \frac{\Big(1 - R(h)/2\sigma^2 - d(h)/2\Big)}{\sqrt{(2\pi)^d\det(\Lambda_{t^*})}}\exp\bigg(-\frac{1}{2\sigma^2}h' \{\nabla^2 \mu_{t^*} \Lambda_{t^*}^{-1}  \nabla^2 \mu_{t^*}\} h \bigg) \cdot \Big(1 + O(\delta_{t^*}^{-2}h^4) + O(h^2)\Big),
		\end{aligned}
	\end{equation*}
	with the first line following since $R(h) = O(\delta^{-2}h^3), d(h) = O(h)$ and the second line using $\exp(-x) = 1 - x + O(x^2)$ as $x \to 0$. This is the claimed result.
	
	\subsection{Proof of Theorem~\ref{thm:no-more-than-one-process-peak-per-signal-peak}}
	\label{subsec:pf-no-more-than-one-process-peak-per-signal-peak}
	\ag{TO COME}
	
	\iffalse
	We will shorten $A = A_n, B = B_{t_n^{\ast}}$. In order to prove Theorem~\ref{thm:no-more-than-one-process-peak-per-signal-peak}, it suffices to upper bound
	\begin{equation*}
		\P(N_{u}(B) > 1, N_{u}(A) = 1), \quad \E[N_{u}(A) \cdot \1\{N_{u}(A) > 1\}].
	\end{equation*}
	We will upper bound both of these quantities by the expectation of the following point process:
	\begin{equation*}
		N_{u}(A,B) := N\{(t,t') \in (A \times B) \cap (\wh{T}_{u} \times \wh{T}_{u})\}.
	\end{equation*}
	We notice that if $N_{u}(B) > 1$ and $N_{u}(A) = 1$, then $N_{u}(A,B) \geq 1$. Additionally, $N_{u}(A) \cdot \1\{N_{u}(A) > 1\} < N_{u}(A,B)$. Since $\P(N_{u}(A,B) \geq 1) \leq \E[N_{u}(A,B)]$ and $\P(N_{u}(A) \geq 1) \leq \E[N_{u}(A)]$, to prove both results of the Theorem, we simply need to show that
	\begin{equation*}
		\E[N_{u}(A,B)] = o\Big(\E[N_{u}(A)] \cdot \exp(-\varepsilon_1\delta_{t^{\ast}}^2)\Big).
	\end{equation*}
	It is possible to write $\E[N_{u}(A,B)]$ in terms of a ``double'' Kac-Rice integral, but we will not do this. Instead, we observe that if $N_{u}(A,B) \geq 1$, then it must be the case that the Hessian of $Y$ flips from negative to non-negative definite somewhere in $B$, or more formally, \ag{Argue why}
	$$
	\sup_{t' \in B, s \in \mathbb{S}^{d}} s' \nabla^2 Y_{t'} s \geq 0. 
	$$
	It follows that $N_{u}(A,B)$ can in turn be further upper bounded by
	$$
	N\Big\{t \in A: \nabla Y_t = 0, Y_t \geq u, \sup_{t' \in B, s \in \mathbb{S}^{d}} s' \nabla^2 Y_{t'} s \geq 0 \Big\}.
	$$
	The expectation of this point process is just a ``single'' Kac-Rice integral:
	\begin{align}
		\E[N_{u}(A,B)] 
		& 
		\leq \int_{A} \E\bigg[\det(\nabla^2 Y_t) \cdot \1\Big(Y_t \geq u, \nabla^2 Y_t \preceq 0, \sup_{t' \in B, s \in \mathbb{S}^{d}} s' \nabla^2 Y_{t'} s \geq 0\Big)|\nabla Y_t = 0\bigg] \cdot \phi_{\nabla Y_t}(0) \,dt \nonumber \\
		& 
		\leq \int_{A} \E\bigg[\det(\nabla^2 Y_t) \cdot \1\Big(Y_t \geq u, \sup_{t' \in B, s \in \mathbb{S}^{d}} s' \nabla^2 Y_{t'} s \geq 0\Big)|\nabla Y_t = 0\bigg] \cdot \phi_{\nabla Y_t}(0) \,dt \nonumber \\
		& 
		\leq \int_{A} \Big(\E\big[\big(\det(\nabla^2 Y_t)\big)^q\big]\Big)^{1/q} \cdot \P\Big(Y_t \geq u, \sup_{t' \in B, s \in \mathbb{S}^{d}} s' \nabla^2 Y_{t'} s \geq 0|\nabla Y_t = 0\Big)^{1/p} \cdot \phi_{\nabla Y_t}(0) \,dt \nonumber \\
		& \leq
		O(\det(\nabla^2 \mu_{t^*})) \int_{A} \P\Big(Y_t \geq u, \sup_{t' \in B, s \in \mathbb{S}^{d}} s' \nabla^2 Y_{t'} s \geq 0|\nabla Y_t = 0\Big)^{1/p} \cdot \phi_{\nabla Y_t}(0) \,dt \label{pf:no-more-than-one-process-peak-per-signal-peak-1}
	\end{align}
	with the second-to-last line following by H\"{o}lder's inequality and holding for any conjugate exponents $p,q$, and the last line following by Lemma~\ref{lem:determinant-moments}. It remains to argue that the probability in the final line above is exponentially smaller than $\P(Y_t \geq u)$: precisely, that there exists $\alpha > 0$ such that
	\begin{equation}
		\label{pf:no-more-than-one-process-peak-per-signal-peak-2}
		\P\Big(Y_t \geq u, \sup_{t' \in B, s \in \mathbb{S}^{d}} s' \nabla^2 Y_{t'} s \geq 0|\nabla Y_t = 0\Big) = o\Big(\P(Y_t \geq u) \exp(-\alpha \delta_{t^{\ast}}^{-2})\Big).
	\end{equation}
	With~\eqref{pf:no-more-than-one-process-peak-per-signal-peak-2} in hand, making a sufficiently small choice of $p > 1$ in~\eqref{pf:no-more-than-one-process-peak-per-signal-peak-1} leads us to deduce 
	\begin{equation*}
		\E[N_{u}(A,B)] = o(\det(\mc{G}_{t^{\ast}}) \exp(-\alpha_1 \delta_{t^{\ast}}^{-2}) \int_{A} \P(Y_t \geq u) \phi_{\nabla Y_t}(0) \,dt = o\big(\E[N_{u}(A)] \cdot \exp(-\alpha_1 \delta_{t^{\ast}}^{-2})\big),
	\end{equation*}
	with the last statement following from Lemma~\ref{lem:approximation-peak-intensity-deterministic-hessian}, and proving the claim. 
	
	It remains to establish~\eqref{pf:no-more-than-one-process-peak-per-signal-peak-2}. It will be useful to decompose the random variable $s' \nabla^2 Y_{t'} s$ into a projection onto $Y_t,\nabla Y_t$, and a residual term: \ag{Check whether Jon has some tensor notation for $B$ that he prefers.}
	\begin{align*}
		s \nabla^2 Y_{t'} s 
		& = Y_t \cdot s' A_{t,t'} s  + s' B_{t,t'}(\nabla Y_t)s + X_{t',s}^{\perp},
	\end{align*}
	where $A_{t,t'} := \Cov(Y_t, \nabla^2 Y_{t'})$, $B_{t,t'}(v) = \sum_{i = 1}^{p} v_i \partial_{i} \nabla^2 C(t' - t) \bLambda_2^{-1}$, and $X_{t',s}^{\perp}$ is Gaussian, independent of $(Y_t,\nabla Y_t)$, with
	\begin{equation*}
		\E[X_{t',s}^{\perp}] = s'(\nabla^2 \mu_{t'} - \mu_t A_{t,t'} - B_{t,t'}(\nabla \mu_t))s, \quad \V[X_{t',s}^{\perp}] \leq u_4.
	\end{equation*}
	It follows that conditional on $\nabla Y_{t} = 0$, 
	\begin{equation*}
		s' \nabla^2 Y_{t'} s \overset{d}{=} Y_t \cdot s' A_{t,t'} s + s'(\nabla^2 \mu_{t'} - \mu_t A_{t,t'} - B_{t,t'}(\nabla \mu_t))s + R_{t',s},
	\end{equation*}
	where $R_{t',s}$ is a mean-zero Gaussian process with variance at most $u_4$. We will further decompose the mean term on the right hand side of the previous expression as follows:
	\begin{align*}
		& -s'\mc{G}_{t^{\ast}}s + (\max(u,\mu_t) - Y_t) \cdot s' A_{t,t'} s  + s'\bigg((u - \mu_{t})_{+} (\bLambda_2 - A_{t,t'})  + B_{t,t'}(\nabla \mu_t) + \nabla^2 \mu_{t'} - \nabla^2 \mu_t + \mc{G}_{t^{\ast}} - \mc{G}_{t}\bigg)s \\
		& := -s'\mc{G}_{t^{\ast}}s + (\max(u,\mu_t) - Y_{t}) \cdot s' A_{t,t'} s  + s'E_{t,t'}s.
	\end{align*}
	Next we show that $s'E_{t,t'}s$ is asymptotically negligible. By our assumptions on the set $A$, $t = t_n^{\ast} + h_n$ and $t' = t_n^{\ast} + h_n'$ where $h_n,h_n' = O(D_n \delta_{t_n}^{\ast}) = o(1)$. Therefore 
	\begin{align*}
		\|\mc{G}_{t^{\ast}} - \mc{G}_{t}\|_F, \|\nabla^2 \mu_t - \nabla^2 \mu_{t'}\|_F = O(\delta_{t^{\ast}}^{-1} \|h_n\|^{\alpha}) = o(\delta_{t^{\ast}}^{-1}) \tag{by~\eqref{eqn:signal-1st-difference-control}} \\
		\|B_{t,t'}(\nabla \mu_t)\| \leq C \max_{ijk} |\partial_{ijk} C(t' - t)| \|\nabla \mu_t\| = o(\delta_{t^{\ast}}^{-1}) \tag{by~\eqref{eqn:signal-1st-and-2nd-derivative-control}} \\
		(u - \mu_t)_+ \|\bLambda_2 - A_{t,t'}\|_F = o(u - \mu_t)_{+} = o(\delta_{t^{\ast}}^{-1}) \tag{since $C \in C^{2 + \alpha}$},
	\end{align*}
	and as a result, for any $\varepsilon > 0$, $-s'\mc{G}_{t^{\ast}}s + s' E_{t,t'}s \leq -(1 - \varepsilon) s'\mc{G}_{t^{\ast}}s$ for all $n$ large enough. For all $n$ large enough, we can therefore upper bound
	$$
	\P\Big(Y_t \geq u, \sup_{t' \in B, s \in \mathbb{S}^{d}} s' \nabla^2 Y_{t'} s \geq 0|\nabla Y_t = 0\Big) \leq \P\Big(Y_t \geq u, \sup_{t' \in B, s \in \mathbb{S}^{d}} X_{s,t'}^{\perp} + (\max(u,\mu_t) - Y_{t}) \cdot s' A_{t,t'} s \geq (1 - \varepsilon) s'\mc{G}_{t^{\ast}} s \Big)
	$$
	We now reason separately about the case where $u \leq \mu_t$ and $u > \mu_t$. In the first case, we have that $X_{s,t'}^{\perp} + (\max(u,\mu_t) - Y_{t}) \cdot s' A_{t,t'} s$ has mean-zero and variance at most $u_4 + \lambda_{\max}(A_{t,t'})$. It therefore follows that 
	\begin{align*}
		& \P\Big(Y_t \geq u, \sup_{t' \in B, s \in \mathbb{S}^{d}} X_{s,t'}^{\perp} + (\max(u,\mu_t) - Y_{t}) \cdot s' A_{t,t'} s \geq (1 - \varepsilon) s'\mc{G}_{t^{\ast}} s\Big) \\
		& \leq \P\Big(\sup_{t' \in B, s \in \mathbb{S}^{d}} X_{s,t'}^{\perp} + (\max(u,\mu_t) - Y_{t}) \cdot s' A_{t,t'} s \geq (1 - \varepsilon) s'\mc{G}_{t^{\ast}} s\Big) \\
		& = o(\exp(-\alpha_1 \delta_{t^{\ast}}^{-2})) \\
		& = o\big(\P(Y_t \geq u) \cdot \exp(-\alpha \delta_{t^{\ast}}^{-2})\big),
	\end{align*}
	with the third line following by~\eqref{eqn:borell-tis}, and the last statement following since $\P(Y_t \geq u) = O(1)$ if $\mu_t \geq u$. In the second case, 
	\begin{align*}
		& \P\Big(Y_t \geq u, \sup_{t' \in B, s \in \mathbb{S}^{d}} X_{s,t'}^{\perp} + (\max(u,\mu_t) - Y_{t}) \cdot s' A_{t,t'} s \geq (1 - \varepsilon) s'\mc{G}_{t^{\ast}} s\Big) \\
		& \leq \P\Big(Y_t \geq u, \sup_{t' \in B, s \in \mathbb{S}^{d}} X_{s,t'}^{\perp} \geq (1 - \varepsilon) s'\mc{G}_{t^{\ast}} s\Big) \\
		& = \P\Big(Y_t \geq u\Big) \cdot \P\Big(\sup_{t' \in B, s \in \mathbb{S}^{d}} X_{s,t'}^{\perp} \geq (1 - \varepsilon) s'\mc{G}_{t^{\ast}} s\Big) \\
		& = o\big(\P(Y_t \geq u) \cdot \exp(-\alpha \delta_{t^{\ast}}^{-2})\big),
	\end{align*}
	with the last line again following by~\eqref{eqn:borell-tis}. In either case, we have the desired bound~\eqref{pf:no-more-than-one-process-peak-per-signal-peak-2}.
	\fi
	
	\subsection{Proof of Theorem~\ref{thm:approximate-joint-distribution}}
	As in the statement of Theorem~\ref{thm:approximate-joint-distribution}, in the proof we will drop all notational dependency on $t^*$. In order to prove the claimed results~\eqref{eqn:approximate-joint-density}-\eqref{eqn:approximate-conditional-density-location}, essentially what remains is to compute an approximation to $\E[N_u(B)]$. In fact we will compute an approximation to $\E[N_v(B)]$ for $v \geq u$, since that will be helpful in proving approximate pivotallness later on. To approximate $\E[N_v(B)]$, we will truncate to a range of integration where the conditions of Theorem~\ref{thm:approximate-joint-intensity} are met, then integrate the approximate intensity $\bar{\rho}(t,y)$. To that end, we introduce the truncation limits $\lambda^{-} = \max(\mu - C\delta^{-1/2}, u), \lambda^{+} = \max(\mu,u) + C\delta^{-1/2}$, and decompose
	\begin{equation*}
		\E[N_v(B)] = \E[N_{[v,\lambda^{-} \vee v)}(B)] + \E[N_{[\lambda^{-} \vee v, \lambda^{+})}(B)] + \E[N_{[\lambda^{+},\infty)}(B)].
	\end{equation*}
	We call these terms the ``lower truncation'', ``main'', and ``upper truncation'' terms, and approximate each separately. As the names suggest, the dominating term will be the main term $\E[N_{[\lambda^{-} \vee v, \lambda^{+})}(B)]$, with the lower and upper truncation terms being comparatively negligible.
	
	\paragraph{Lower truncation term.}
	\ag{TO COME}
	
	\paragraph{Upper truncation term.}
	\ag{TO COME}
	
	\paragraph{Main term.}
	Notice that all $t,y$ for which $t \in B, y \in [\lambda^{-} \vee v, \lambda^{+})$ satisfy the conditions of Theorem~\ref{thm:approximate-joint-intensity}.  Therefore, by that theorem,
	\begin{equation*}
		\begin{aligned}
			\E[N_{[\lambda^{-} \vee v, \lambda^{+})}(B)] & =  \int_{\lambda^{-} \vee v}^{\lambda^{+}} \int_{B} \Big(1 + O\big(\Delta h^2\big) + O(h^2) + O\big(\Delta\delta^2\big) + O(\delta^2)\Big) \cdot \bar{\rho}(t,y) \,dt \,dy.
		\end{aligned}
	\end{equation*}
	The dominating term in the inner integral is 
	\begin{equation*}
	\begin{aligned}
		\int_{B} \bar{\rho}(t,y) \,dt 
		& = \frac{\det(H(y))}{\sqrt{(2\pi)^{d + 1}\det(\Lambda)}} \cdot \exp(-(y - \mu)^2/2) \cdot \int_{B} \Big(1 + d(y,h) - \frac{d(h)}{2} - \frac{R(h)}{2}\Big) \exp\Big(-\frac{h'G(y)h}{2}\Big) \,dh \\
		& = \frac{\det(H(y))}{\sqrt{(2\pi)^{d + 1}\det(\Lambda)}} \cdot \exp(-(y - \mu)^2/2) \cdot \int_{B} \exp\Big(-\frac{h'G(y)h}{2}\Big) \,dh \\
		& = \Big(1 + O(\delta^{-C_0(1 - \varepsilon)})\Big) \cdot \frac{\det(H(y))}{\sqrt{(2\pi)\det(\Lambda) \det(G(y))}} \cdot \exp(-(y - \mu)^2/2) \\
		& = \Big(1 + O(\delta^{-C_0(1 - \varepsilon)})\Big) \cdot \frac{\sqrt{\det(H(y))}}{\sqrt{(2\pi)\det(\nabla^2\mu)}} \cdot \exp(-(y - \mu)^2/2).
	\end{aligned}
	\end{equation*}
	The second line above uses standard properties for the first and third moment of a centered Gaussian (recorded in Section~\ref{subsec:gaussian-integrals}), recalling that $d(y,h), d(h)$ and $R(h)$ are linear and cubic in $h$, respectively. The third line uses the Gaussian concentration inequality~\eqref{eqn:gaussian-integrals-concentration}. As for the remainder terms, 
	\begin{equation*}
	\begin{aligned}
		\int_B O(h^2) \bar{\rho}(t,y) \,dt
		& = \frac{\det(H(y))}{\sqrt{(2\pi)^{d + 1}\det(\Lambda)}} \cdot \exp(-(y - \mu)^2/2) \cdot \int_{B} O(h^2) \Big(1 + d(y,h) - \frac{d(h)}{2} - \frac{R(h)}{2}\Big) \exp\Big(-\frac{h'G(y)h}{2}\Big) \\
		& = \frac{\det(H(y))}{\sqrt{(2\pi)^{d + 1}\det(\Lambda)}} \cdot \exp(-(y - \mu)^2/2) \cdot \int_{B} O(h^2 + \delta^{-2}h^4) \exp\Big(-\frac{h'G(y)h}{2}\Big) \,dh \\
		& = O(\delta^2) \cdot \frac{\sqrt{\det(H(y))}}{\sqrt{(2\pi)\det(\nabla^2 \mu)}} \cdot \exp(-(y - \mu)^2/2),
	\end{aligned}
	\end{equation*}
	where the second line follows as $d(y,h) = O(h), d(h) = O(h)$ and $R(h) = O(\delta^{-2}h^3)$ and the third line uses standard properties of growth rate of Gaussian moments as recorded in~\eqref{eqn:gaussian-integrals-moments}. In summary, we have shown that
	\begin{equation}
	\label{pf:approximate-joint-distribution-1}
		\int_{B} \bar{\rho}(t,y) \,dt = (1 + O(\Delta \delta^2) + O(\delta^2) + O(\delta^{-C_0(1 - \varepsilon)})) \cdot \frac{\sqrt{\det(H(y))}}{\sqrt{(2\pi)\det(\nabla^2 \mu)}} \cdot \exp(-(y - \mu)^2/2) \,dy.
	\end{equation}
	This will of independent use later on. For now, we note that $(y - u^+)\delta \to 0$ for all $y \in [\lambda^{-} \vee v,\lambda^{+})$. It therefore follows from~\eqref{eqn:taylor-expansion-dety} that the first-order Taylor expansion of $y \to \sqrt{\det(H(y))}$ around $y = u^{+}$ is
	\begin{equation*}
	\begin{aligned}
		\sqrt{\det(H(y))} 
		& = \sqrt{\det(H(u^+))}\sqrt{1 + (y - u^+) \tr(\{H(u^+)\}^{-1}\Lambda)  + O((y - u^+)^2\delta^2)} \\
		& = \Big(1 + O\big((y - u^+)^2\delta^2\big)\Big) \cdot  \sqrt{\det(H(u^+))} \exp\Big(\frac{(y - u^+) \tr(\{H(u^+)\}^{-1}\Lambda)}{2}\Big) \\
		& = \Big(1 + O\big((y - u^+)^2\delta^2\big)\Big) \cdot  \sqrt{\det(H(u^+))} \exp\Big(\frac{(y - \mu) \tr(\{H(u^+)\}^{-1}\Lambda)}{2}\Big) \exp\Big(\frac{(\mu - u^{+}) \tr(\{H(u^+)\}^{-1}\Lambda)}{2}\Big),
	\end{aligned}
	\end{equation*}
	with the second line following as $\sqrt{1 + x} = 1 + x/2 + O(x^2)$ as $x \to 0$. 	We proceed by plugging this back in to~\eqref{pf:approximate-joint-distribution-1}: writing $H = H(u^+), d = \tr(H^{-1}\Lambda)/2$ and $c'(u) = (\sqrt{\det(H))})/(\sqrt{(2\pi)\det(\nabla^2 \mu)}) \exp(d(\mu - u^{+}))$ for notational simplicity, we have that
	\begin{equation*}
	\label{pf:approximate-joint-distribution-2}
	\begin{aligned}
	\int_{B} \rho(t,y) \,dt
	& = c'(u) \cdot (1 + O((\Delta + 1) \delta^2) + O\big((y - u^+)^2\delta^2)) \cdot \exp\Big(-\frac{(y - \mu)^2}{2} + \frac{(y - \mu - d)}{2}\Big) \,dy \\
	& = c'(u) \cdot \Big(1 + O(((y - \mu) + 1) \delta^2) + O\big((y - u^+)^2\delta^2)\Big) \cdot \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) \,dy.
	 \end{aligned}
	\end{equation*}
	Since this applies to all $y \in [v \vee \lambda^{-}, \lambda^{+}]$, it follows that
	\begin{equation*}
		\E[N_{[v \vee \lambda^{-},\lambda^+)}(B)] = c'(u) \int_{v \vee \lambda^{-}}^{\lambda^+} \Big(1 + O(((y - \mu) + 1) \delta^2) + O\big((y - u^+)^2\delta^2)\Big) \cdot \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) \,dy.
	\end{equation*}
	The dominant term in the integral above can be related to a Gaussian survival function evaluated at $v$ by undoing the effects of truncation. For the upper truncation term, by applying Lemma~\ref{lem:ratio-gaussian-survival-functions} with $\lambda = u^{+}, \theta = \mu + d$ and $x = C\delta^{-1/3}$, and noting that $\lambda^{+} = u^{+} + x$, we can conclude that
	\begin{equation*}
		\int_{\lambda^{+}}^{\infty} \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) = \Psi(u^{+} + C\delta^{-1/3}/2 - \mu - d) \cdot O\big(\exp(-C\delta^{-2/3})\big) \leq \Psi(v - \mu - d) \cdot O\big(\exp(-C\delta^{-2/3})\big),
	\end{equation*}
	with the last inequality following upon appropriate choice of $C$ in the assumed upper bound $v \leq u + C\delta^{-1/3}$.  The lower truncation term is $0$ if $u = \lambda^{-}$ -- since then $v \geq \lambda^{-}$ -- and otherwise
	\begin{equation*}
		\int_{v}^{v \vee \lambda^{-}} \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) \leq \int_{-\infty}^{\mu - C\delta^{-1/2}} \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) \,dy = O(\exp(-C\delta^{-1})) = \Psi(v - \mu - d) \cdot O(\exp(-C\delta^{-1/3})),
	\end{equation*}
	with the last equality following from the upper bound $v \leq u + C\delta^{-1/3}$. Thus, the dominant term is 
	\begin{equation*}
		c'(u) \int_{\lambda^{-} \vee v}^{\lambda^{+}} \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) \,dy = \big(1 + O(\exp(-C\delta^{-1/3}))\big) \cdot \sqrt{2\pi} c'(u) \cdot \Psi(v - \mu - d).
	\end{equation*}
	As for the lower-order terms, applying the bounds of~\eqref{eqn:gaussian-integrals-tail-behavior} on moments of a truncated Gaussian gives
	\begin{equation*}
	 \int_{\lambda^{-} \vee v}^{\lambda^{+}} O(y - \mu) \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) \,dy \leq \int_{v}^{\infty} O(y - \mu) \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) \,dy = O\big((v - \mu)_{+} \vee 1\big) \cdot \Psi(v - \mu - d),
	\end{equation*}
	and \ag{Need to handle the second line below more carefully...}
	\begin{equation*}
	\begin{aligned}
		\int_{\lambda^{-} \vee v}^{\lambda^{+}} O((y - u^{+})^2) \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) \,dy 
		& \leq \int_{v}^{\infty} O((y - u^{+})^2) \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) \,dy \\
		& = \int_{v}^{\infty} \Big(O((v - u^{+})_{+}^2) + O((y - v)^2)\Big) \exp\Big(-\frac{(y - \mu - d)^2}{2}\Big) \,dy \\
		& = \Big(O((v - u^{+})_{+}^2) + O(1)\Big) \Psi(v - \mu - d).
	\end{aligned}
	\end{equation*}
	Combining these bounds on truncation terms and lower order terms, we arrive at an approximation for the main term:
	\begin{equation*}
		\E[N_{\lambda^{-} \vee v,\lambda^+}(B)] = \Big(1 + O((v - \mu)_{+}\delta^2) + O((v - u)^2\delta^2) + O(\delta^2)\Big) \cdot \frac{\sqrt{\det(H)}}{\sqrt{\det(\nabla^2 \mu)}} \cdot \exp(d(\mu - u^{+})) \cdot \Psi(v - \mu - d).
	\end{equation*}
	\paragraph{Completing the proof.}
	Combining our results for the main term, and upper and lower truncation term, we obtain the following a second-order accurate approximation to $\E[N_v(B)]$ for all $v \geq u, v \leq u + C\delta^{-1/3}/2$:
	\begin{equation}
	\label{eqn:expectation-counting-process}
	\begin{aligned}
		\E[N_v(B)] & = \Big(1 + O((v - \mu)_{+}\delta^2) + O((v - u)^2\delta^2) + O(\delta^2)\Big) \cdot \frac{\sqrt{\det(H(u^+))}}{\sqrt{\det(\nabla^2 \mu)}} \\
		& \times \exp\Big(\frac{(\mu - u^{+})\tr(\{H(u^+)\}^{-1}\Lambda)}{2}\Big) \cdot \Psi\Big(v - \mu - \frac{\tr(\{H(u^+)\}^{-1}\Lambda)}{2}\Big).
	\end{aligned}
	\end{equation}
	Theorem~\ref{thm:no-more-than-one-process-peak-per-signal-peak} implies each of the following:
	\begin{equation*}
	\begin{aligned}
	p(t,y) & = \big(1 - o(\exp(-c_2\delta^{-2})\big) \cdot \frac{\rho(t,y)}{\E[N_u(B)]} \\
	p(y)  & = \big(1 - o(\exp(-c_2\delta^{-2})\big) \frac{\int_{B} \rho(t,y) \,dt}{\E[N_u(B)]} \\
	p(t|y) & = \big(1 - o(\exp(-c_2\delta^{-2})\big) \frac{\rho(t,y)}{\int_{B} \rho(t,y) \,dt}.
	\end{aligned}
	\end{equation*}
	Plugging in our approximations of $\rho(t,y)$, $\int_{B} \rho(t,y) \,dy$ and $\E[N_u(B)]$ -- given in~\eqref{eqn:approximate-joint-intensity},~\eqref{pf:approximate-joint-distribution-1} and~\eqref{eqn:expectation-counting-process}, respectively -- yields the claims of Theorem~\ref{thm:approximate-joint-distribution}.
	
	\section{Proofs for Section~\ref{sec:asymptotic-distribution-randomized}}
	
	\ag{Again, this is still wip.}
	Recall that we are considering a sequence of $t_n = t_n^* + h_n$ for $h_n = o(1)$, and $y_n \geq \mu_{t_n^*} - o(\delta_{t_n^*}^{-1})$; hereafter we drop the dependence on $n$ as usual. For convenience, we copy the intensity function of Kac-Rice for randomized peaks here:
	\begin{equation*}
		\rho({\tt t},{\tt y}) = \E[\det(-JV_{{\tt t}}) \cdot \1(J V_{{\tt t}} \preceq 0)|V_{{\tt t}} = 0, U_{{\tt t}} = {\tt y}]  \cdot p_{V_{{\tt t}},U_{{\tt t}}}(0,{\tt y}).
	\end{equation*} 
	Our ultimate interest is in approximating $\rho^{B_{t^*},u}(t,y) := \int_{B_{t^*}} \int_{u}^{\infty} \rho({\tt t},{\tt y}) \,dt' \,dy'$. To do so, we will approximate $\rho({\tt t},{\tt y})$ by Taylor expansion around $t' = t$, and then integrate. \ag{For the moment we will assume $u \geq \mu_{t^*}$, so that $y' \geq \mu_{t^*}$ in the region of integration. Eventually we will need to replace this by a truncation argument.} 
	
	\subsection{Expansion of density term}
	We consider the density term $p_{V_{{\tt t}},U_{{\tt t}}}(0,{\tt y})$, which factorizes into
	\begin{equation*}
		p_{V_{{\tt t}},U_{{\tt t}}}(0,{\tt y}) = p_{Y_{t'}^s, \nabla Y_{t'}^s|Y_t,\nabla Y_t}(y',0|y,0) \cdot p_{Y_t,\nabla Y_t}(y,0).
	\end{equation*}
	We approximate the conditional density term through Taylor expansion of relevant quantities around $t' = t$. The conditional mean is
	\begin{align*}
		\E\bigg[
		\begin{pmatrix}
			Y_{t'}^s \\
			\nabla Y_{t'}^s
		\end{pmatrix}|Y_t = y, \nabla Y_t = 0\bigg] 
		& = 
		\begin{bmatrix}
			\mu_{t'} \\
			\nabla \mu_{t'}
		\end{bmatrix}
		+ 
		\begin{bmatrix}
			C(t',t) & C_{01}(t',t)' \\
			C_{10}(t',t) & C_{11}(t',t)
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0 \\
			0 & \Lambda_{t}
		\end{bmatrix}^{-1}
		\begin{bmatrix}
			(y - \mu_t) \\
			(0 - \nabla \mu_t)
		\end{bmatrix} \\
		& = 
		\begin{bmatrix}
			\mu_{t'} + (y - \mu_t)C(t',t) - C_{0,1}(t',t)' \Lambda_t^{-1} \nabla\mu_t \\
			\nabla \mu_{t'} + (y - \mu_t)C_{10}(t',t) - C_{11}(t',t)\Lambda_{t}^{-1} \nabla \mu_t
		\end{bmatrix} \\
		& = 
		\begin{bmatrix}
			y + \frac{1}{2}\eta'(\nabla^2 \mu_t - (y - \mu_t)\Lambda_t) \eta + O(\eta^3(\delta_{t^*}^{-1} \vee |y  - \mu_{t^*}|)) \\
			\nabla^2 \mu_t \eta + \frac{1}{2}\nabla^3 \mu_t(\eta^2) - (y - \mu_t) \Lambda_t \eta - \Gamma_t(\nabla\mu_t) \eta + O(\eta^3 \delta_{t^*}^{-1} \vee \eta^2|y  - \mu_{t^*}|) 
		\end{bmatrix}
		\\
		& = 
		\begin{bmatrix}
			y - \frac{1}{2}\eta^{\top}H_t(y) \eta + O(\eta^3(\delta_{t^*}^{-1} \vee |y  - \mu_{t^*}|)) \\
			-H_t(y)h+ \frac{1}{2}\nabla^3 \mu_t(\eta^2) + O(\eta^3 \delta_{t^*}^{-1} \vee \eta^2|y  - \mu_{t^*}|) 
		\end{bmatrix}.
	\end{align*}
	with the first line being definitional, the second line being algebra, the third line following from~\eqref{eqn:signal-taylor-expansion} and~\eqref{eqn:covariance-taylor-expansion-2}, and the fourth line simply being the definition of $H_t(y)$. The conditional variance is 
	\begin{align*}
		& \Var\bigg[
		\begin{pmatrix}
			Y_{t'}^s \\
			\nabla Y_{t'}^s
		\end{pmatrix}|Y_t = y, \nabla Y_t = 0\bigg] \\
		& = 
		\begin{bmatrix}
			1  + \gamma & 0 \\
			0 & (1 + \gamma) \Lambda_{t'}
		\end{bmatrix}
		- 
		\begin{bmatrix}
			C(t',t) & C_{01}(t',t)' \\
			C_{10}(t',t) & C_{11}(t',t)
		\end{bmatrix}
		\begin{bmatrix}
			1 & 0 \\
			0 & \Lambda_{t}
		\end{bmatrix}^{-1}
		\begin{bmatrix}
			C(t,t') & C_{10}(t,t')' \\
			C_{01}(t,t') & C_{11}(t,t')
		\end{bmatrix} \\
		& = 
		\begin{bmatrix}
			1 + \gamma - C(t',t) - C_{01}(t',t)'\Lambda_t^{-1}C_{01}(t,t') & 
			- C(t',t) C_{10}(t,t')' - C_{01}(t',t)' \Lambda_t^{-1}C_{11}(t,t') \\
			\cdot & (1 + \gamma) \Lambda_{t'} - C_{10}(t',t) C_{10}(t,t')' - C_{11}(t',t)\Lambda_{t}^{-1}C_{11}(t,t')
		\end{bmatrix}
		\\
		& =
		\gamma 
		\begin{bmatrix}
			1 & 0 \\
			0 & \Lambda_t + C_{12}(t,t)(\eta) + (C_{12}(t,t)(\eta))'
		\end{bmatrix}
		+ O(\eta^2) \\
		& := 
		\gamma 
		\begin{bmatrix}
			1 & 0 \\
			0 & \Lambda_t + \dot{\Lambda}_t(\eta)
		\end{bmatrix}
		+ O(\eta^2),	
	\end{align*}
	with the first line being definitional, the second line being algebra, and the third and final lines following from~\eqref{eqn:covariance-taylor-expansion-2}; in the last line we have written $\dot{\Lambda}_t(\eta) = C_{12}(t,t)(\eta) + (C_{12}(t,t)(\eta))'$. Thus conditional on $Y_t,\nabla Y_t$ the pair $Y_{t'}^s,\nabla Y_{t'}^s$ are approximately independent, with
	\begin{align*}
		& p_{Y_{t'}^s, \nabla Y_{t'}^s|Y_t,\nabla Y_t}(y',0|y,0) 
		= 
		\Big(1 + O\big(\psi_0(h,\eta,y,y')\big)\Big) \cdot \tilde{p}_{Y_{t'}^s}(y'|y,0) \cdot \tilde{p}_{\nabla Y_{t'}^s}(0|y,0) \\
		& \tilde{p}_{Y_{t'}^s}(y'|y,0) 
		:= 
		\frac{1}{\sqrt{2 \pi \gamma}} \exp\Big(-\frac{1}{2\gamma}\big(y' - y + \frac{1}{2}\eta'H_t(y)\eta\big)^2\Big) \\
		& \tilde{p}_{\nabla Y_{t'}^s}(0|y,0) 
		:= 
		\frac{1}{\sqrt{(2 \pi \gamma)^d \det(\Lambda_t + \dot{\Lambda}_t(\eta))}} \exp\Big(-\frac{1}{2\gamma}(-H_t(y)\eta + \frac{1}{2}\nabla^3\mu_t(\eta^2))'\{\Lambda_t + \dot{\Lambda}_t(\eta)\}^{-1}(-H_t(y)\eta + \frac{1}{2}\nabla^3\mu_t(\eta^2))\Big).
	\end{align*}
	where $\psi_0(h,\eta,y,y') = \eta^3(1 \vee |y' - y|)(\delta_{t^*}^{-1} \vee |y - \mu_{t^*}|) \vee \eta^4 \delta_{t^*}^{-2}(1 \vee |y - \mu_{t^*}|)$. Both of the approximate density terms can be further simplified. For the height, expanding the square gives
	\begin{equation*}
		\big(y' - y + \frac{1}{2}\eta'H_t(y)\eta\big)^2
		= (y' - y)^2 + (y' - y)\eta'H_t(y)\eta + O(\eta^4 \delta_{t^*}^{-2}),
	\end{equation*}
	and therefore the density is 
	\begin{equation}
		\label{eqn:randomized-kac-rice-density-height}
		\begin{aligned}
			\tilde{p}_{Y_{t'}^s}(y'|y,0) 
			& = \big(1 + O(\psi_0(h,\eta,y,y'))\big) \cdot \bar{p}_{Y_{t'}^s}(y'|y,0),  \\
			\bar{p}_{Y_{t'}^s}(y'|y,0) 
			& := \frac{1}{\sqrt{2 \pi \gamma}}\exp\Big(-\frac{1}{2\gamma}(y' - y)^2\Big)\exp\Big(\frac{(y' - y)}{2\gamma}\eta'H_t(y)\eta\Big).
		\end{aligned}
	\end{equation} 
	The approximate density $\tilde{p}_{\nabla Y_{t'}^s}(y'|y,0)$ can also be simplified: Taylor expansions give
	\begin{equation*}
		\{\Lambda_t + \dot{\Lambda}_t(\eta)\}^{-1} = \Lambda_t^{-1}\big(I - \bar{\Gamma}_{t}(\eta)\big) + O(\eta^2),
	\end{equation*}
	and
	\begin{equation*}
		\det(\Lambda_t + \dot{\Lambda}_t(\eta)) = \det(\Lambda_t)\Big(1 + \tr\big(\Lambda_t^{-1}\dot{\Lambda}_{t}(\eta)\big) + O(\eta^2)\Big).
	\end{equation*}
	Writing $\bar{\Gamma}_t(\eta) = \Lambda_t^{-1}\dot{\Lambda}_t(\eta)$, we conclude
	\begin{align}
		& \tilde{p}_{\nabla Y_{t'}^s}(y'|y,0) \\
		& \quad = \frac{1 + O(\eta^2 \vee \eta^4 \delta_{t^*}^{-2} \vee \eta^4 |y  -\mu_{t^*}|^2)}{\sqrt{(2 \pi \gamma)^d \det(\Lambda_t)(1 + \tr(\bar{\Gamma}_t(\eta)))}} \exp\Big(-\frac{1}{2\gamma}\big(-H_t(y)\eta + \frac{1}{2}\nabla^3\mu_t(\eta^2)\big)'\Lambda_{t}^{-1}(I - \bar{\Gamma}_t(\eta))\big(-H_t(y)\eta + \frac{1}{2}\nabla^3\mu_t(\eta^2)\big)\Big) \nonumber \\
		& \quad = \frac{1 + O(\eta^2 \vee \eta^4 \delta_{t^*}^{-2} \vee \eta^4 |y  -\mu_{t^*}|^2)}{\sqrt{(2 \pi \gamma)^d \det(\Lambda_t)}} \exp\Big(-\frac{1}{2\gamma}\big(-H_t(y)\eta + \frac{1}{2}\nabla^3\mu_t(\eta^2)\big)'\Lambda_{t}^{-1}(I - \bar{\Gamma}_t(\eta))\big(-H_t(y)\eta + \frac{1}{2}\nabla^3\mu_t(\eta^2)\big)\Big) \exp\Big(-\frac{\tr(\bar{\Gamma}_t(\eta))}{2}\Big) \nonumber \\
		& \quad = \frac{1 + O(\eta^2 \vee \eta^4 \delta_{t^*}^{-2} \vee \eta^4 |y  -\mu_{t^*}|^2)}{\sqrt{(2 \pi \gamma)^d \det(\Lambda_t)}} \exp\Big(-\frac{1}{2\gamma} \eta' H_{t}(y)\Lambda_t^{-1}H_t(y)\eta + L(\eta) + R(\eta^3)\Big) \nonumber \\
		& \quad := \Big(1 + O(\eta^2 \vee \eta^4 \delta_{t^*}^{-2} \vee \eta^4 |y  -\mu_{t^*}|^2)\Big) \cdot \bar{p}_{\nabla Y_{t'}^s}(0|y,0), \label{eqn:randomized-kac-rice-density-gradient}
	\end{align}
	where $L(\eta) = \frac{1}{2}\tr(\bar{\Gamma}_t(\eta))$ is linear in $\eta$, and 
	$$
	R(\eta^3) = \frac{1}{2\gamma}\Big(\eta'H_{t}(y)\bar{\Gamma}_t(\eta)H_t(y)\eta + \nabla^3\mu_t(\eta^2)'\Lambda_t^{-1}H_t(y)\eta\Big)
	$$ is cubic in $\eta$.
	
	\subsection{Expansion of determinant term}
	We begin by verifying that the negative definite indicator is exponentially ignorable. Keeping in mind that $J V_{{\tt t}}$ is block diagonal with blocks $-\nabla^2 Y_t$ and $-\nabla^2 Y_{t'}^s$, and therefore $\det(J V_{{\tt t}}) = \det(\nabla^2 Y_t) \cdot \det(\nabla^2 Y_{t'}^s)$, we have
	\begin{equation}
		\label{pf:approximation-peak-intensity-1}
		\begin{aligned}
			& \bigg|\E\Big[\det(-J V_{\tt t}) \cdot \1(JV_{\tt t} \preceq 0)|U_{\tt t} = {\tt y},V_{\tt t} = 0\Big] - \E\Big[\det(-J V_{\tt t})|U_{\tt t} = {\tt y},V_{\tt t} = 0\Big]\bigg| \\
			& \quad = \bigg|\E\Big[\det(-J V_{\tt t}) \cdot \1(JV_{\tt t} \not\preceq 0)|U_{\tt t} = {\tt y},V_{\tt t} = 0\Big]\bigg| \\
			& \quad \leq \Big\{\E\Big[|\det(\nabla^2 Y_t)|^{p_1}|U_{\tt t} = {\tt y},V_{\tt t} = 0\Big]\Big\}^{1/p_1} \cdot \Big\{\E\Big[|\det(\nabla^2 Y_{t'}^s)|^{p_2}|U_{\tt t} = {\tt y},V_{\tt t} = 0\Big]\Big\}^{1/p_2} \\
			& \quad \times \Big\{\P\Big(\nabla^2 Y_{t} \not\preceq 0|U_{\tt t} = {\tt y},V_{\tt t} = 0\Big)\Big\}^{1/q_1} \cdot \Big\{\P\Big(\nabla^2 Y_{t} \not\preceq 0|U_{\tt t} = {\tt y},V_{\tt t} = 0\Big)\Big\}^{1/q_2} \\
			& \quad = O\Big(\delta_{t^*}(y)^{-d} \delta_{t^*}(y')^{-d} \exp(-c_1(\delta_{t^*}(y)^{-2} + \delta_{t^*}(y')^{-2}))\Big),
		\end{aligned}
	\end{equation}
	with the inequality holding for any conjugate exponents $1/p_1 + 1/p_2 + 1/q_1 + 1/q_2 = 1$, and the final line following from~\eqref{eqn:asymptotics-hessian-determinant-moments-2},~\eqref{eqn:asymptotics-hessian-negative-definite} and~\eqref{eqn:randomized-kac-rice-hessian-moments-2}, with $c_1 = c_0/\min(q_1,q_2)$. 
	
	We now use the decomposition in~\eqref{eqn:hessian-decomposition} to write
	\begin{equation*}
		\E\Big[\det(-JV_{\tt t})|U_{{\tt t}} = {\tt y}, V_{{\tt t}} = 0\Big] = \E\Big[\det\big(J_{{\tt t}}({\tt y}) + R_t\big)\Big],
	\end{equation*}
	where we recall the notation $J_{{\tt t}}({\tt y}) = \E[-JV_{\tt t}|U_{{\tt t}} = {\tt y}, V_{{\tt t}} = 0]$ and $R_t = R(-JV_{\tt t};W_{{\tt t}})$. Applying the representation of determinant in~\eqref{eqn:representation-determinant} and using linearity of expectation gives
	\begin{align*}
		\E[\det(J_{\tt t}(y) + R_t)] = \sum_{k = 0}^{2d} \sum_{\mc{P}} \omega(p) \sum_{s \in \mc{S}_k} \E[(R_t)_{sp_{s}}] J_{{\tt t}}({\tt y})_{s^cp_{s^c}} =: \sum_{k = 0}^{2d} D_k.
	\end{align*}
	The leading order term above is the determinant of the conditional expectation of the Jacobian: by~\eqref{eqn:randomized-kac-rice-det-jacobian},
	$$
	D_0 = \det(J_{\tt t}({\tt y})) = \det(H_t(y)) \cdot \det(H_{t'}(y')) \cdot \Big(1 + O\Big(\eta\{\delta_{t^*}(y)\}|y - \mu_{t^*}| \vee \eta\{\delta_{t^*}(y)\}|y - y'| + \eta^2 + h^2\Big)\Big).
	$$ 
	Noting that $R_t$ is a mean-zero Gaussian, it follows that $\E[(R_t)_{s^cp_{s^c}}] = 0$ for any $s \in \mc{S}_k$ with $k$ odd, so that the contributions of all such terms vanish in the expected determinant, and in particular $D_1 = 0$. On the other hand, it follows from~\eqref{eqn:deterministic-hessian-pointwise} that
	$(J_{\tt t}({\tt y}))_{sp_s} = O(\delta_{t}(y)^{-|s|})$; since $\E[(R_t)_{sp_s}] = O(1)$, we conclude that $D_k = O(\{\delta_{t}(y)\}^{-|d - k|})$ for all $k = 2,\ldots,d$. In summary,
	\begin{equation*}
		\E[\det(J_{\tt t}({\tt y}) + R_{\tt t})] = \det(H_t(y)) \cdot \det(H_{t'}(y')) \cdot \Big(1 + O\Big(\eta\{\delta_{t^*}(y)\}|y - \mu_{t^*}| \vee \eta\{\delta_{t^*}(y)\}|y - y'| + \eta^2 + h^2 + \{\delta_{t^*}(y)\}^{2}\Big)\Big)
	\end{equation*}
	Finally, a Taylor expansion of $\det(H_{t'}(y'))$ around $t' = t$ as in \eqref{eqn:taylor-expansion-det} gives
	\begin{equation*}
		\det(H_{t'}(y')) = \det(H_t(y')) \cdot \Big(1 + \tr\big(\bar{\Gamma}_t(y)(\eta)\big) + O(\eta^2)\Big),
	\end{equation*}
	where $\bar{\Gamma}_t(y)(\eta) = \{H_t(y)\}^{-1}(\dot{H}_t(y))(\eta)$. Combining this with \red{previous equations}, we conclude that 
	\begin{equation*}
		\begin{aligned}
			& \E\Big[\det(-J V_{\tt t}) \cdot \1(JV_{\tt t} \preceq 0)|U_{\tt t} = {\tt y},V_{\tt t} = 0\Big] 
			\\
			& = \det(H_t(y)) \det(H_{t}(y')) \Big(1 + \tr\big(\bar{\Gamma}_t(y)(\eta)\big)\Big) \cdot \Big(1 + O\Big(\eta\{\delta_{t^*}(y)\}|y - \mu_{t^*}| \vee \eta\{\delta_{t^*}(y)\}|y - y'| + \eta^2 + h^2 + \{\delta_{t^*}(y)\}^2\Big)\Big).
		\end{aligned}
	\end{equation*}
	
	\subsection{Marginalize over randomized critical points}
	Combining the results of the previous two sections, we have the following approximation to $\rho({\tt y},{\tt t})$:
	\begin{equation*}
		\begin{aligned}
			\rho({\tt y},{\tt t}) & = \big(1 + O(\psi_2(\eta,h,y,y'))\big) \bar{\rho}({\tt y},{\tt t}) \\
			\bar{\rho}({\tt y},{\tt t}) & := \frac{\det(H_t(y))\det(H_t(y'))(1 + \tr\big(\bar{\Gamma}_t(y)(\eta)\big))}{\sqrt{(2 \pi \gamma)^{d + 1} \det(\Lambda_t)}} \exp\Big(-\frac{1}{2\gamma}(y' - y)^2\Big) \exp\Big(-\frac{1}{2\gamma}h'H_t(y')\Lambda_{t}^{-1}H_t(y)h + L(h) + R(h)\Big) \cdot p_{Y_t,\nabla Y_t}(y,0).
		\end{aligned}
	\end{equation*}
	The relative error is on the order of 
	\begin{align*}
		& \psi_2(\eta,h,y,y') := \eta^2 \vee h^2 \vee \eta^4 \delta_{t^*}^{-2} \vee |y' - y||y - \mu_{t^*}| \eta^2 \vee |y' - y|\eta^3 \delta_{t^*}^{-1} \\
		& \quad \vee \eta^4 |y - \mu_{t^*}|^2 \vee \eta\{\delta_{t^*}(y)\}|y - \mu_{t^*}| \vee \eta\{\delta_{t^*}(y)\}|y - y'| \vee \{\delta_{t^*}(y)\}^2,
	\end{align*} 
	uniformly over all $\eta,h = t^* + o(1)$ and $y,y' \geq \mu_{t^*} - o(\delta_{t^*}^{-1})$. Integrating this approximation over $B_{t^*}$ gives
	\begin{equation*}
		\begin{aligned}
			\int_{B_{t^*}} \rho({\tt y},{\tt t}) \,dt' 
			& = \int_{B_{t^*}} \big(1 + O(\psi_2(\eta,h,y,y'))\big) \bar{\rho}({\tt y},{\tt t}) \,dt' \\
			& \red{= (1 + \psi_3(h,y,y'))\frac{\sqrt{\det(H_t(y)) \cdot \det(H_t(y'))}}{\sqrt{2 \pi \gamma}} \exp\Big(-\frac{1}{2\gamma}(y' - y)^2\Big) \cdot p_{Y_t,\nabla Y_t}(y,0)}
		\end{aligned}
	\end{equation*}
	with relative error $\red{\psi_3(h,y,y') = \delta_{t^*}^2(1 \vee |y' - y|^2 \vee |y - \mu_{t^*}|^2) \vee h^2}$. The next step is to Taylor expand $\det(H_{t}(y'))$ -- treated as a function of $y'$ -- about $y' = y$, giving
	\begin{equation*}
		\det\big(H_{t}(y')\big) = \det\big(H_t(y) + (y' - y)\Lambda_t\big) = \det\big(H_t(y)\big) \cdot \Big(1 + (y' - y)\tr(\{H_t(y)\}^{-1}\Lambda_t) + O(|y - y'|^2\delta_{t^*}^2)\Big),
	\end{equation*}
	by~\eqref{eqn:matrix-det-taylor-expansion}. Plugging this back into our previous expression gives
	\begin{align*}
		& \big(1 + O(\psi_3(\eta,h,y,y'))\big) \cdot \frac{\det(H_t(y))}{\sqrt{2\pi \gamma}} \exp\Big(-\frac{1}{2\gamma}(y'  - y)^2 + \frac{(y' - y)}{2}\tr(\{H_t(y)\}^{-1}\Lambda_t)\Big) \cdot p_{Y_t,\nabla Y_t}(0),
	\end{align*}
	where we have used the approximation $1 + x = \exp(x)(1 + O(x^2))$ as $x \to 0$. \ag{But we are applying the approximation for some $x$ that may not be small...} Integrating over $y' \in [u,\infty)$, this becomes
	\begin{equation*}
		\int_{u}^{\infty} \int_{B_{t^*}} \rho({\tt y},{\tt t}) \,dy' \,dt' \approx \Psi\bigg(\frac{u - y - \gamma\tr(\{H_t(y)\}^{-1}\Lambda_t)}{\sqrt{\gamma}}\bigg) \cdot \det(H_t(y)) \cdot p_{Y_t,\nabla Y_t}(y,0).
	\end{equation*}
	Taylor expansion of the function $\tr(\{H_t(y)\}^{-1}\Lambda_t)$ about $y = u_{\gamma}, t = t^*$, combined with standard bounds on the tails of a Gaussian survival function, should then imply that this is close to
	\begin{equation}
		\label{eqn:randomized-kac-rice-marginalize-2}
		\Psi\bigg(\frac{u - y - \gamma\tr(\{H_{t^*}(u_{\gamma})\}^{-1}\Lambda_{t^*})}{\sqrt{\gamma}}\bigg) \cdot \det(H_t(y)) \cdot p_{Y_t,\nabla Y_t}(y,0).
	\end{equation}
	Finally, applying previously derived approximations to $\det(H_t(y))$ and $p_{Y_t,\nabla Y_t}(y,0)$, we conclude that $\rho^{B_{t^*},u}(y,t)$ is approximately
	\begin{equation*}
		\Psi\bigg(\frac{u - y - \gamma\tr(\{H_{t^*}(u_{\gamma})\}^{-1}\Lambda_{t^*})}{\sqrt{\gamma}}\bigg) \cdot \bar{\rho}(y,t).
	\end{equation*}
	
	\section{Technical Results}
	
	\subsection{Matrix calculus}
	Let $A,E$ be matrices, with $\|A^{-1}\| = O(1)$. The first-order Taylor expansion of $(A + E)^{-1}$ about $E = 0$ is
	\begin{equation}
		\label{eqn:matrix-inverse-taylor-expansion}
		(A + E)^{-1} = A - A^{-1} E A^{-1} + O\|E\|^2).
	\end{equation}
	The first order Taylor expansion of $\det(A + E)$ about $E = 0$ is
	\begin{equation}
		\label{eqn:matrix-det-taylor-expansion}
		\det(A + E) = \det(A)(1 + \tr(A^{-1}E)) + O(\|E\|^2). 
	\end{equation}
	
	\subsection{Representation of determinant}
	The following is a simple consequence of the definition of determinant (see e.g. RFG): for any matrices $A = M + B$, we can write
	\begin{equation}
		\label{eqn:representation-determinant}
		\begin{aligned}
			\det(A) 
			& = \sum_{\mc{P}} \omega(p) A_{1p_1} \cdots A_{dp_d} \\
			& = \sum_{\mc{P}} \omega(p) \sum_{s \in \{0,1\}^d} M_{sp_s} B_{s^cp_{s^c}} \\
			& = \sum_{\mc{P}} \omega(p) \sum_{k = 1}^{d} \sum_{s \in \mc{S}_k} M_{sp_s} B_{s^cp_{s^c}}
		\end{aligned}
	\end{equation}
	where $\mc{P}$ is the set of all permutations of $(1,\ldots,d)$, $\omega(p) = +1$ or $-1$ depending on the order of $p$, and for $p \in \mc{P}$, $s \in \{0,1\}^d$, the notation $A_{sp_s} = \prod_{j \in s} A_{jp_j}$, the notation $s^c = (1,\ldots,1) - s$, and finally $\mc{S}_k \subseteq \{0,1\}^d$ contains all $s \in \{0,1\}^d$ such that $\sum_{i = 1}^{d} s_i = k$.
	
	\subsection{Maximum of a Gaussian process}
	We will use the \emph{Borell-TIS inequality}, as recorded in \red{Taylor and Adler}: for a mean-zero Gaussian process $X_t$ bounded a.s. over $A$, we have that for all $\varepsilon > 0$, as $u \to \infty$,
	\begin{equation}
		\label{eqn:borell-tis}
		\P\Big(\sup_{t \in A} X_t \geq u\Big) = O(\exp(\varepsilon u^2 - u^2/2\sigma_{A}^2)), 
	\end{equation}
	where $\sigma_{A}^2 := \sup \Var[X_t]$.  
	
	\subsection{Moments of Determinant of a Gaussian Matrix}
	The following Lemma, which follows immediately from~\eqref{eqn:representation-determinant}, bounds the moments of the determinant of a Gaussian matrix with growing mean and constant variance.
	\begin{lemma}
		\label{lem:determinant-moments}
		Consider a sequence of Gaussian random matrices $G_n \in \R^{d \times d}$ with $\Var[G_n] = \Sigma$ constant in $n$. Then for all $p \geq 1$,
		\begin{equation*}
			\E[\det(G_n)^{p}] = O\Big(\E[\|G_n\|_F^{dp}]\Big) = O\Big(1 \vee \|\E[G_n]\|_{F}^{dp}\Big).
		\end{equation*} 
	\end{lemma}
	
	\subsection{Gaussian integrals}
	\label{subsec:gaussian-integrals}
	Consider a multivariate Gaussian $Z \sim N(0,G^{-1})$, and let $B$ be a ball centered at $0$ of radius $r$. For any vector $x \in \Rd$ and array $A \in \R^{d \times d \times d}$,
	\begin{equation}
	\label{eqn:gaussian-integrals-truncated-moments}
		\E\Big[(\sum x_i Z_i) \cdot \1(\|Z\| \leq r)\Big] = 0, \quad \E\Big[(\sum A_{ijk} Z_i Z_jZ_k) \cdot \1(\|Z\| \leq r)\Big] = 0.
	\end{equation}
	Now take $r = C_0 \delta \sqrt{\log(1/\delta)}$ and suppose $\{\lambda_{\min}(G)\}^{-1} = O(\delta^2)$ as $\delta \to 0$. Clearly,
	\begin{equation}
		\label{eqn:gaussian-integrals-moments}
		\E\big[\|Z\|^2\big] = O(\delta^2), \quad \E\big[\|Z\|^4\big] = O(\delta^4).
	\end{equation} 
	Moreover, application of Borell-TIS shows that for any $\varepsilon > 0$,
	\begin{equation}
	\label{eqn:gaussian-integrals-concentration}
		\P(Z \in B) = \P\big(\sup_{w \in \S^{d - 1}} w^{\top} Z \leq r\big) = 1 - O(\delta^{-C(1 - \varepsilon)}).
	\end{equation}

	\paragraph{Gaussian tail behavior.}
	Now suppose $Z \sim N(m,1)$, and consider truncating $Z$ at threshold $\lambda$. We have
	\begin{equation}
	\begin{aligned}
		\label{eqn:gaussian-integrals-tail-behavior}
		\E[(Z - m) \cdot \1(Z \geq \lambda)] 
		& = \Big(O(|\lambda - m|) + O(1)\Big) \cdot \Psi(\lambda - m) \\
		\E[(Z - \lambda)^2 \cdot \1(Z \geq \lambda)]
		& = O(\Psi(\lambda - m))
	\end{aligned}
	\end{equation}
	
	\subsection{Gaussian tail behavior}
	We will use the following asymptotics of \emph{Mills ratio} frequently: as $a_n \to \infty$,
	\begin{equation}
		\label{eqn:mills-ratio-asymptotics}
		\Psi(a_n) = \frac{\phi(a_n)}{a_n}(1 + O(a_n^{-2})).
	\end{equation}
	The following Lemma is a consequence of~\eqref{eqn:mills-ratio-asymptotics}.
	\begin{lemma}
	\label{lem:ratio-gaussian-survival-functions}
		For any $\lambda,m,x \in \R$ such that $\lambda \geq m - O(1)$ as $x \to \infty$, we have
		\begin{equation}
		\label{eqn:ratio-gaussian-survival-functions}
			\frac{\Psi(\lambda - m + x)}{\Psi(\lambda - m + x/2)} = O(\exp(-Cx^2)).
		\end{equation}
	\end{lemma}
	
	
	Additionally, for any $p \geq 1$, \ag{Needs a reference or proof.}
	\begin{equation}
		\label{eqn:gaussian-tail-polynomial-term}
		\int_{a_n}^{\infty} (x - a_n)^{p} \phi(x) \,dx = O(a_n^{p - 1} \Psi(a_n)).
	\end{equation}
	Finally, the following Lemma bounds the relative error in perturbations of the Gaussian survival function. Note that by taking $\epsilon = \delta/x$ in the Lemma, we can recover that the normalized overshoot $W = x(Z - x)$, conditional on $Z \sim N(0,1)$ being at least $x$, has $\Exp(1)$ limiting distribution. \ag{References, and explain how the error here is smaller than previous results when $\epsilon \to 0$ faster than $1/x$.}
	\begin{lemma}
		\label{lem:gaussian-survival-function-perturbation}
		For $x \to \infty, \epsilon \to 0$,
		\begin{equation*}
			\frac{|\Psi(x + \epsilon) - \Psi(x) \exp(-x\epsilon)|}{\Psi(x) \exp(-x\epsilon)} \leq C\Big(\frac{\epsilon}{x} + (1 - \exp(-\epsilon^2/2))\Big) = O(\epsilon).
		\end{equation*}
	\end{lemma}
	\begin{proof}
		\ag{Add Jon's proof.}
	\end{proof}
\end{document}