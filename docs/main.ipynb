{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18338cc5-fbc3-445f-9614-e7c5025d332d",
   "metadata": {},
   "source": [
    "# Inference for $\\hat{\\theta}=\\eta'Z$ conditioned on selection of $\\bar{Z}=Z+\\omega$\n",
    "\n",
    "Suppose $Z \\sim N(\\zeta^*, \\Sigma)$ and $\\omega|Z \\sim N(0, \\bar{\\Sigma})$. We're interested in conditional inference \n",
    "on a target $\\hat{\\theta}=\\eta'Z$ based on the joint law of $(Z,\\omega)$ truncated to $\\{(Z,\\omega):A(Z+\\omega) \\leq b\\}$. We could consider\n",
    "some soft truncation of the form $\\pi(Z+\\omega)$.\n",
    "but affine constraints are perhaps the most tractable.\n",
    "\n",
    "As in {cite}`LeeLasso` we will condition on additional information to make the distribution tractable and to eliminate\n",
    "dependence on nuisance parameters. This means we are essentially forced to condition on\n",
    "$$\n",
    "N = Z - \\Sigma \\eta (\\eta'\\Sigma \\eta)^{-1}\\hat{\\theta} = Z - \\Gamma \\hat{\\theta} = RZ.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba17f64-3d06-4a15-80d3-6f068f6851af",
   "metadata": {},
   "source": [
    "We can condition on more if convenient of course. Conditioning on too much leads to intervals that \n",
    "may be too short even if selection is trivial. For instance, we can condition on $Z+\\omega$ (this is\n",
    "\"data splitting / thinning\" but then we are left only with the \"second half\" of the data to form unbiased estimates.\n",
    "\n",
    "This second half is easily seen to be\n",
    "$$\n",
    "Z - \\Sigma \\bar{\\Sigma}^{-1}\\omega\n",
    "$$\n",
    "with variance\n",
    "$$\n",
    "\\Sigma + \\Sigma \\bar{\\Sigma}^{-1}\\Sigma > \\Sigma.\n",
    "$$\n",
    "Hence the variance of the \"thinned\" estimator is\n",
    "$$\n",
    "\\eta'\\Sigma \\eta + \\eta'\\Sigma \\bar{\\Sigma}^{-1}\\Sigma \\eta.\n",
    "$$\n",
    "\n",
    "Let's verify the independence of $Z+\\omega$ and $Z-\\Sigma \\bar{\\Sigma}^{-1}\\omega$\n",
    "$$\n",
    "\\text{Cov}(Z+\\omega, Z-\\Sigma \\bar{\\Sigma}^{-1}\\omega) = \\Sigma - \\bar{\\Sigma}\\bar{\\Sigma}^{-1}\\Sigma = 0.\n",
    "$$\n",
    "\n",
    "In general, we can condition on anything linear function of $(Z+\\omega)$ (assuming invertibility of $\\Sigma, \\bar{\\Sigma}$ where necessary). \n",
    "But which contrasts will keep the nominal variance under trivial selection the same as $\\text{Var}(\\hat{\\theta})$?\n",
    "\n",
    "It is not hard to see that conditioning on any linear contrast $v'(Z+\\omega)$ such that $\\text{Cov}(v'(Z+\\omega), \\hat{\\theta} | N)=0$ preserves\n",
    "this \"best case variance\". Well, as $\\text{Cov}(\\hat{\\theta},N)=0$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Cov}(v'(Z+\\omega), \\hat{\\theta}|N) &= \\text{Cov}(v'(Z+\\omega), \\hat{\\theta})\\\\\n",
    "&= \\text{Cov}(v'Z, \\hat{\\theta}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We see then, that the *maximal* contrast of $Z+\\omega$ we can condition on without affecting this \"best\" case variance is $R(Z+\\omega)$ where \n",
    "$N=RZ$. Of course, conditioning on $\\sigma(RZ, R(Z+\\omega))$ is equivalent to conditioning on $\\sigma(RZ,R\\omega)$.\n",
    "\n",
    "Conditioning on this pair of vectors restricts variation in our original joint law of $(Z,\\omega)$ to a 2-dimensional affine plane\n",
    "with \"linear\" part given by $\\eta'Z$ and $c'\\omega$ where $c$ is chosen such that $\\text{Cov}(c'\\omega, R\\omega) = c'\\bar{\\Sigma}R'=0$. Of course, $c$ is defined\n",
    "only up to scaling which we can choose.  We can (and will) take\n",
    "$$\n",
    "c = \\bar{\\Sigma}^{-1}\\Sigma \\eta.\n",
    "$$\n",
    "\n",
    "Let's verify this claim\n",
    "$$\n",
    "\\begin{aligned}\n",
    "c'\\bar{\\Sigma}R' &= c'\\bar{\\Sigma}(I - (\\eta'\\Sigma\\eta)^{-1}\\eta\\eta'\\Sigma) \\\\\n",
    "&= \\eta'\\Sigma \\bar{\\Sigma}^{-1} \\bar{\\Sigma}(I - (\\eta'\\Sigma\\eta)^{-1}\\eta\\eta'\\Sigma) \\\\\n",
    "&= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The computational complexity here is the cost of solving for $c$ in a linear system $\\bar{\\Sigma}c=\\Sigma \\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4f1579-fa19-44b3-a08d-bc15be1f1893",
   "metadata": {},
   "source": [
    "#### Decomposition of $\\omega$ given $c$\n",
    "\n",
    "Having computed $c$, we can decompose $\\omega$ as\n",
    "$$\n",
    "\\omega - \\bar{\\Gamma} \\cdot c'\\omega + \\bar{\\Gamma} \\cdot c'\\omega\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\bar{\\Gamma} = (c'\\bar{\\Sigma}c)^{-1} \\text{Cov}(\\omega, c'\\omega) = (\\eta'\\Sigma \\bar{\\Sigma}^{-1} \\Sigma \\eta)^{-1} \\Sigma \\eta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ab322",
   "metadata": {},
   "source": [
    "#### Well-specified assumption\n",
    "\n",
    "In certain scenarios, such as classical data splitting or other resampling and inference procedures such as {cite}`AssumptionLean` it may be the case that for some $\\gamma > 0$\n",
    "$$\n",
    "\\bar{\\Sigma} = \\gamma^2 \\cdot \\Sigma.\n",
    "$$\n",
    "**In this case we can take $c=\\eta$ which implies $\\bar{\\Gamma}=\\Gamma$.**\n",
    "\n",
    "### Reduced affine constraints\n",
    "\n",
    "Given $c$, we'll write $\\bar{\\omega}=c'\\omega$, a centered univariate Gaussian with variance $c'\\bar{\\Sigma}c=\\eta'\\Sigma \\bar{\\Sigma}^{-1}\\Sigma \\eta = \\bar{s}^2$. We can always write\n",
    "$$\n",
    "\\omega = \\left(\\omega - (c'\\bar{\\Sigma}c)^{-1}\\bar{\\Sigma}c \\cdot \\bar{\\omega} \\right) + (c'\\bar{\\Sigma}c)^{-1}\\bar{\\Sigma}c \\cdot \\bar{\\omega} = \\bar{N} + \\bar{\\Gamma} \\bar{\\omega}\n",
    "$$\n",
    "with $\\bar{N}$ a linear functional of $R\\omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b545aae-caf8-4bf5-8e43-db8725e330bd",
   "metadata": {},
   "source": [
    "## Data splitting estimator\n",
    "\n",
    "The \"sample splitting\" estimator of {cite}`TianTaylor`, sometimes referred to\n",
    "as the \"data thinning\" estimator is found by constructing the unbiased estimate after conditioning on the full $(Z+\\omega)$. This can be done \n",
    "using the corresponding \"half\" of $Z+\\omega$, i.e. $Z-\\Sigma \\bar{\\Sigma}^{-1}\\omega$.\n",
    "\n",
    "\n",
    "\n",
    "Hence, our data splitting estimator is\n",
    "$$\n",
    "\\cup{\\theta} = \\eta'(Z - \\Sigma \\bar{\\Sigma}^{-1}\\omega) = \\hat{\\theta} - \\bar{\\omega}\n",
    "$$\n",
    "with variance (as expected from above)\n",
    "$$\n",
    "\\eta'\\Sigma \\eta + \\eta' \\Sigma \\bar{\\Sigma}^{-1}\\Sigma \\eta.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3671ca1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import numpy as np\n",
    "from lassoinf.selective_inference import SelectiveInference\n",
    "\n",
    "# Compute the required parameters for inference\n",
    "print(inspect.getsource(SelectiveInference.compute_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d54888-d14a-4142-8f48-10fc4a3362f8",
   "metadata": {},
   "source": [
    "We have decomposed the joint law $(Z,\\omega)$ into 4 independent pieces $(\\hat{\\theta}, N, \\bar{\\omega}, \\bar{N})$ such that $\\text{Var}(\\hat{\\theta} | N, \\bar{N}) = \\text{Var}(\\hat{\\theta})$. Hence,\n",
    "when selection is trivial the corresponding confidence intervals and $p$-values for testing $H_0:\\eta'\\zeta^*=\\theta_0$ will be essentially as if we had done no selection.\n",
    "\n",
    "Our selection event can be rewritten as\n",
    "$$\n",
    "\\left\\{(\\hat{\\theta}, N, \\bar{\\omega}, \\bar{N}): A\\left(N + \\Gamma \\hat{\\theta} + \\bar{N} + \\bar{\\Gamma} \\bar{\\omega}\\right) \\leq b \\right\\}.\n",
    "$$\n",
    "Equivalently, fixing $(N,\\bar{N})$ at observed values $(N_o, \\bar{N}_o)$ this is\n",
    "$$\n",
    "\\left\\{(\\hat{\\theta}, \\bar{\\omega}): A (\\Gamma \\hat{\\theta} + \\bar{\\Gamma} \\bar{\\omega}) \\leq b - A(N_o + \\bar{N}_o) \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6e1cd-6ea6-4cd6-8d63-e6a74588a463",
   "metadata": {},
   "source": [
    "### Post-selection density\n",
    "\n",
    "Fixing $\\hat{\\theta}$ at some nominal density argument $t$ we can now compute the selective adjustment to the marginal law $N(\\eta'\\zeta^*, \\eta'\\Sigma \\eta)$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t &\\mapsto \\mathbb{P} \\left( \\bar{A} \\bar{\\omega}\\leq \\bar{b}(N,\\bar{N}, \\hat{\\theta}) \\biggl| \\hat{\\theta}=t, N=N_o, \\bar{N}=N_o\\right) \\\\\n",
    "&= \\Phi(U(N_o, \\bar{N}_o, t)/\\bar{s}) - \\Phi(L(N_o, \\bar{N}_o, t)/\\bar{s}).\n",
    "\\end{aligned}\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\bar{b}(N,\\bar{N},\\hat{\\theta}) = b - A( N +  \\bar{N} + \\Gamma \\hat{\\theta}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the truncation interval [L, U]\n",
    "print(inspect.getsource(SelectiveInference.get_interval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac47c5ee-ed69-4ac5-9d0c-2aebcebafb47",
   "metadata": {},
   "source": [
    "The conclusion follows from the fact that, as discussed in {cite}`LeeLasso` $\\left\\{\\bar{\\omega}: \\bar{A} \\bar{\\omega} \\leq \\bar{b}(N_o,\\bar{N}_o,t)\\right\\}$ is an interval $[L(N_o,\\bar{N}_o, t), U(N_o, \\bar{N}_o, t)]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7dfc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the selection probability (weight)\n",
    "print(inspect.getsource(SelectiveInference.get_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5373db9",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Below is a Python implementation of the framework described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8586e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The core dataclass holding the problem parameters\n",
    "source = inspect.getsource(SelectiveInference)\n",
    "print(source[:source.find(\"    def compute_params\")].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4d862",
   "metadata": {},
   "source": [
    "### Step-by-Step Computation\n",
    "\n",
    "We can demonstrate the computation by instantiating the class and looking at the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation setup\n",
    "np.random.seed(42)\n",
    "n = 10\n",
    "Z = np.random.randn(n)\n",
    "Q = np.eye(n)\n",
    "gamma_val = 0.5\n",
    "Q_noise = (gamma_val**2) * Q\n",
    "omega = np.random.multivariate_normal(np.zeros(n), Q_noise)\n",
    "Z_noisy = Z + omega\n",
    "\n",
    "# Instantiate class\n",
    "si = SelectiveInference(Z, Z_noisy, Q, Q_noise)\n",
    "\n",
    "# Define contrast eta (v)\n",
    "v = np.zeros(n)\n",
    "v[0] = 1.0  # target is Z[0]\n",
    "\n",
    "# Compute parameters\n",
    "params = si.compute_params(v)\n",
    "\n",
    "print(\"Target theta_hat:\", params['theta_hat'])\n",
    "print(\"Contrast c (should be eta under well-specified):\", params['c'][:3], \"...\")\n",
    "print(\"Variance bar_s:\", params['bar_s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c6c59",
   "metadata": {},
   "source": [
    "And computing the interval $[L, U]$ given some constraints $AZ_{noisy} \\leq b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57a9065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example constraints: Z_noisy > 0 (or -Z_noisy <= 0)\n",
    "A = -np.eye(n)\n",
    "b = np.zeros(n)\n",
    "\n",
    "# Observed value of theta_hat\n",
    "theta_obs = params['theta_hat']\n",
    "\n",
    "# Compute interval for bar_theta\n",
    "L, U = si.get_interval(v, theta_obs, A, b)\n",
    "print(f\"Interval [L, U] for bar_theta: [{L:.4f}, {U:.4f}]\")\n",
    "print(f\"Observed bar_theta: {params['bar_theta']:.4f}\")\n",
    "\n",
    "# Compute selection weight function\n",
    "weight_f = si.get_weight(v, A, b)\n",
    "print(f\"Selection weight at theta_obs: {weight_f(theta_obs):.4f}\")\n",
    "\n",
    "# Evaluate over a range of t values\n",
    "t_grid = np.linspace(theta_obs - 5, theta_obs + 5, 100)\n",
    "weights = [weight_f(t) for t in t_grid]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(t_grid, weights, label=\"Selection Weight\", color=\"blue\")\n",
    "ax.axvline(theta_obs, color=\"red\", linestyle=\"--\", label=\"Observed $\\hat{\\\\theta}$\")\n",
    "ax.set_xlabel(\"Target value $t$\")\n",
    "ax.set_ylabel(\"Selection Probability\")\n",
    "ax.set_title(\"Selection Weight Function\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb372fca",
   "metadata": {},
   "source": [
    "## C++ Implementation via pybind11\n",
    "\n",
    "For performance-critical applications, a C++ implementation using Eigen is also available. It is mirrored from the Python logic and exposed via `pybind11`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c316c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lassoinf.lassoinf_cpp import SelectiveInference as SelectiveInferenceCPP\n",
    "\n",
    "# Instantiate C++ class\n",
    "si_cpp = SelectiveInferenceCPP(Z, Z_noisy, Q, Q_noise)\n",
    "\n",
    "# Compute parameters using C++\n",
    "params_cpp = si_cpp.compute_params(v)\n",
    "print(\"C++ Target theta_hat:\", params_cpp.theta_hat)\n",
    "\n",
    "# Compare selection weight function\n",
    "weight_f_cpp = si_cpp.get_weight(v, A, b)\n",
    "print(f\"C++ Selection weight at theta_obs: {weight_f_cpp(theta_obs):.4f}\")\n",
    "\n",
    "# Cross-check weights\n",
    "weights_cpp = [weight_f_cpp(t) for t in t_grid]\n",
    "diff = np.abs(np.array(weights) - np.array(weights_cpp)).max()\n",
    "print(f\"Maximum difference between Python and C++ weights: {diff:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcb9d9f",
   "metadata": {},
   "source": [
    "### Post-selection density under the well-specified assumption\n",
    "\n",
    "Under the well-specified assumption with $c=\\eta$ and $\\bar{\\Gamma}=\\Gamma$ we see that $N + \\bar{N} = L(Z+\\omega)$ and $\\Gamma \\hat{\\theta}+\\bar{\\Gamma}\\bar{\\omega} = \\Gamma \\eta'(Z+\\omega)$ with $\\eta'(Z+\\omega)$ which we might call $\\bar{\\theta}$, a noisy\n",
    "estimate of our target. In this case, it is more natural to consider the equivalent law of $\\hat{\\theta}, \\bar{\\theta}$: \n",
    "$$\n",
    "\\left\\{(\\hat{\\theta}, \\bar{\\theta}): A\\bar{\\theta} \\leq b - AL(Z+\\omega) \\right\\} = \\left\\{(\\hat{\\theta}, \\bar{\\theta}): \\bar{\\theta} \\in [L(Z+\\omega), U(Z+\\omega)]\\right\\}\n",
    "$$\n",
    "In this case the adjustment to the selection density takes the form\n",
    "$$\n",
    "t \\mapsto \\Phi((\\bar{U}(Z+\\omega)-t)/s) - \\Phi((\\bar{L}(Z+\\omega)-t)/s)\n",
    "$$\n",
    "with $s = \\text{Var}(\\bar{\\theta} | \\hat{\\theta})$. These upper and lower limits are exactly the truncation intervals of {cite}`LeeLasso` when applied to the noisy data $Z+\\omega$!"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3",
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
