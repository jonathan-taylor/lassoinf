# -*- coding: utf-8 -*-
"""Optimization Problems

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11o904LJg6kQgbDoIRV5uDOOOQHHwljvx
"""

# Optimization Solvers using CVXPY
# This script addresses post-selection inference for the Lasso in quadratic form.
# It computes the active sets, derives the conditional polyhedrons Z ~ AZ <= b,
# computes the decoupled orthogonalized constraints for exact inference,
# and provides the bijective affine mappings between Z and (\hat{\beta}, G_{-E}).
#
# Extension: Handles Logistic Regression LASSO via local quadratic approximation.

try:
    import cvxpy as cp
except ImportError:
    print("Installing cvxpy...")
    import subprocess
    subprocess.check_call(["pip", "install", "cvxpy"])
    import cvxpy as cp

import numpy as np

def solve_lasso_quadratic(Q, Z, D_diag, L=None, U=None):
    """
    Solves the Lasso problem in quadratic form:
        min_{\beta} - \beta^T Z + 1/2 \beta^T Q \beta + ||D \beta||_1
        subject to L <= \beta <= U
    """
    n = Q.shape[0]
    beta = cp.Variable(n)

    quad_term = 0.5 * cp.quad_form(beta, Q)
    linear_term = -beta @ Z
    reg_term = cp.norm1(cp.multiply(D_diag, beta))

    objective = cp.Minimize(quad_term + linear_term + reg_term)

    constraints = []
    if L is not None:
        L = np.asarray(L)
        constraints.append(beta >= L)
    if U is not None:
        U = np.asarray(U)
        constraints.append(beta <= U)

    problem = cp.Problem(objective, constraints)
    problem.solve()

    return beta.value, problem.value

def solve_logistic_lasso(X, y, D_diag, L=None, U=None):
    """
    Solves the Logistic Regression Lasso problem:
        min_{\beta} sum(log(1 + exp(X \beta)) - y * X \beta) + ||D \beta||_1
        subject to L <= \beta <= U

    Parameters:
    - X: (n_samples, p_vars) design matrix
    - y: (n_samples,) binary target {0, 1}
    - D_diag: (p_vars,) diagonal penalty weights
    - L: (p_vars,) lower bounds
    - U: (p_vars,) upper bounds
    """
    n_samples, p_vars = X.shape
    beta = cp.Variable(p_vars)

    # cp.logistic(z) is log(1 + exp(z)) elementwise
    loss = cp.sum(cp.logistic(X @ beta)) - y.T @ (X @ beta)
    reg_term = cp.norm1(cp.multiply(D_diag, beta))

    objective = cp.Minimize(loss + reg_term)

    constraints = []
    if L is not None:
        L = np.asarray(L)
        constraints.append(beta >= L)
    if U is not None:
        U = np.asarray(U)
        constraints.append(beta <= U)

    problem = cp.Problem(objective, constraints)
    # The exponential cone solver (ECOS/SCS) is required for cp.logistic
    problem.solve()

    return beta.value, problem.value

def solve_step_size(A, V, b, eta, mode='max'):
    """
    Solves the LP for the maximum or minimum step size 't' in a given direction
    before violating a set of affine constraints.
    """
    t = cp.Variable()
    constraints = [A @ (V + t * cp.reshape(eta, (len(eta), 1))) <= cp.reshape(b, (len(b), 1))]

    if mode == 'max':
        objective = cp.Maximize(t)
    else:
        objective = cp.Minimize(t)

    problem = cp.Problem(objective, constraints)
    problem.solve()

    return t.value, problem.status

def lasso_post_selection_constraints(Q, Z, D_diag, L=None, U=None, tol=1e-5):
    """
    Derives the linear constraints AZ <= b characterizing the polytope where
    the active set, signs, and bound-activations of the Lasso remain constant.
    """
    n = Q.shape[0]
    L = np.full(n, -np.inf) if L is None else np.asarray(L)
    U = np.full(n, np.inf) if U is None else np.asarray(U)

    beta_hat, _ = solve_lasso_quadratic(Q, Z, D_diag, L=L, U=U)

    E = []
    E_c = []
    s_E = []
    v_Ec = []
    g_min = []
    g_max = []

    for j in range(n):
        beta_val = beta_hat[j]
        at_L = (beta_val <= L[j] + tol)
        at_U = (beta_val >= U[j] - tol)
        at_0 = (abs(beta_val) <= tol)

        if not at_L and not at_U and not at_0:
            E.append(j)
            s_E.append(np.sign(beta_val))
        else:
            E_c.append(j)
            if at_0: v_j = 0.0
            elif at_U: v_j = U[j]
            else: v_j = L[j]
            v_Ec.append(v_j)

            dj = D_diag[j]
            gmin, gmax = -np.inf, np.inf
            if at_0:
                if L[j] < -tol: gmin = -dj
                if U[j] > tol:  gmax = dj
            elif at_U: gmin = dj
            elif at_L: gmax = -dj

            g_min.append(gmin)
            g_max.append(gmax)

    E = np.array(E, dtype=int)
    E_c = np.array(E_c, dtype=int)
    s_E = np.array(s_E)
    v_Ec = np.array(v_Ec)
    g_min = np.array(g_min)
    g_max = np.array(g_max)

    A_list, b_list = [], []
    E_M = np.eye(n)[E] if len(E) > 0 else np.zeros((0, n))
    E_F = np.eye(n)[E_c] if len(E_c) > 0 else np.zeros((0, n))

    if len(E) > 0:
        Q_EE = Q[np.ix_(E, E)]
        Q_EEc = Q[np.ix_(E, E_c)] if len(E_c) > 0 else np.zeros((len(E), 0))
        W = np.linalg.inv(Q_EE)

        H_E = W @ E_M
        c_E = W @ (Q_EEc @ v_Ec + D_diag[E] * s_E)

        A_list.append(-np.diag(s_E) @ H_E)
        b_list.append(-np.diag(s_E) @ c_E)

        for k, j in enumerate(E):
            if s_E[k] == 1 and U[j] < np.inf:
                A_list.append(H_E[k:k+1, :])
                b_list.append(np.array([U[j] + c_E[k]]))
            elif s_E[k] == -1 and L[j] > -np.inf:
                A_list.append(-H_E[k:k+1, :])
                b_list.append(np.array([-L[j] - c_E[k]]))
    else:
        H_E = np.zeros((0, n))
        c_E = np.zeros(0)

    if len(E_c) > 0:
        Q_EcE = Q[np.ix_(E_c, E)] if len(E) > 0 else np.zeros((len(E_c), 0))
        Q_EcEc = Q[np.ix_(E_c, E_c)]

        H_Ec = E_F - Q_EcE @ H_E
        c_Ec = Q_EcE @ c_E - Q_EcEc @ v_Ec

        for k, j in enumerate(E_c):
            if g_max[k] < np.inf:
                A_list.append(H_Ec[k:k+1, :])
                b_list.append(np.array([g_max[k] - c_Ec[k]]))
            if g_min[k] > -np.inf:
                A_list.append(-H_Ec[k:k+1, :])
                b_list.append(np.array([-g_min[k] + c_Ec[k]]))

    A = np.vstack(A_list) if A_list else np.zeros((0, n))
    b = np.concatenate(b_list) if b_list else np.zeros(0)

    return A, b, E, E_c, s_E, v_Ec

def lasso_orthogonal_constraints(Q, Z, D_diag, L=None, U=None, tol=1e-5):
    """
    Derives the decoupled linear constraints characterizing the Lasso polytope
    in terms of the statistically independent orthogonalized variables.
    """
    n = Q.shape[0]
    L = np.full(n, -np.inf) if L is None else np.asarray(L)
    U = np.full(n, np.inf) if U is None else np.asarray(U)

    _, _, E, E_c, s_E, v_Ec = lasso_post_selection_constraints(Q, Z, D_diag, L=L, U=U, tol=tol)

    beta_hat, _ = solve_lasso_quadratic(Q, Z, D_diag, L=L, U=U)
    g_min = []
    g_max = []

    for j in E_c:
        beta_val = beta_hat[j]
        at_0 = (abs(beta_val) <= tol)
        at_U = (beta_val >= U[j] - tol)
        at_L = (beta_val <= L[j] + tol)

        dj = D_diag[j]
        gmin, gmax = -np.inf, np.inf

        if at_0:
            if L[j] < -tol: gmin = -dj
            if U[j] > tol:  gmax = dj
        elif at_U: gmin = dj
        elif at_L: gmax = -dj

        g_min.append(gmin)
        g_max.append(gmax)

    g_min = np.array(g_min)
    g_max = np.array(g_max)

    A_E_list, b_E_list = [], []
    A_Ec_list, b_Ec_list = [], []

    # --- Constraints on bar_beta_E ---
    if len(E) > 0:
        Q_EE = Q[np.ix_(E, E)]
        Q_EEc = Q[np.ix_(E, E_c)] if len(E_c) > 0 else np.zeros((len(E), 0))
        W = np.linalg.inv(Q_EE)

        c_E = W @ (Q_EEc @ v_Ec + D_diag[E] * s_E)

        A_E_list.append(-np.diag(s_E))
        b_E_list.append(-np.diag(s_E) @ c_E)

        for k, j in enumerate(E):
            if U[j] < np.inf:
                e_k = np.zeros(len(E)); e_k[k] = 1.0
                A_E_list.append(e_k)
                b_E_list.append(np.array([U[j] + c_E[k]]))
            if L[j] > -np.inf:
                e_k = np.zeros(len(E)); e_k[k] = -1.0
                A_E_list.append(e_k)
                b_E_list.append(np.array([-L[j] - c_E[k]]))
    else:
        c_E = np.zeros(0)

    # --- Constraints on U_minus_E ---
    if len(E_c) > 0:
        Q_EcE = Q[np.ix_(E_c, E)] if len(E) > 0 else np.zeros((len(E_c), 0))
        Q_EcEc = Q[np.ix_(E_c, E_c)]

        c_Ec = -Q_EcEc @ v_Ec + Q_EcE @ c_E

        for k, j in enumerate(E_c):
            if g_max[k] < np.inf:
                e_k = np.zeros(len(E_c)); e_k[k] = 1.0
                A_Ec_list.append(e_k)
                b_Ec_list.append(np.array([g_max[k] - c_Ec[k]]))
            if g_min[k] > -np.inf:
                e_k = np.zeros(len(E_c)); e_k[k] = -1.0
                A_Ec_list.append(e_k)
                b_Ec_list.append(np.array([-g_min[k] + c_Ec[k]]))

    A_E = np.vstack(A_E_list) if A_E_list else np.zeros((0, len(E)))
    b_E = np.concatenate(b_E_list) if b_E_list else np.zeros(0)

    A_Ec = np.vstack(A_Ec_list) if A_Ec_list else np.zeros((0, len(E_c)))
    b_Ec = np.concatenate(b_Ec_list) if b_Ec_list else np.zeros(0)

    return A_E, b_E, A_Ec, b_Ec, E, E_c, s_E, v_Ec

def reconstruct_Z(beta_hat, G_Ec, E, E_c, s_E, D_diag, Q):
    """
    Reconstructs the sufficient statistic Z from the optimal solution \hat{\beta}
    and the subgradient on the inactive/fixed coordinates G_{-E}.
    """
    n = Q.shape[0]
    G = np.zeros(n)

    if len(E) > 0:
        G[E] = D_diag[E] * s_E
    if len(E_c) > 0:
        G[E_c] = G_Ec

    Z = Q @ beta_hat + G
    return Z

def affine_map_Z_to_solution(Z, E, E_c, s_E, v_Ec, D_diag, Q):
    """
    Computes the optimal point estimate \hat{\beta} and the fixed-coordinate
    subgradient G_{-E} directly from Z as an affine map.
    """
    n = Q.shape[0]
    beta_hat = np.zeros(n)

    if len(E_c) > 0:
        beta_hat[E_c] = v_Ec

    if len(E) > 0:
        Q_EE = Q[np.ix_(E, E)]
        Q_EEc = Q[np.ix_(E, E_c)] if len(E_c) > 0 else np.zeros((len(E), 0))

        rhs = Z[E] - Q_EEc @ v_Ec - D_diag[E] * s_E
        beta_E = np.linalg.inv(Q_EE) @ rhs
        beta_hat[E] = beta_E
    else:
        beta_E = np.zeros(0)

    if len(E_c) > 0:
        Q_EcE = Q[np.ix_(E_c, E)] if len(E) > 0 else np.zeros((len(E_c), 0))
        Q_EcEc = Q[np.ix_(E_c, E_c)]

        G_Ec = Z[E_c] - Q_EcE @ beta_E - Q_EcEc @ v_Ec
    else:
        G_Ec = np.zeros(0)

    return beta_hat, G_Ec

# --- Example Usage ---

if __name__ == "__main__":
    np.random.seed(42) # For reproducible tests

    print("--- Problem 1: Lasso Quadratic Form ---")
    n_lasso = 5
    _tmp = np.random.randn(n_lasso, n_lasso)
    Q_val = _tmp.T @ _tmp + np.eye(n_lasso) * 0.1
    Z_val = np.random.randn(n_lasso)
    D_val = np.random.uniform(0.1, 1.0, n_lasso)

    D_val[0] = 0.0 # Make one variable unpenalized
    L_bounds = [0.0, -3.0, -np.inf, -np.inf, -np.inf]
    U_bounds = [np.inf, 2.0, np.inf, np.inf, np.inf]

    opt_beta, _ = solve_lasso_quadratic(Q_val, Z_val, D_val, L=L_bounds, U=U_bounds)
    print(f"Optimal Beta:\n{opt_beta}")

    print("\n--- Problem 2: Polytope Extraction ---")
    A, b, E, E_c, s_E, v_Ec = lasso_post_selection_constraints(
        Q_val, Z_val, D_val, L=L_bounds, U=U_bounds
    )
    print(f"Free Set (E): {E}, Fixed Set (-E): {E_c}")

    print("\n--- Problem 3: Bijective Affine Mapping Test ---")
    beta_hat_map, G_Ec_map = affine_map_Z_to_solution(Z_val, E, E_c, s_E, v_Ec, D_val, Q_val)
    diff = np.max(np.abs(opt_beta - beta_hat_map))
    print(f"Max difference in beta: {diff:.2e}")
    Z_reconstructed = reconstruct_Z(beta_hat_map, G_Ec_map, E, E_c, s_E, D_val, Q_val)
    z_diff = np.max(np.abs(Z_val - Z_reconstructed))
    print(f"Reconstructed Z matches original Z? {z_diff < 1e-10} (Diff: {z_diff:.2e})")

    print("\n=======================================================")
    print("--- Problem 4: Logistic Regression LASSO (GLM) ---")

    # 1. Generate Synthetic Data
    n_samples, p_vars = 100, 5
    X_log = np.random.randn(n_samples, p_vars)
    true_beta = np.array([1.5, -2.0, 0.0, 0.0, 0.5])
    # Safe sigmoid computation
    logits = X_log @ true_beta
    p_true = np.where(logits >= 0, 1 / (1 + np.exp(-logits)), np.exp(logits) / (1 + np.exp(logits)))
    y_log = (np.random.rand(n_samples) < p_true).astype(float)

    # Setup constraints (e.g. unpenalized intercept, bounded variables)
    D_log = np.array([0.0, 1.0, 1.0, 1.0, 1.0])
    L_log = np.array([-np.inf, -np.inf, -1.0, 0.0, -np.inf])
    U_log = np.array([np.inf, 0.0, np.inf, np.inf, 2.0])

    # 2. Solve Logistic LASSO exactly
    beta_log, _ = solve_logistic_lasso(X_log, y_log, D_log, L=L_log, U=U_log)
    print(f"Logistic Beta Hat:\n{beta_log}")

    # 3. Compute Local Quadratic Approximation around optimum \hat{\beta}
    logits_hat = X_log @ beta_log
    p_hat = np.where(logits_hat >= 0, 1 / (1 + np.exp(-logits_hat)), np.exp(logits_hat) / (1 + np.exp(logits_hat)))

    # Gradient of smooth loss: nabla f(beta) = X^T (p - y)
    G_loss = X_log.T @ (p_hat - y_log)

    # Hessian of smooth loss: nabla^2 f(beta) = X^T W X
    W = np.diag(p_hat * (1 - p_hat))
    Q_log = X_log.T @ W @ X_log

    # 4. Construct Sufficient Statistic Z
    # In the quadratic map: Q \beta - Z = \nabla f(\beta)
    # So, Z = Q \beta - \nabla f(\beta)
    Z_log = Q_log @ beta_log - G_loss

    print(f"\nConstructed Z for local approx:\n{Z_log}")

    # 5. Extract Polytope for the local quadratic approximation
    # Notice we pass the constructed Q_log and Z_log to the exact same quadratic function!
    A_log, b_log, E_log, Ec_log, s_E_log, v_Ec_log = lasso_post_selection_constraints(
        Q_log, Z_log, D_log, L=L_log, U=U_log
    )

    print(f"Free Set (E): {E_log}")
    print(f"Fixed Set (-E): {Ec_log}")

    # 6. Verify Exact Feasibility!
    # Because CVXPY's ECOS solver handles exponential cones with a precision ~1e-7,
    # we allow a tiny tolerance for the constraint check.
    constraint_residuals = A_log @ Z_log - b_log
    max_violation = np.max(constraint_residuals)
    is_inside_log = max_violation <= 1e-5

    print(f"Does constructed Z satisfy A @ Z <= b? {is_inside_log} (Max violation: {max_violation:.2e})")

    # 7. Verify the Quadratic solver maps back to the Logistic solution
    beta_quad_map, _ = affine_map_Z_to_solution(
        Z_log, E_log, Ec_log, s_E_log, v_Ec_log, D_log, Q_log
    )

    diff_log = np.max(np.abs(beta_log - beta_quad_map))
    print(f"Max diff between Logistic Beta and local Quadratic Beta: {diff_log:.2e}")