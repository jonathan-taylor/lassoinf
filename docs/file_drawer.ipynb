{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fad8efb1",
   "metadata": {},
   "source": [
    "# File Drawer Example\n",
    "\n",
    "This example demonstrates the \"File Drawer\" problem, a classic form of selection bias where a test statistic $Z$ is only observed or reported if it exceeds a certain thresholdâ€”often representing \"statistical significance.\" In this scenario, results that fail to meet the threshold are effectively left in the \"file drawer,\" leading to overestimates of effect sizes and inflated type I errors if standard inference is used.\n",
    "\n",
    "## Problem Setup\n",
    "\n",
    "We model a simple version of this phenomenon:\n",
    "- **Full Data**: $Z \\sim \\text{Normal}(\\mu, 1)$, where $\\mu$ is the true effect size we wish to estimate.\n",
    "- **Reporting Noise**: $\\omega \\sim \\text{Normal}(0, \\gamma^2)$, representing additional variability in the selection process (e.g., small variations in experimental conditions or data cleaning). We set $\\gamma = 0.5$.\n",
    "- **Selection Event**: The result is only \"published\" if the noisy version of the statistic, $Z + \\omega$, exceeds a threshold of 2.0.\n",
    "- **Observation**: We observe $Z = 1.73$. \n",
    "\n",
    "Note that $Z=1.73$ is actually below the threshold of 2.0, but it was \"selected\" because the unobserved noise $\\omega$ was large enough to push $Z + \\omega$ over 2.0. If we ignore this selection process, we may produce confidence intervals\n",
    "with poor coverage and $p$-values with poor Type I error. Selective inference allows us to adjust for the fact that we are only looking at this data point because it passed the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "578b11a6-e925-4005-af89-2946111b3378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm as normal_dbn\n",
    "from lassoinf.selective_inference import SelectiveInference\n",
    "from lassoinf.discrete_family import discrete_family\n",
    "\n",
    "# 1. Parameters\n",
    "mu_null = 0\n",
    "gamma = 0.5\n",
    "threshold = 2.0\n",
    "z_obs = 1.73\n",
    "\n",
    "# SelectiveInference expects arrays\n",
    "Z = np.array([z_obs])\n",
    "Q = np.eye(1)\n",
    "Q_noise = np.array([[gamma**2]])\n",
    "\n",
    "# Z_noisy is Z + omega. \n",
    "# For the weight function calculation, the specific value of omega doesn't change \n",
    "# the probability P(Z + omega > threshold | Z=t), but we need to provide a Z_noisy.\n",
    "Z_noisy = Z.copy() \n",
    "\n",
    "si = SelectiveInference(Z, Z_noisy, Q, Q_noise)\n",
    "\n",
    "# 2. Define target and constraints\n",
    "v = np.array([1.0])  # Target is Z itself\n",
    "A = np.array([[-1.0]])  # -(Z + omega) <= -threshold  => Z + omega >= threshold\n",
    "b = np.array([-threshold])\n",
    "\n",
    "# 3. Get the weight function\n",
    "weight_f = si.get_weight(v, A, b)\n",
    "\n",
    "# 4. Plot the weight function\n",
    "t_grid = np.linspace(0, 4, 100)\n",
    "weights = weight_f(t_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c87a35-d1c7-4a84-8e18-aedd663d3ced",
   "metadata": {},
   "source": [
    "### Exact selection adjustment\n",
    "\n",
    "In this problem we can compute the exact adjustment as\n",
    "$$\n",
    "t \\mapsto P(Z+\\omega > 2 | Z=t) = 1 - \\Phi((2-t)/\\gamma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e81f3e-9f70-472f-b2fb-72d00021d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(t_grid, weights, label='Selection Weight $W(t)$', c='k', linewidth=5)\n",
    "ax.plot(t_grid, normal_dbn.sf((2 - t_grid) / gamma), c='r', label=r'$1 - \\Phi((2-t)/\\gamma)$')\n",
    "assert np.allclose(weights, normal_dbn.sf((2 - t_grid) / gamma))\n",
    "ax.axvline(z_obs, color='red', linestyle='--', label=f'Observed $Z={z_obs}$')\n",
    "ax.set_xlabel('Value of $Z$')\n",
    "ax.set_ylabel('Probability of Selection')\n",
    "ax.set_title('File Drawer Selection Weight')\n",
    "ax.legend()\n",
    "ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00b7bb-fb52-430d-a5fc-04fda50a1e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Form the Discrete Family\n",
    "# We use a grid for the sufficient statistic (observed Z)\n",
    "grid = np.linspace(-2, 5, 500)\n",
    "\n",
    "# Reference measure is N(mu_null, 1) * weight_f(t)\n",
    "# We assume the null mu=0 for the reference distribution\n",
    "reference_pdf = normal_dbn.pdf(grid, loc=mu_null, scale=1.0)\n",
    "sel_weights = weight(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7068800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_weights = reference_pdf * sel_weights\n",
    "\n",
    "# Initialize family\n",
    "family = discrete_family(grid, reference_weights)\n",
    "\n",
    "# 6. Inference\n",
    "# 95% Confidence Interval\n",
    "lower, upper = family.interval(z_obs, alpha=0.05, randomize=False)\n",
    "print(f\"95% Confidence Interval for mu: ({lower:.3f}, {upper:.3f})\")\n",
    "\n",
    "# P-value for H0: mu = 0\n",
    "# We can use the CDF at the observed value under theta=0 (which is our reference)\n",
    "p_val_cdf = family.cdf(0, z_obs, gamma=0.5)\n",
    "p_val_two_sided = 2 * min(p_val_cdf, 1 - p_val_cdf)\n",
    "print(f\"Two-sided p-value for H0 (mu=0): {p_val_two_sided:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055217e-1551-45ce-a71e-e998d3a3a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "family.cdf(1, z_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62040e5",
   "metadata": {},
   "source": [
    "## Selective Inference Analysis\n",
    "\n",
    "### The Weight Function\n",
    "The weight function $W(t) = P(Z + \\omega > 2 \\mid Z = t)$ is critical. It calculates the probability that the selection criterion is met for any possible realization $t$ of $Z$. \n",
    "- When $t$ is very large, the probability of selection is near 1.\n",
    "- When $t$ is very small, selection is unlikely but still possible if $\\omega$ is large.\n",
    "The plot generated above shows how this probability \"filters\" our view of the data.\n",
    "\n",
    "### Adjusting the Distribution\n",
    "In standard inference, we would use $Z \\sim \\text{Normal}(\\mu, 1)$. However, given selection, the conditional distribution of $Z$ is:\n",
    "$$f_{\\mu}(z \\mid \\text{selected}) \\propto \\phi(z-\\mu) \\cdot W(z)$$\n",
    "The `discrete_family` class takes a grid of values and their corresponding weights (the product of the base density and the selection weight) to represent this adjusted exponential family.\n",
    "\n",
    "### Valid Post-Selection Inference\n",
    "By inverting the tests in this adjusted family, we obtain confidence intervals and p-values that are valid even though the data point was chosen specifically because it was \"large.\" This approach directly mitigates the bias inherent in the file-drawer effect, providing a more honest assessment of the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff490d85-ec10-4fef-aaa6-a444d56f08aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md:myst",
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
