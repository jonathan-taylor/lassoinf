{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c32915",
   "metadata": {},
   "source": [
    "Under $\\zeta^*=0$, this is of course just \n",
    "- $Z$: the score\n",
    "- $\\bar{Z} = Z + \\omega$, the noisy score\n",
    "- $\\hat{\\theta} = \\eta' Z$, the target estimator signal projection)\n",
    "- $\\bar{\\alpha} = E[\\hat{\\theta} | \\bar{Z}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b132863",
   "metadata": {},
   "source": [
    "We seek the residual of the stacked vector $X_{stack} = [Z, \\bar{Z}]^\\top$ after projecting onto the basis $V = [\\hat{\\theta}, W]^\\top$.\n",
    "\n",
    "## Covariance Analysis and Sherman-Morrison-Woodbury\n",
    "The covariance of $W$ and the cross-covariance with $\\hat{\\theta}$ involve the Wiener filter matrix $A = \\Sigma(\\Sigma + \\bar{\\Sigma})^{-1}$. \n",
    "The variance-covariance matrix of the basis $V$ is given by:\n",
    "$$ Var(V) = \\begin{pmatrix} \\sigma_{\\hat{\\theta}}^2 & \\gamma \\\\ \\gamma & \\gamma \\end{pmatrix} $$\n",
    "where:\n",
    "- $\\sigma_{\\hat{\\theta}}^2 = \\eta^\\top \\Sigma \\eta$\n",
    "- $\\gamma = \\eta^\\top \\Sigma (\\Sigma + \\bar{\\Sigma})^{-1} \\Sigma \\eta$\n",
    "\n",
    "Applying the \\textbf{Sherman-Morrison-Woodbury Identity} to the expression for $\\gamma$:\n",
    "$$ (\\Sigma + \\bar{\\Sigma})^{-1} = \\Sigma^{-1} - \\Sigma^{-1}(\\Sigma^{-1} + \\bar{\\Sigma}^{-1})^{-1}\\Sigma^{-1} $$\n",
    "Substituting this into $\\gamma$ yields the residual variance $\\delta$:\n",
    "$$ \\delta = \\sigma_{\\hat{\\theta}}^2 - \\gamma = \\eta^\\top (\\Sigma^{-1} + \\bar{\\Sigma}^{-1})^{-1} \\eta $$\n",
    "\n",
    "## The $\\bar{Z}$-Residual\n",
    "The projection of $\\bar{Z}$ onto the span of $V$ simplifies significantly because $Cov(\\bar{Z}, \\hat{\\theta}) = Cov(\\bar{Z}, W) = \\Sigma \\eta$. The resulting $\\bar{Z}$-residual is:\n",
    "$$ \\bar{Z}_{res} = \\left[ I - \\frac{\\Sigma \\eta \\eta^\\top \\Sigma (\\Sigma + \\bar{\\Sigma})^{-1}}{\\gamma} \\right] \\bar{Z} $$\n",
    "\n",
    "### Special Case: Homoscedastic Noise ($\\bar{\\Sigma} = \\alpha \\Sigma$)\n",
    "When the noise is proportional to the signal covariance, the matrix terms cancel:\n",
    "$$ \\bar{Z}_{res} = \\bar{Z} - \\frac{\\eta^\\top \\bar{Z}}{\\eta^\\top \\Sigma \\eta} \\Sigma \\eta $$\n",
    "In this case, the residual is independent of the noise magnitude $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93535ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_projection_system(Sigma, Sigma_bar, eta):\n",
    "    \"\"\"\n",
    "    Computes the projection components for a Gaussian stacked system.\n",
    "    \n",
    "    Parameters:\n",
    "    Sigma (n x n array): Covariance of Z\n",
    "    Sigma_bar (n x n array): Covariance of omega\n",
    "    eta (n, array): Vector defining theta_hat = eta'Z\n",
    "    \n",
    "    Returns:\n",
    "    dict: Contains the 2x2 basis covariance and the rank-1 projection matrix.\n",
    "    \"\"\"\n",
    "    # Ensure eta is a column vector for matrix ops\n",
    "    eta = eta.reshape(-1, 1)\n",
    "    \n",
    "    # 1. Precompute the shared signal vector: v = Sigma @ eta\n",
    "    v = Sigma @ eta\n",
    "    \n",
    "    # 2. Compute the filtered vector x = (Sigma + Sigma_bar)^-1 @ v\n",
    "    # Using solve() is more stable than inv()\n",
    "    S_sum = Sigma + Sigma_bar\n",
    "    x = np.linalg.solve(S_sum, v)\n",
    "    \n",
    "    # 3. Scalar components\n",
    "    # sigma_theta_sq = eta' @ Sigma @ eta\n",
    "    sigma_theta_sq = (eta.T @ v).item()\n",
    "    \n",
    "    # gamma = eta' @ Sigma @ (Sigma + Sigma_bar)^-1 @ Sigma @ eta\n",
    "    # This is equivalent to v' @ (S_sum^-1) @ v, or more simply:\n",
    "    gamma = (v.T @ x).item()\n",
    "    \n",
    "    # 4. Construct the 2x2 Basis Covariance Matrix Var(V)\n",
    "    # V = [theta_hat, W]\n",
    "    cov_basis = np.array([\n",
    "        [sigma_theta_sq, gamma],\n",
    "        [gamma,          gamma]\n",
    "    ])\n",
    "    \n",
    "    # 5. Projection Matrix M such that Y_proj = M @ Y\n",
    "    # Based on our derivation: M = (v @ x.T) / gamma\n",
    "    M = (v @ x.T) / gamma\n",
    "    \n",
    "    return {\n",
    "        \"cov_basis\": cov_basis,\n",
    "        \"projection_matrix_M\": M,\n",
    "        \"gamma\": gamma,\n",
    "        \"residual_variance\": sigma_theta_sq - gamma\n",
    "    }\n",
    "\n",
    "# --- Example Setup ---\n",
    "n = 5\n",
    "eta_vec = np.random.randn(n)\n",
    "S = np.diag(np.linspace(1, 2, n))  # Example Sigma\n",
    "S_b = np.eye(n) * 0.1             # Example Sigma_bar\n",
    "\n",
    "res = compute_projection_system(S, S_b, eta_vec)\n",
    "\n",
    "print(\"--- 2x2 Basis Covariance Matrix ---\")\n",
    "print(res[\"cov_basis\"])\n",
    "print(\"\\n--- Projection Matrix M (Rank-1) ---\")\n",
    "print(res[\"projection_matrix_M\"])\n",
    "print(f\"\\nCheck: Rank of M is {np.linalg.matrix_rank(res['projection_matrix_M'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ccb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Optimization Problems\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1OnSk5QfnJUJrKu9Y35Vvr2fHQwgVLYbw\n",
    "\"\"\"\n",
    "\n",
    "# Optimization Solvers using CVXPY\n",
    "# This script addresses two specific problems:\n",
    "# 1. Lasso in quadratic form with diagonal regularization weight.\n",
    "# 2. Finding the step-size 't' for an affine constraint.\n",
    "\n",
    "# Install cvxpy if running in a fresh Colab environment\n",
    "try:\n",
    "    import cvxpy as cp\n",
    "except ImportError:\n",
    "    print(\"Installing cvxpy...\")\n",
    "    !pip install cvxpy\n",
    "    import cvxpy as cp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def solve_lasso_quadratic(Q, Z, D_diag):\n",
    "    \"\"\"\n",
    "    Solves min -b'Z + 1/2 b'Qb + ||D beta||_1\n",
    "    where D is diagonal.\n",
    "\n",
    "    Parameters:\n",
    "    - Q: (n, n) Positive Semidefinite Matrix\n",
    "    - Z: (n,) vector\n",
    "    - D_diag: (n,) vector representing diagonal of D\n",
    "    \"\"\"\n",
    "    n = Q.shape[0]\n",
    "    beta = cp.Variable(n)\n",
    "\n",
    "    # 0.5 * beta.T @ Q @ beta\n",
    "    quad_term = 0.5 * cp.quad_form(beta, Q)\n",
    "\n",
    "    # -beta.T @ Z\n",
    "    linear_term = -beta @ Z\n",
    "\n",
    "    # ||D @ beta||_1\n",
    "    # Since D is diagonal, D @ beta is element-wise multiplication\n",
    "    reg_term = cp.norm1(cp.multiply(D_diag, beta))\n",
    "\n",
    "    objective = cp.Minimize(quad_term + linear_term + reg_term)\n",
    "    problem = cp.Problem(objective)\n",
    "\n",
    "    problem.solve()\n",
    "\n",
    "    return beta.value, problem.value\n",
    "\n",
    "def solve_step_size(A, V, b, eta, mode='max'):\n",
    "    \"\"\"\n",
    "    Solves min/max t subject to A(V + t * eta) <= b\n",
    "\n",
    "    Parameters:\n",
    "    - A: (m, n) matrix\n",
    "    - V: (n,) initial vector\n",
    "    - b: (m,) constraint vector\n",
    "    - eta: (n,) direction vector\n",
    "    - mode: 'min' or 'max'\n",
    "    \"\"\"\n",
    "    t = cp.Variable()\n",
    "\n",
    "    # Constraint: A @ (V + t * eta) <= b\n",
    "    # CVXPY handles the affine expression distribution automatically\n",
    "    constraints = [A @ (V + t * cp.reshape(eta, (len(eta), 1))) <= cp.reshape(b, (len(b), 1))]\n",
    "\n",
    "    if mode == 'max':\n",
    "        objective = cp.Maximize(t)\n",
    "    else:\n",
    "        objective = cp.Minimize(t)\n",
    "\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    return t.value, problem.status\n",
    "\n",
    "def lasso_post_selection_constraints(Q, Z, D_diag, tol=1e-5):\n",
    "    \"\"\"\n",
    "    Derives the linear constraints A * Z <= b characterizing the polytope\n",
    "    where the active set and signs of the Lasso remain constant,\n",
    "    based on Lee, Sun, Sun, and Taylor (2016).\n",
    "    Allows for unpenalized variables by passing 0 in D_diag.\n",
    "    \"\"\"\n",
    "    # 1. Solve the lasso to get the empirical active set and signs\n",
    "    beta_hat, _ = solve_lasso_quadratic(Q, Z, D_diag)\n",
    "    n = Q.shape[0]\n",
    "\n",
    "    # 2. Extract active set (M) and inactive set (M_c)\n",
    "    # Unpenalized variables are forced into the active set\n",
    "    M_empirical = np.where(np.abs(beta_hat) > tol)[0]\n",
    "    unpenalized = np.where(D_diag <= tol)[0]\n",
    "    M = np.union1d(M_empirical, unpenalized).astype(int)\n",
    "    M_c = np.setdiff1d(np.arange(n), M).astype(int)\n",
    "\n",
    "    # Edge case: No variables are active\n",
    "    if len(M) == 0:\n",
    "        A = np.vstack([np.eye(n), -np.eye(n)])\n",
    "        b = np.concatenate([D_diag, D_diag])\n",
    "        return A, b, M, np.array([])\n",
    "\n",
    "    s_M = np.sign(beta_hat[M])\n",
    "    # Handle exact zeros to avoid multiplying by 0 incorrectly in subgradients\n",
    "    s_M[s_M == 0] = 1\n",
    "\n",
    "    # Matrix blocks\n",
    "    Q_MM = Q[np.ix_(M, M)]\n",
    "    Q_McM = Q[np.ix_(M_c, M)]\n",
    "\n",
    "    # Inverse of Q_MM\n",
    "    invQ_MM = np.linalg.inv(Q_MM)\n",
    "\n",
    "    D_M = D_diag[M]\n",
    "    D_Mc = D_diag[M_c]\n",
    "\n",
    "    # Precompute shared terms to avoid redundant multiplications\n",
    "    invQ_D_s = invQ_MM @ (D_M * s_M)\n",
    "\n",
    "    # --- Constraint 1: Sign Constraints for Active Variables ---\n",
    "    # Condition: diag(s_M) @ beta_M > 0\n",
    "    # Only enforce for PENALIZED variables (where D_M > 0)\n",
    "    penalized_in_M = np.where(D_M > tol)[0]\n",
    "\n",
    "    if len(penalized_in_M) > 0:\n",
    "        A1_full = -np.diag(s_M) @ invQ_MM\n",
    "        A1 = np.zeros((len(penalized_in_M), n))\n",
    "        A1[:, M] = A1_full[penalized_in_M]\n",
    "\n",
    "        b1_full = -np.diag(s_M) @ invQ_D_s\n",
    "        b1 = b1_full[penalized_in_M]\n",
    "    else:\n",
    "        A1 = np.zeros((0, n))\n",
    "        b1 = np.zeros(0)\n",
    "\n",
    "    # --- Constraint 2 & 3: Subgradient Bounds for Inactive Variables ---\n",
    "    if len(M_c) > 0:\n",
    "        Q_Mc_invQ = Q_McM @ invQ_MM\n",
    "\n",
    "        # Condition: Z_Mc - Q_McM @ beta_M <= D_Mc\n",
    "        # A2 * Z <= b2\n",
    "        A2 = np.zeros((len(M_c), n))\n",
    "        A2[:, M] = -Q_Mc_invQ\n",
    "        A2[:, M_c] = np.eye(len(M_c))\n",
    "        b2 = D_Mc - Q_Mc_invQ @ (D_M * s_M)\n",
    "\n",
    "        # Condition: -Z_Mc + Q_McM @ beta_M <= D_Mc\n",
    "        # A3 * Z <= b3\n",
    "        A3 = np.zeros((len(M_c), n))\n",
    "        A3[:, M] = Q_Mc_invQ\n",
    "        A3[:, M_c] = -np.eye(len(M_c))\n",
    "        b3 = D_Mc + Q_Mc_invQ @ (D_M * s_M)\n",
    "    else:\n",
    "        A2 = np.zeros((0, n))\n",
    "        b2 = np.zeros(0)\n",
    "        A3 = np.zeros((0, n))\n",
    "        b3 = np.zeros(0)\n",
    "\n",
    "    # Combine all constraints into a single Polytope A*Z <= b\n",
    "    A = np.vstack([A1, A2, A3])\n",
    "    b = np.concatenate([b1, b2, b3])\n",
    "\n",
    "    return A, b, M, s_M\n",
    "\n",
    "def compute_affine_w_constraints(A, b, z0, C, w0):\n",
    "    \"\"\"\n",
    "    Computes the equivalent affine constraints for w, given the constraint\n",
    "    set {Z : AZ <= b} and the substitution Z = z0 - C*w0 + C*w.\n",
    "\n",
    "    This finds A_bar and b_bar such that the set can be written as\n",
    "    {w : A_bar * w <= b_bar}.\n",
    "\n",
    "    Parameters:\n",
    "    - A: (m, n) constraint matrix for Z\n",
    "    - b: (m,) constraint vector for Z\n",
    "    - z0: (n,) feasible vector in the Z space\n",
    "    - C: (n, k) transformation matrix\n",
    "    - w0: (k,) reference vector in the w space\n",
    "\n",
    "    Returns:\n",
    "    - A_bar: (m, k) equivalent constraint matrix for w\n",
    "    - b_bar: (m,) equivalent constraint vector for w\n",
    "    \"\"\"\n",
    "    # From A @ (z0 - C @ w0 + C @ w) <= b\n",
    "    # We expand to: A @ z0 - A @ C @ w0 + A @ C @ w <= b\n",
    "    # Isolate w: (A @ C) @ w <= b - A @ z0 + A @ C @ w0\n",
    "\n",
    "    A_bar = A @ C\n",
    "    b_bar = b - A @ z0 + A_bar @ w0\n",
    "\n",
    "    return A_bar, b_bar\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Problem 1: Lasso Quadratic Form ---\")\n",
    "    n_lasso = 5\n",
    "    # Generate random PSD matrix Q\n",
    "    _tmp = np.random.randn(n_lasso, n_lasso)\n",
    "    Q_val = _tmp.T @ _tmp + np.eye(n_lasso) * 0.1\n",
    "    Z_val = np.random.randn(n_lasso)\n",
    "    D_val = np.random.uniform(0.1, 1.0, n_lasso)\n",
    "\n",
    "    # Test unpenalized variables by setting the first weight to 0\n",
    "    D_val[0] = 0.0\n",
    "\n",
    "    opt_beta, opt_val = solve_lasso_quadratic(Q_val, Z_val, D_val)\n",
    "    print(f\"Optimal Beta:\\n{opt_beta}\")\n",
    "    print(f\"Optimal Value: {opt_val:.4f}\\n\")\n",
    "\n",
    "    print(\"--- Problem 2: Step Size Optimization ---\")\n",
    "    m, n_step = 10, 3\n",
    "    A_val = np.random.randn(m, n_step)\n",
    "    V_val = np.zeros(n_step) # Start at origin\n",
    "    b_val = np.ones(m)      # Constraints are A @ x <= 1\n",
    "    eta_val = np.random.randn(n_step)\n",
    "\n",
    "    max_t, status_max = solve_step_size(A_val, V_val, b_val, eta_val, mode='max')\n",
    "    min_t, status_min = solve_step_size(A_val, V_val, b_val, eta_val, mode='min')\n",
    "\n",
    "    print(f\"Direction eta: {eta_val}\")\n",
    "    print(f\"Max t: {max_t} (Status: {status_max})\")\n",
    "    print(f\"Min t: {min_t} (Status: {status_min})\\n\")\n",
    "\n",
    "    print(\"--- Problem 3: Post-Selection Inference Polytope (Lee et al.) ---\")\n",
    "    # Reuse Q_val, Z_val, D_val from Problem 1\n",
    "    A_poly, b_poly, active_set, signs = lasso_post_selection_constraints(Q_val, Z_val, D_val)\n",
    "\n",
    "    print(f\"Active Set M: {active_set}\")\n",
    "    print(f\"Signs s_M: {signs}\")\n",
    "    print(f\"Polytope constraints A shape: {A_poly.shape}, b shape: {b_poly.shape}\")\n",
    "\n",
    "    # Verify the current Z lies inside the polytope (A @ Z <= b)\n",
    "    # We add a small tolerance for floating point inaccuracies from cvxpy\n",
    "    is_inside = np.all(A_poly @ Z_val <= b_poly + 1e-6)\n",
    "    print(f\"Does the original Z satisfy A @ Z <= b? {is_inside}\\n\")\n",
    "\n",
    "    print(\"--- Problem 4: Equivalent Affine Constraints for w ---\")\n",
    "    k = 2  # dimension of w\n",
    "    C_val = np.random.randn(n_lasso, k)\n",
    "    w0_val = np.random.randn(k)\n",
    "\n",
    "    A_bar, b_bar = compute_affine_w_constraints(A_poly, b_poly, Z_val, C_val, w0_val)\n",
    "\n",
    "    print(f\"A_bar shape: {A_bar.shape}, b_bar shape: {b_bar.shape}\")\n",
    "\n",
    "    # Verify that w0 is inside the new polytope (A_bar @ w0 <= b_bar)\n",
    "    # This must be true since Z_val is feasible and substituting w0 yields Z_val\n",
    "    w0_is_feasible = np.all(A_bar @ w0_val <= b_bar + 1e-6)\n",
    "    print(f\"Is w0 feasible in the new constraint set A_bar @ w <= b_bar? {w0_is_feasible}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3",
   "formats": "ipynb,md:myst"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
