\documentclass{article}

\input{../preamble.sty}
\usepackage{lmodern}

\newcommand{\OLS}{\bar{\beta}}
\newcommand{\REG}{\hat{\beta}}
\def\qed{\hfill $\Box$\newline}
\newcommand{\ag}[1]{{\bf{{\red{[{AG: #1}]}}}}}
\newcommand{\jt}[1]{{\bf{{\red{[{JT: #1}]}}}}}
\newcommand{\InnerProduct}[2]{\langle #1,#2 \rangle}
\newcommand{\Norm}[1]{\|#1\|}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\br}{{\bf r}}
\newcommand{\bs}{{\bf s}}
\newcommand{\bt}{{\boldsymbol t}}
\newcommand{\bH}{{\boldsymbol H}}
\newcommand{\Newton}{\mathrm{Newton}}
\newcommand{\DummyField}{{\tt f}}
\newcommand{\DummyGradient}{{\bf g}}
\newcommand{\InformationMatrix}{{\boldsymbol J}}
\newcommand{\lin}{\mathrm{lin}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\newcommand{\Exp}{\mathrm{Exp}}

\newcommand{\RandomField}{Y}
\newcommand{\bLambda}{{\boldsymbol \Lambda}}
\newcommand{\bGamma}{{\boldsymbol \Gamma}}
\newcommand{\Err}{\mathrm{Err}}
\newcommand{\bQ}{{\boldsymbol Q}}
\newcommand{\bJ}{{\boldsymbol J}}
\newcommand{\bV}{{\boldsymbol V}}
\newcommand{\bI}{{\boldsymbol I}}
\newcommand{\bC}{{\boldsymbol C}}
\newcommand{\convweak}{\overset{d}{\to}}

\newcommand{\appropto}{\mathrel{\vcenter{
			\offinterlineskip\halign{\hfil$##$\cr
				\propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

\renewcommand{\thealgorithm}{\arabic{algorithm}}
\newcommand{\randsec}{\xi}
\newcommand{\linprocess}{\mathring{\randsec}}
\newcommand{\meansec}{\bar{\mu}}
\newcommand{\indep}{\perp\kern-5pt\perp}

\title{ {\bf Inference (bootstrap or otherwise)
    for projected parameter in the LASSO via homotopy}}
\author{Alden Green and Jonathan Taylor}

\begin{document}

	\maketitle \RaggedRight

        
        \section{The proposal}

        Suppose the LASSO selects variables $E \subset \{1, \dots, p\}$ and, within the
        selected model
        $${\cal M}_E = \left\{N(X_E\beta_E^*, I): \beta_E^* \in \text{row}(X_E)\right\}$$
        we want to
        test $H_0:\eta'\beta_E^*=\theta$ for some contrast\footnote{Without loss of generality we will assume that all contrasts are standardized: $\eta'(X_E'X_E)^{\dagger}X_E=1$.} $\eta \in \text{row}(X_E)$. The general
        results of FST tells us that this density takes the form
        $$
        z \mapsto \exp\left(-\frac{1}{2}(z-\theta)^2\right) \ell_E(z)
        $$
        for some function $\ell_E(z)$ that adjusts the density accordingly. The hard
        work in all of this business is getting our hand on $\ell_E(z)$.

        \subsection{A homotopy}

        Having found a solution to the LASSO with variables $E$ and signs $s_E$, we run the
        homotopy of solving the LASSO along an appropriate  path that fixes (i.e. conditions on)
        the statistic
        $$
        N_{\eta,E}(y) = \OLS_E(y) - (X_E'X_E)^{\dagger} \eta \eta'\OLS_E
        $$
        with $\OLE_E(y) = X_E^{\dagger}y$.
 By construction, the residual off of $X_E$ will not change
        and hence the inactive constraint always holds. It remains to
        check the active constraint along the path.  The signs $\hat{s}(z)$ along
        the homotopy path will be a vector in $\{-1,0,1\}^E$ with a 0
        denoting some variable has dropped from the set $E$. Within
        intervals, the signs will therefore be fixed. We assign the
        following score to each sign where all variables are in $E$
        are non-zero\footnote{Where needed $\Lambda$ will denote a
          diagonal matrix with entries from $\lambda$. Similarly for
          $\Lambda_E$. 
        }
        $$
        \bar{\gamma}_{s_E} = s_E'\Lambda_E(X_E'X_E)^{\dagger}\Lambda_Es_E = \|\zeta_E\Lambda_Es_E\|^2_2
        $$
        with
        \begin{equation}
          \label{eq:sign:offsets}
        \zeta_{E,s_E} = (X_E')^{\dagger}\Lambda_Es_E.
        \end{equation}
The
          $\zeta_{E,s_E}$ can be thought of as the vector in $\R^n$
          that reflects the shrinkage applied to $y$ before fitting
          the model\footnote{Alternatively, it is a distinguished point 
          in the base space of the normal bundle of $\|\Lambda^{-1}X'r\|_{\infty} \leq 1$ that any solution with signs $s_E$ projects to.}. In particular, the fitted values of the LASSO at
          $\zeta_{E,s_E}$ are 0: $\REG_E(\zeta_{E,s_E})=0$ even though
          it is a LASSO ``solution'' at $\zeta_{E,s_E}$.

        The following is an approximation to $\ell_E(z)$ (that should be close to normalized):
        \begin{equation}
          \label{eq:selective:RN}
        \ell_E(z) \approx \begin{cases}
          e^{-\bar{\gamma}_{\hat{s}(z)}/2} & \hat{s}(z) \in \{-1,1\}^E \\
          0 & \text{otherwise.}
          \end{cases}
        \end{equation}
        This is a piecewise constant function that  is
        non-zero on the complement of an interval. This implies the intervals will be
        bounded and well-behaved as in LMT.
        
        {\bf My mistake last week: I had tried to simply ignore a part of the meddlesome
          $\ell_E(z)$, namely the $e^{-\bar{\gamma}_{\hat{s}(z)}/2}$...}

        \section{Back to the beginning}

        Let's begin in the parametric case: $y|X \sim N(\mu, \sigma^2
        I)$ with $\sigma^2$ known which we take to be 1. Of course
        $\mu=\mu(X)$ but all inference that follows is conditional on
        $X$ so we drop this bit of notation.

        Let $\hat{E}=\hat{E}_{\lambda}(y,X)$ denote the set selected
        by the LASSO at a fixed value of $\lambda$. We similarly drop
        the dependence of $\hat{E}$ on $X$.  The LASSO also selects
        signs $\hat{s}_{\hat{E}}(y,X)$ which we will denote by
        $\hat{s}(y)$. This is a bit sloppy as we have to remember that
        $s$ is not defined unless we have fixed $E$, but that's not
        too bad.
        
        Lee et al. writes
        $$
        \begin{aligned}
        \bar{\cal S}_{E,s_E} &\overset{def}{=} \left\{y:
        (\hat{E}(y),\hat{s}(y))=(E, s_E)\right\} &= \left\{y:
        A_{E,s_E,\lambda}\OLS_E(y) \geq 0, I_{E,s_E,\lambda}(R_Ey) \geq 0 \right\}
        \end{aligned}
        $$ where $A_{E,s_E,\lambda}$ is an affine inequality on
        $$ \OLS_E(y) = X_E^{\dagger}y
        $$ and $I_{E,s_E,\lambda}$ is an affine inequality on\footnote{There is a dangling
        detail here about whether or not $X_E'X_E$ is invertible. If it is not invertible,
        then the only estimable contrasts of $\OLS_E(y)$ would be $\eta \in \text{row}(X_E)$.}
        $$ R_Ey = (I - X_EX_E^{\dagger})y=(I-P_E)y.
        $$

        Now let's write
        $$ {\cal S}_E \overset{def}{=} \left\{y: \hat{E}(y)=E\right\}
        = \bigcup_{s_E \in \{-1,1\}^E} \bar{\cal S}_{E,s_E}.
        $$

        Fithian, Sun, Taylor (FST), of which Lee et al. is a specific
        application, describes how to do conditional inference about a
        given contrast $\bar{\eta}'\mu$ on ${\cal S}_E$ by decomposing
        $y$ into two pieces as
        $$ (\bar{\eta}'y, N_{\eta}(y))=(I - \bar{\eta}\bar{\eta}')y).
        $$

        The relevant law conditions on $N_{\eta}$ to remove nuisance
        parameter $(I-\eta\eta')y$ and for any
        $H_0:\bar{\eta}'\mu=\theta$ we use a conditional law with
        density
        \begin{equation}
          \label{eq:selective:density}
        z \mapsto f_{\theta}(z | N_{\eta}(y)=n) \propto \exp
        \left(-\frac{1}{2}(z-\theta)^2 \right) \cdot \P_{\mu}({\cal
          S}_E | \bar{\eta}'y=z, N_{\eta}(y)=n).
        \end{equation}
        The {\em magic} of Lee et al. / FST is that the last
        probability, while nominally a function of $\mu$ is actually
        expressible as
        \begin{equation}
          \label{eq:prob:selection}
        \P_{\mu}({\cal S}_E | \bar{\eta}'y=z, N_{\eta}(y)=n) =
        \begin{cases}
          1 & z \in T(n) \\ 0 & \text{otherwise.}
          \end{cases}
        \end{equation}
        where $T(n) \subset \R$ is computable as a union of intervals,
        one for each $s_E$.

        This is magical in that it seems we computed an integral
        involving $\mu$ over all of $\mathbb{R}^n$ without knowing
        $\mu$. But of course, we did no such thing.  Given $(z,n)$ the
        random vector $y$ can be reconstructed explicitly and we can
        simply check whether the resulting $y=y(z,n) \in {\cal S}_E$.
        Without further assumptions on $\mu$ it also seems to be about
        the best we can do because otherwise we'd need to do something
        about the nuisance parameter $N_{\eta}(\mu)$.

        Now suppose we're interested in a contrast\footnote{There is a dangling
        detail here about whether or not $X_E'X_E$ is invertible. If it is not invertible,
        then the only estimable contrasts of $\OLS_E(y)$ would be $\eta \in \text{row}(X_E)$.} $\eta'\beta_E^*$
        of
        $\beta_E^*$ in ${\cal M}_E$. Note that $\eta$ must be in
        $\text{row}(X_E)$ to be sensible, and contrast can of course
        be expressed as $\bar{\eta}'\mu$ with $\bar{\eta} \in \text{col}(X_E)$ and
        $\mu=X_E\beta_E^*$.
        
        There is a finer decomposition in this context that is quite natural:\footnote{This is the
        subbundle discussion: picking coordinates on a bigger bundle
        to get a sensible restriction of a process to a submanifold of
        the base space of the bundle.}
        $$ (\eta'\OLS_E, R_{\eta,E}(y), R_E(y)) 
        $$ with
        $$ R_{\eta,E} = P_E - \bar{\eta}\bar{\eta}'.
        $$

        The selected model device of FST uses the heuristic that says
        we've selected a good model so the residual must have little
        information in $\mu$ associated to it.  Specifically, assumes
        $R_E(y) \sim N(0, R_E)$. This assumption is a spectrum, a
        researcher could choose some other features to add to $E$ they
        might think are relevant and make the assumption that $R_{M}y
        \sim N(0, R_M)$ for some $R_{M}-R_E \geq 0$. They might also
        look at $E$ and say, some of these are clearly garbage, in
        which case $R_E-R_M \geq 0$\footnote{Models where $R_E-R_M$ are not
        of definite sign are just not natural: they won't respect the
        sub-bundle restriction. When we generalize to $M \neq E$, we'll make the
        assumption throughout that $R_E-R_M$ is symmetric and has eigenvalues all
        $1$ or all $-1$, i.e. either $R_E-R_M$ is a projection or $R_M-R_E$ is a projection.}.

        {\bf Comment (don't ignore):} For the Gaussian case, when $p >
        n$ and $\sigma^2$ is unknown it is essentially unavoidable to
        make a choice of $M$ simply necessary to be able to estimate
        variance. In GLMs and Cox this wouldn't be strictly necessary. How good
        the estimate of variance is will depend on the selection pressure. Some
        results on consistency in selected models are described in \cite{xiaoying:randomized}.
        One can always randomize of course, for which our proposal here would
        end up being slightly different. Not addressed here yet.

        {\bf Comment (don't ignore):} This choice of $M$ determines an
        ancillary statistic (at least for the pre-selection density),
        when $\sigma^2$ is known, namely $R_M(y)$. If $\sigma^2$ is
        unknown, then we can proceed once $M$ is fixed, by
        conditioning on $\|R_{M}(y)\|^2_2$. This is similar to what we
        saw in Lucas' talk, as well as \cite{sqrtlasso} or other
        examples in FST.  We leave this for now, assuming that we will
        simply plugin
        $$ \hat{\sigma}^2_{M}(y) = \frac{1}{\text{Tr}(R_{M})}
        \|R_{M}y\|^2_2
        $$ as an estimate of variance.

        {\bf Comment (maybe OK to ignore):} This choice of ancillary
        is, in the selective inference setting, a choice of the
        researcher. As mentioned above, the researcher could choose $M
        \subset E$, though we would reparametrize our statistics. If
        $n>p$ we can also take $M=\{1, \dots, p\}$.  We see then, that
        the choice of $M$ corresponds to choosing an orthonormal frame
        on the $E$ strata of the normal bundle\footnote{Referring here
        to the normal bundle of the convex set $\{r \in \mathbb{R}^n:
        \|\Lambda^{-1}X'r\|_{\infty} \leq 1\}$ as it sits in $\R^n$}. The data
        selected $E$ but the researcher chooses $M$.

        {\bf Comment (safe to ignore on 1st reading):} This spectrum of choice of
        ancillary subspace (better said orthonormal frame) is related
        to ancillarity in curved exponential families and the {\em
          Efron-Hinkley} ancillary statistic. These problems are
          well-specified problems without selection.  In that setting,
          there is similarly a choice of orthonormal frame and {\em
            Efron-Hinkley} is somehow the smallest orthonormal frame
          that allows for curvature. In our problem, Efron-Hinkley
          choice would set $M=E$ if we insist on defining the target
          parameters to be $X_E^{\dagger}\mu$. In this (exponential
          family) context, the Efron-Hinkley statistic would be
          degenerate, it is simply 0.

          {\bf Comment (probably best for 2nd reading):} Once we've
          chosen $M$ we have a choice on whether to condition on the
          maximal ancillary or some projections of it. The
          Efron-Hinkley ancillary is somehow the smallest thing one
          could reasonably think about conditioning on if you want to
          see the effect of curvature in your densities. Our densities
          are flat because we are in an exponential family. FST and
          Lee et al. condition on the maximal ancillary.

          \subsection{OK, enough with the comments}
        
          Let's get back to implications of fixing a selected model and consider
          inference for a target $\eta'\beta_M(\mu)=\eta'X_M^{\dagger}\mu$.
          Recall that this parameterizes $y$ as
          $$ \left(\bar{\eta}'y, (P_M-\bar{\eta}\bar{\eta}')(y), R_M(y)\right)
          \overset{def}{=} (\eta'\OLS_M, N_{M,\eta}(y), R_M(y))
          $$ with (pre-selection) law
          $$ R_M(y) \sim N(0, R_M).
          $$

          FST tells us that we should use the following density to test
          $H_0:\eta'\beta_M=0$:
        \begin{equation}
          \label{eq:selective:density:M}
        z \mapsto f_{\theta}(z | N_{M,\eta}(y)=n) \propto \exp
        \left(-\frac{1}{2}(z-\theta)^2 \right) \cdot \P_{\mu}({\cal
          S}_E | \bar{\eta}'y=z, N_{M,\eta}(y)=n).
        \end{equation}
        Note that \eqref{eq:selective:density:M}  has marginalized
        over $R_M(y)$. To do so, we {\bf must integrate out $R_M(y)$}
        which may not be easy.

        The approach taken in FST is to condition on a finer
        $\sigma$-algebra replacing it with
        \begin{equation}
          \label{eq:selective:density:FST}
        z \mapsto f_{\theta}(z | N_{M,\eta}(y)=n, R_M(y)=r) \propto
        \exp \left(-\frac{1}{2}(z-\theta)^2 \right) \cdot
        \P_{\mu}({\cal S}_E | \bar{\eta}'y=z, N_{M,\eta}(y)=n,
        R_M(y)=r).
        \end{equation}
        To be precise, Lee et al. also mentions marginalizing over
        signs as well.
        
        Again, this is computable because
        $$ \P_{\mu}({\cal S}_E | \bar{\eta}'y=z, N_{M,\eta}(y)=n,
        R_M(y)=r) =
        \begin{cases}
          1 & \hat{E}(z,n,r)=E \\ 0 & \text{otherwise.}
          \end{cases}
        $$
        All implementation so far of Lee et al. condition on the signs as well using density
        \begin{equation}
          \label{eq:selective:density:FST}
        z \mapsto f_{\theta}(z | N_{M,\eta}(y)=n, R_M(y)=r) \propto
        \exp \left(-\frac{1}{2}(z-\theta)^2 \right) \cdot
        \P_{\mu}(\bar{\cal S}_{E,s_E} | \bar{\eta}'y=z,
        N_{M,\eta}(y)=n, R_M(y)=r).
        \end{equation}

        The polyhedral lemma of Lee et al. shows that
        $$ \P_{\mu}(\bar{\cal S}_{E,s_E} | \bar{\eta}'y=z,
        N_{M,\eta}(y)=n, R_M(y)=r) =
        \begin{cases}
          1 & z \in [L_{E,s_E}(n,r), U_{E,s_E}(n,r)] \\ 0 &
          \text{otherwise,}
          \end{cases}
        $$ and describes an algorithm to find $[L_{E,s_E}(n,r),
          U_{E,s_E}(n,r)] $.

        \subsection{So what?}

        Let's suppose we actually want to use FST within the model
        $M$.  We need to compute
        \begin{equation}
          \label{eq:selective:probability}
          \begin{aligned}
            \P_{\mu}({\cal S}_E | \bar{\eta}'y=z, N_{M,\eta}(y)=n) &=
            \bigcup_{s_E} \P_{\mu}(\bar{\cal S}_{E,s_E} | \bar{\eta}'y=z,
            N_{M,\eta}(y)=n) \\ &= \bigcup_{s_E} \P_{\mu}\left( A_{E,s_E,\lambda}(y(z,n,R_M)) \geq 0,
            I_{E,s_E,\lambda}(y(z,n,R_M)) \geq 0  \bigl | N_{\eta,M}=n, \eta'\OLS_M=z
            \right)
          \end{aligned}
          \end{equation}
        We've used the notation $y(z,n,r)$ to denote that $y$ can be
        reconstructed fully from these parameters, i.e. it reflects a
        parametrization of $y$.
        
        The basic object to compute is therefore, fixing $n$
        $$ z \mapsto \P_{\mu}\left( A_{E,s_E,\lambda}(y(z,n,R_M)) \geq 0, I_{E,s_E,\lambda}(y(z,n,R_M))
        \geq 0  \bigl | N_{\eta,M}=n, \eta'\OLS_M=z \right).
        $$ This is an integral of the law of $R_M$ over a convex set
        $K_{E,s_E}$ determined by $(z,n)$.

        How can we do this? It is clearly a difficult integral to
        compute. How can this be done? \cite{sifan} has some methods
        based on quasi-monte Carlo that are applicable.

        Alternatively, since $R_M$ is Gaussian\footnote{I've just
        introduced a slight abuse of notation here: when context calls
        for a random variable then $R_M$ should be read as
        $R_My$. When context calls for a projection matrix, it should
        be read as the corresponding projection matrix. Hopefully
        there will not be } in our model we can use a large deviations
        approximation similar to \cite{snigdha...}
        \begin{equation}
          \label{eq:LD}
        \P(R_M \in K_{E,s_E}(z,n)) \propto \exp\left(-\frac{1}{2}
        \inf_{\{r \in K_{E,s_E}(z,n)\}} \|R_Mr\|^2_2\right)
        \end{equation}

        {\bf Comment:} A closely related version of this has already been
        proposed in \cite{snigdha:jelena}, even in the non-parametric
        bootstrap context though in that work the integral only
        integrates out the randomness introduced to the problem.  That
        work also does not follow through with the union over $s_E$.
        In the bootstrap context, the parametrization of the
        statistics uses population versions of the coordinate
        systems. I.e. in constructing the random variables
        $\eta'\OLS_M, N_{M,\eta}, R_M$ it orthogonalizes with respect
        to covariances estimated via the bootstrap.  That is, in the
        non-parametric case, there is no special reason to separately
        parametrize $(N_{M,\eta}, R_M)$ as they are not independent.
        More precisely, with $\omega$ the auxiliary randomization,
        that work goes after the expression
        \begin{equation}
          \label{eq:LD:rand}
        (z,n) \mapsto \P(\omega \in \bar{K}_{E,s_E}(z,n)) \propto
          \exp\left(-\frac{1}{2} \inf_{\{\omega \in
            \bar{K}_{E,s_E}(z,n)\}} \omega' \Theta \omega\right).
        \end{equation}
        with $\Theta$ the precision of the auxiliary randomization.

        \section{A concrete proposal when $E=M$}



        In the parametric case, the form of the selection event for
        the LASSO (as will be the case for any additive polyhedral
        regularizer) there is a special structure not present
        otherwise. It is most obvious when we set $M=E$. In this
        context, the joint density of $(\eta'\OLS_E, N_{\eta,E}, R_E)$
        factorizes pre-selection. But, crucially
        \begin{equation}
          \label{eq:cond:independence}
          (\eta'\OLS_E, N_{\eta,E}) \indep R_E | \hat{E}=E,
          \hat{s}=s_E
        \end{equation}
        
        In turn, this, combined with the assumption that $R_E(y) \sim
        N(0, R_E)$
        \begin{equation}
          \P_{\mu}(\bar{\cal S}_{E,s_E} | \bar{\eta}'y=z,
          N_{E,\eta}(y)=n, R_E(y)=r) = 1_{[L_{E,s_E}(n),
              U_{E,s_E}(n)]}(z) \cdot \P_{0}(I_{E,s_E,\lambda}(R_E) \geq 0).
        \end{equation}
        Above, note that
        $$ 1_{[L_{E,s_E}(n), U_{E,s_E}(n)]}(z) = \begin{cases} 1 &
          A_{E,s_E,\lambda}P_Ey(z,n) \geq 0 \\ 0 & \text{otherwise,}
          \end{cases}
        $$ is just a different way of expressing the active
        constraint.
          
        \begin{lemma}[Selected model reference law]
          Under selected model $M=E$, under
          $H_0:\eta'\beta_E(\mu)=\theta$, the conditional law of
          $\eta'\OLS_E$ given $\hat{E}=E, N_{E,\eta}=n$ is the usual
          $N(\theta, 1)$ density modified by the factor
          \begin{equation}
            \label{eq:union}
            \sum_{s_E} 1_{[L_{E,s_E}(n), U_{E,s_E}(n)]}(z) \cdot
            \gamma_{s_E}
            \end{equation}
          with
        $$ \gamma_{s_E} \mapsto \P_0(I_{E,s_E,\lambda}(R_E) \geq 0).
        $$
          \end{lemma}

        {\bf Comment:} In words, this density is a piecewise Gaussian
        density. In FST which fixes $E$ or Lee et al. approach which
        condition on $R_E$ this final probability is always 1. When
        integrating out $R_E$ this is a number: each sign pattern gets
        a different weight. In order to use this approach we must
        compute these numbers.  We might as allow for different means
        for $R_E$ and define
          \begin{equation}
            \label{eq:sign:prob}
            \gamma_{s_E}(\mu) \overset{def}{=} \P_{\mu}(I_{E,s_E,\lambda}(R_E)
            \geq 0) = \P_{R_E\mu}(I_{E,s_E,\lambda}(R_E) \geq 0).
          \end{equation}
        
          \subsection{Approximating $\gamma_{s_E}(0)$}

          We start with a lemma used to approximate the maps
          $\gamma_{s_E}$. Recall that the inactive constraints of the
          LASSO can be expressed as
          \begin{equation}
            \label{eq:inactive}
            \|\Lambda_{-E}^{-1}X_{-E}'(R_E + \zeta_{E,s_E})\|_{\infty}
            \leq 1
            \end{equation}
          with $\zeta_{E,s_E}$ defined in \eqref{eq:sign:offsets}.

          
        \begin{lemma}[Value lemma]
          For $w \in \text{row}(P_E)$, consider the value of the
          program
          \begin{equation}
            \label{eq:dual:value}
            V_{E,s_E}(w,\lambda) = \inf_{\beta} \left(\frac{1}{2}
            \|w-X\beta\|^2_2 + s_E'\Lambda_E\beta_E +
            \|\Lambda_{-E}\beta_{-E}\|_1 \right).
          \end{equation}
The value satisfies
          \begin{equation}
            \label{eq:value:map}
\left( \inf_{z:P_Ez=0,
  \|\Lambda_{-E}^{-1}X_{-E}'(R_Ez+\zeta_{E,s_E})\|_{\infty} \leq 1}
\frac{1}{2} \|z\|^2_2 \right) = \frac{1}{2} \|w\|^2_2 -
\frac{1}{2}\|\zeta_{E,s_E}-w\|^2_2 - V_{E,s_E}(w,\lambda).
          \end{equation}
          Further, $V_{E,s_E}(\zeta_{E,s_E}, \lambda) = 0$ so that
          \begin{equation}
            \label{eq:LD:lasso}
            \begin{aligned}
            \inf_{z:P_Ez=0,
              \|\Lambda_{-E}^{-1}X_{-E}'(R_Ez+\zeta_{E,s_E})\|_{\infty}
              \leq 1}\frac{1}{2} \|z\|^2_2  &= \frac{1}{2}
            \|\zeta_{E,s_E}\|^2_2 \\ &=
            \frac{1}{2}s_E'\Lambda_E(X_E'X_E)^{\dagger}\Lambda_Es_E.
          \end{aligned}
            \end{equation}
          \end{lemma}

        {\bf Proof:} In a familiar fashion we introduce a new variable
        $\mu$ and the linear constraint $\mu=X_{-E}\beta_{-E}$ leading
        to the Lagrangian
        $$
        \begin{aligned}
        {\cal L}(z|\beta,\mu) &= \frac{1}{2} \|w-\mu\|^2_2 +
        s_E'\Lambda_E\beta_E + \lambda_{-E} \|\beta_{-E}\|_1 +
        z'(\mu-X\beta) \\ &= \frac{1}{2} \|w-\mu\|^2_2 +
        s_E'\Lambda_E\beta_E + \lambda_{-E} \|\beta_{-E}\|_1 +
        z'(\mu-X\beta) \\
        \end{aligned}
        $$ Minimizing over $\mu$ yields $\mu^*(z)=w-z$. Minimizing
        over $\beta$ yields the constraint and
        $\|\Lambda_E^{-1}X_{-E}'z\|_{\infty} \leq 1$ and
        $P_Ez=\zeta_{E,s_E}$.  Plugging in the value of
        $\hat{\mu}^*(z)$ yields
        $$ \frac{1}{2}\|z\|^2_2 - z'(w-z) = z'w - \frac{1}{2}\|z\|^2_2
        $$ subject to those two constraints.

        We conclude that
        $$ \inf_{\beta_{-E},\mu}{\cal L}(z|\beta,\mu) = z'w -
        \frac{1}{2}\|z\|^2_2
        $$ with the constraint $\|X_{-E}'R_Ez\|_{\infty} \leq
        \lambda_{-E}$, $P_Ez=\zeta_{E,s_E}$.

        As strong duality holds (the problem \eqref{eq:primal:problem}
        has a solution), the value \eqref{eq:dual:value} is
        $$
        \begin{aligned}
          \lefteqn{\sup_{z: P_Ez=\zeta_{E,s_E},
              \|\Lambda_E^{-1}X_{-E}'z\|_{\infty} \leq 1} \left(z'w -
            \frac{1}{2}\|z\|^2_2 \right)} \\ &= \sup_{z:
            P_Ez=\zeta_{E,s_E}, \|\Lambda_E^{-1}X_{-E}'z\|_{\infty}
            \leq 1} \left( \frac{1}{2}\|w\|^2_2 -
          \frac{1}{2}\|z-w\|^2_2 \right) \\ &= \sup_{z: P_Ez=0,
            \|\Lambda_E^{-1}X_{-E}'(R_Ez+\zeta_{E,s_E})\|_{\infty}
            \leq 1} \left( \frac{1}{2}\|w\|^2_2 -
          \frac{1}{2}\|R_Ez\|^2_2 - \frac{1}{2}\|\zeta_{E,s_E}-w\|^2_2
          \right) \\ &= \frac{1}{2} \|w\|^2_2 -
          \frac{1}{2}\|\zeta_{E,s_E}-w\|^2_2 + \sup_{z: P_Ez=0,
            \|\Lambda_E^{-1}X_{-E}'(R_Ez+\zeta_{E,s_E})\|_{\infty}
            \leq 1} -\frac{1}{2}\|R_Ez\|^2_2 \\ &= \frac{1}{2}
          \|w\|^2_2 - \frac{1}{2}\|\zeta_{E,s_E}-w\|^2_2 - \inf_{z:
            P_Ez=0,
            \|\Lambda_E^{-1}X_{-E}'(R_Ez+\zeta_{E,s_E})\|_{\infty}
            \leq 1} -\frac{1}{2}\|R_Ez\|^2_2 \\
        \end{aligned}
        $$ \qed

        The lemma above essentially computes \eqref{eq:LD} for this
        problem.  We therefore define $\bar{\gamma}_{E,s_E}(0)$, our
        approximation to $\gamma_{E,s_E}(0)$, as
        \begin{equation}
          \label{eq:gamma:val}
          \bar{\gamma}_{E,s_E}(0) =
          e^{-\frac{1}{2}\|\zeta_{E,s_E}\|^2_2}.
          \end{equation}

        \subsection{Finally...}

        Equation \eqref{eq:gamma:val} allows us to make the following proposal for the
        law of $y \sim N(X_E\beta_E^*, I) | \hat{E}(y)=E$. On the set
        $\hat{E}(y)=E$, the signs $\hat{s}(y) = \hat{s}(\beta_E(y)) \in \{-1,1\}^E$ are well-defined as is
        $$
        \REG_E(y) = \OLS_E(y) - X_E^{\dagger}\zeta_{E,\hat{s}(\OLS_E(y))}.
        $$

        Applying \eqref{eq:gamma:val} we see that on
        the event $\hat{E}(y)=E$ the (approximate) selective density of
        $\OLS_E(y)$ is
        \begin{equation}
          \label{eq:proposal}
          \beta_E \mapsto \exp\left(-\frac{1}{2} \beta_E'X_E'X_E\beta_E - \frac{1}{2}\hat{s}(\beta_E)'(X_E'X_E)^{\dagger}\hat{s}(\beta_E)\right).
          \end{equation}
        This is a mixture of exponential families, each with different support. 

        Let $\OLS_E=\OLS_E(y)$ denote the observed value and consider an affine line of interest through
        $\OLS_E$ such with value $\OLS_E$ at $t=0$:
        $$t \mapsto \OLS_E + (t- \eta'\OLS_E) \cdot \xi_{\eta,E}' \OLS_E.$$
        The direction is defined by $$
        \xi_{\eta,E} = \frac{1}{\eta'(X_E'X_E)^{\dagger}\eta} \cdot (X_E'X_E)^{\dagger} \eta \eta'.
        $$ 
        Define the sign along the path as
        $$
        \bar{s}_{\eta,E}(t) = \textrm{sign}\left( \OLS_E + (t - \eta'\OLS_E) \xi_{\eta,E} \right)
        \in \{-1,0,1\}^E
        $$

        Define the limit signs along this line as
        $$
        \begin{aligned}
          \bar{s}^{\pm}_{\eta, E} &= \lim_{t \to \pm \infty} \bar{s}_{\eta, E}(t)  \in \{-1,1\}^E \\
          &= \begin{cases}
            \pm \; \text{sign} (e_j'\xi_{\eta,E}) & e_j'\xi_{\eta,E}) \neq 0 \\
            \text{sign}(e_j'\OLS_{E}) & e_j'\xi_{\eta,E}=0.
            \end{cases}
        \end{aligned}
        $$
        This establishes that as $t \to \pm \infty$ all coefficients are non-zero.

        The reference density to test $H_0:\eta'\beta_E^*=\theta$
        conditional on $N_{\eta, E}$ is 
        $$
        t \mapsto
        \exp\left(-\frac{1}{2}\left(\eta'(X_E'X_E)^{\dagger}X_E\eta\right)^{-1}(t-\theta)^2 - \frac{1}{2} \bar{s}_{\eta,E}(t)' (X_E'X_E)^{-1} \bar{s}_{\eta,E}(t)\right)
        $$
        restricted to $\left\{t: \bar{s}_{\eta,E}(t) \in \{-1,1\}^E\right\}$. That is,
        restricted to the event that the entire set $E$ is selected.

        The support of this density is the complement of the interval
        $$
        \left\{t: A_{E,s_E,\lambda}(\OLS_E + (t - \eta'\OLS_E)) < 0 \ \forall s_E \in \{-1,1\}^E \right\}.
        $$
        
        \section{Selected model for which $P_M \neq P_E$}

        Details to come...

\end{document}
