\documentclass{article}

\input{../preamble.sty}
\usepackage{lmodern}

\newcommand{\ag}[1]{{\bf{{\red{[{AG: #1}]}}}}}
\newcommand{\InnerProduct}[2]{\langle #1,#2 \rangle}
\newcommand{\Norm}[1]{\|#1\|}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\br}{{\bf r}}
\newcommand{\bs}{{\bf s}}
\newcommand{\bt}{{\boldsymbol t}}
\newcommand{\bH}{{\boldsymbol H}}
\newcommand{\Newton}{\mathrm{Newton}}
\newcommand{\DummyField}{{\tt f}}
\newcommand{\DummyGradient}{{\bf g}}
\newcommand{\InformationMatrix}{{\boldsymbol J}}
\newcommand{\lin}{\mathrm{lin}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\newcommand{\Exp}{\mathrm{Exp}}

\newcommand{\RandomField}{Y}
\newcommand{\bLambda}{{\boldsymbol \Lambda}}
\newcommand{\bGamma}{{\boldsymbol \Gamma}}
\newcommand{\Err}{\mathrm{Err}}
\newcommand{\bQ}{{\boldsymbol Q}}
\newcommand{\bJ}{{\boldsymbol J}}
\newcommand{\bV}{{\boldsymbol V}}
\newcommand{\bI}{{\boldsymbol I}}
\newcommand{\bC}{{\boldsymbol C}}
\newcommand{\convweak}{\overset{d}{\to}}

\newcommand{\appropto}{\mathrel{\vcenter{
			\offinterlineskip\halign{\hfil$##$\cr
				\propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

\renewcommand{\thealgorithm}{\arabic{algorithm}}

\title{ {\bf Inference for Local Maxima of a Gaussian Random Field} \\ Pivots}

\begin{document}
	
\maketitle
\RaggedRight

\section{Summary of second-order notes}
We begin by recalling the setup and a few conclusions of the second-order notes. We observe
\begin{equation}
	Y_t = \mu_t + \epsilon_t, \quad {t \in \mc{T}},
\end{equation}
where the signal $\mu \in C^2(\mc{T})$, the noise $\epsilon \sim N(0,C)$, and $C(t,t) = 1$ for all $t$. Recall $T^*$ denotes the local maxima of $\mu$, and $\hat{T}_u$ the local maxima of $Y$ that are of height at least $u$. 

At a particular $t^* \in T^*$, recall that $\nabla^2 \mu_{t^*}$ is the Hessian of $\mu$ at $t^*$, and let $\delta := \{\lambda_{\min}(\nabla^2\mu_{t^*})\}^{-1}$ be one over the minimum eigenvalue of this Hessian. Under regularity conditions on $\mu,C$ and $\mc{T}$, in the high-curvature limit $\delta \to 0$, with exponentially high probability there will be a unique local maximum $\hat{t} \in B := B(t^*,C\delta\sqrt{\log(1/\delta)})$; in other words, $N_u(B) \leq 1$, with $N_u(B) = 1$ only if $Y_{\hat{t}} \geq u$. 

The second-order notes work out an approximation the density of $(\hat{t},Y_{\hat{t}})$ conditional on $N_u(B) = 1$. The approximation is accurate to relative error $O(\delta^2)$ at typical values of its arguments $(t,y)$. In what follows, recall that we use the notation $H_t(u^+)$ for a deterministic asymptotic equivalent to $-\nabla^2 Y_{\hat{t}}$, where
\begin{equation}
	H_t(y) := \E[\nabla^2 Y_t|\nabla Y_t = 0, Y_t = y], \quad u^{+} := \max(u,\mu_{t^*}),
\end{equation}
and that the Goldilocks expression for the conditional precision matrix of $\hat{t}$ is
\begin{equation}
	G_t(y) := -\nabla^2 \mu_t \Lambda_t^{-1} H_t(y).
\end{equation}

\begin{theorem}
	\label{thm:approximate-joint-distribution}
	Assume the regularity conditions. Then as $\delta \to 0$, the joint density of $(\hat{t},Y_{\hat{t}})$ at $t = t^* + h,y = \mu + \Delta$ is
	\begin{equation}
		\label{eqn:approximate-joint-density}
		\begin{aligned}
			p(t,y) & = \Big(1 + O(h^2) + O(\delta^2) + O(\Delta h^2) + O(\Delta\delta^2)\Big) \cdot \bar{p}(t,y), \quad {\rm where} \\
			\bar{p}(t,y) & := c(u) \cdot\Big(1 + d(y,h) - R(h)/2 - d(h)/2\Big) \cdot \det(H(y)) \cdot \exp\Big(-\frac{(y - \mu)^2}{2}\Big) \cdot \exp\bigg(-\frac{h' G(y)h}{2}\bigg),
		\end{aligned}
	\end{equation}
	and the constant of proportionality is
	\begin{equation}
		\label{eqn:approximate-joint-density-constant-of-proportionality}
		c(u) = \frac{1}{\sqrt{(2\pi)^{d + 1}}} \exp\bigg(\frac{(u^+ - \mu)\tr(\{H(u^{+})\}^{-1}\Lambda)}{2}\bigg)
		\frac{1}{\Psi(u - \mu - \frac{1}{2}\tr(\{H(u^{+})\}^{-1}\Lambda))}.
	\end{equation}
	Moreover, the marginal density of $Y_{\hat{t}}$ is 
	\begin{equation}
		\label{eqn:approximate-marginal-density-height}
		\begin{aligned}
			p(y) & = \big(1 + O(\delta^2) + O(\Delta \delta^2) + O(|y - u|^2 \delta^2)\big) \cdot \bar{p}(y), \quad {\rm where} \\ 
			\bar{p}(y) & := \frac{1}{\sqrt{2\pi}}\frac{\exp\Big(-\big(y - \mu - \frac{1}{2}\tr(\{H(u^+)\}^{-1}\Lambda)\big)^2/2\Big) }{\Psi(y - \mu - \frac{1}{2}\tr(\{H(u^+)\}^{-1}\Lambda))}.
		\end{aligned}
	\end{equation}
	Finally, the conditional density of $\hat{t}$ given $Y_{\hat{t}} = y$ is 
	\begin{equation}
		\label{eqn:approximate-conditional-density-location}
		\begin{aligned}
			p(t|y) & = \big(1 + O(h^2) + O(\delta^2) + O(\Delta h^2) + O(\Delta\delta^2) + O(|y - u|^2 \delta^2)\big) \cdot \bar{p}(t|y), \quad {\rm where} \\
			\bar{p}(t|y) & := \Big(1 + d(y,h) - d(h)/2 - R(h)/2\Big) \cdot \frac{\sqrt{\det(G(y))}}{\sqrt{(2\pi)^d}} \cdot \exp\Big(-\frac{1}{2}h'G(y) h\Big).
		\end{aligned}
	\end{equation}
\end{theorem}
Above the functions $d(y,h), R(h)$ and $d(h)$ are
\begin{equation}
\begin{aligned}
	\label{eqn:first-order}
	d(y,h) & = \tr\big(\{H(y)\}^{-1} \dot{H}_{t^*}(y)(h)\big) \\
	d(h) & = \tr(\Lambda^{-1} \dot{\Lambda}(h)), \\
	R(h) & = h' \nabla^2 \mu \Lambda^{-1} \{\nabla^3\mu(h^2)\} +  h'\nabla^2 \mu \Lambda^{-1}\{\dot{\Lambda}(\Lambda^{-1}h)\} h.
\end{aligned}
\end{equation}
The notation $\dot{H}(y)(h)$ represents $\|h\|$ times the directional derivative of the mapping $t \mapsto H_t(y)$ in the direction $h$, and likewise for $\dot{\Lambda}$. We could give more detailed algebraic expressions for these terms, but what's more important to remember is (1) their magnitude $d(y,h) = O(h), d(h) = O(h), R(h) = O(\delta^{-2}h^3)$, and (2) $d(y,h)$ and $d(h)$ are linear in $h$ while $R(h)$ is cubic in $h$. Thus, these terms represent the first-order disturbance of $\bar{p}(t|y)$ away from a Gaussian density. 

\section{Pivot for height}
\ag{TO COME}

\section{Pivot for location}

\subsection{Pivot with Goldilocks variance}
We begin by considering the following candidate pivot
$$
W = (\hat{t} - t^*)' G(Y_{\hat{t}}) (\hat{t} - t^*).
$$
The conditional distribution of $W$ given $Y_{\hat{t}} = y$ is 
\begin{align*}
	\Q_{B,u}\big(W \leq a|Y_{\hat{t}} = y\big) 
	& = \int_{h'Gh \leq a} p(t^* + h|y) \,dh \\
	& = \int_{h'Gh \leq a} \big(1 + O(h^2) + O(\delta^2) + O(\Delta h^2) + O(\Delta\delta^2) + O(|y - u|^2 \delta^2)\big) \cdot \bar{p}(t^* + h|y) \,dh \\
	& = \int_{h'Gh \leq a} \big(1 + O(h^2) + O(\delta^2) + O(\Delta h^2) + O(\Delta\delta^2) + O(|y - u|^2 \delta^2)\big) \\
	& \times  \Big(1 + d(y,h) - d(h)/2 - R(h)/2\Big) \phi_{0,G^{-1}}(h) \,dh,
\end{align*}
where $G = G(y)$, and $\phi_{0,G^{-1}}$ is a multivariate Gaussian density with mean $0$ and variance $G^{-1}$.  Let us suppose now that $a = O(1)$ and $y \geq \mu_{t^*} - o(\delta^{-1})$. The latter implies that $\{\lambda_{\min}(G)\}^{-1} = O(\delta^2)$, and so $h'Gh \leq a$ means $h = O(\delta)$. From this we find that the error terms above simplify, and
\begin{equation}
	\label{pf:location-pivot-0}
	\Q_{B,u}\big(W \leq a|Y_{\hat{t}} = y\big) = \int_{h'Gh \leq a} \Big(1 + d(y,h) - d(h)/2 - R(h)/2\Big) \phi_{0,G^{-1}}(h) \,dh + O(\delta^2) + O(\Delta \delta^2) + O(|y - u|\delta^2). 
\end{equation}
Changing variables to $z = G^{1/2}h$, the main term above can be written as
\begin{equation}
	\label{pf:location-pivot-1}
	\int_{z'z \leq a} \Big(1 + d(y,G^{-1/2}z) - d(G^{-1/2}z)/2 - R(G^{-1/2}z)/2\Big) \phi_{0,I}(z) \,dz = \int_{z'z} \phi_{0,I}(z) \,dz,
\end{equation}
The equality follows by~\eqref{eqn:gaussian-integrals-truncated-moments}, and simply says that the first and third moments of a mean-zero Gaussian, truncated to belong to a ball of radius $a$, are zero. From~\eqref{pf:location-pivot-0} and~\eqref{pf:location-pivot-1} we see that
\begin{equation}
	\Q_{B,u}(W \leq a|Y_{\hat{t}} = y) = \chi_d^2(a) + O(\delta^2) + O(\Delta \delta^2) + O(|y - u|\delta^2), 
\end{equation}
where $\chi_d^2$ is the distribution function of a chi-squared random variable with $d$ degrees of freedom. 

\section{Technical Results}

\subsection{Gaussian integrals}
\label{subsec:gaussian-integrals}
Consider a multivariate Gaussian $Z \sim N(0,G^{-1})$, and let $B$ be a ball centered at $0$ of radius $r$. For any vector $x \in \Rd$ and array $A \in \R^{d \times d \times d}$,
\begin{equation}
	\label{eqn:gaussian-integrals-truncated-moments}
	\E\Big[(\sum x_i Z_i) \cdot \1(\|Z\| \leq r)\Big] = 0, \quad \E\Big[(\sum A_{ijk} Z_i Z_jZ_k) \cdot \1(\|Z\| \leq r)\Big] = 0.
\end{equation}



  



	
\end{document}