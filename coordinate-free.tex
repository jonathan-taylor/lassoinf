\documentclass{article}

\input{../preamble.sty}
\usepackage{lmodern}

\newcommand{\ag}[1]{{\bf{{\red{[{AG: #1}]}}}}}
\newcommand{\InnerProduct}[2]{\langle #1,#2 \rangle}
\newcommand{\Norm}[1]{\|#1\|}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\br}{{\bf r}}
\newcommand{\bs}{{\bf s}}
\newcommand{\bt}{{\boldsymbol t}}
\newcommand{\bH}{{\boldsymbol H}}
\newcommand{\Newton}{\mathrm{Newton}}
\newcommand{\DummyField}{{\tt f}}
\newcommand{\DummyGradient}{{\bf g}}
\newcommand{\InformationMatrix}{{\boldsymbol J}}
\newcommand{\lin}{\mathrm{lin}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\newcommand{\Exp}{\mathrm{Exp}}

\newcommand{\RandomField}{Y}
\newcommand{\bLambda}{{\boldsymbol \Lambda}}
\newcommand{\bGamma}{{\boldsymbol \Gamma}}
\newcommand{\Err}{\mathrm{Err}}
\newcommand{\bQ}{{\boldsymbol Q}}
\newcommand{\bJ}{{\boldsymbol J}}
\newcommand{\bV}{{\boldsymbol V}}
\newcommand{\bI}{{\boldsymbol I}}
\newcommand{\bC}{{\boldsymbol C}}
\newcommand{\convweak}{\overset{d}{\to}}

\newcommand{\appropto}{\mathrel{\vcenter{
			\offinterlineskip\halign{\hfil$##$\cr
				\propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

\renewcommand{\thealgorithm}{\arabic{algorithm}}

\title{ {\bf Inference for Local Maxima of a Gaussian Random Field} \\ Coordinate free intensity}

\begin{document}
	
	\maketitle
	\RaggedRight
	
	Consider a generic signal-plus-noise random field
	\begin{equation}
		\label{eqn:signal-plus-noise}
		Y_t = \mu_t + \epsilon_t, \quad {\rm for} ~ t \in \mc{T}.
	\end{equation}

        We're going to consider $\mc{T}$ an abstract manifold for the moment, meaning that any {\em local calculations}
        must be done in a chart $\varphi: U \rightarrow \R^d$ with $U \subset \mc{T}$.
        It is also a Riemannian manifold with metric: given two vector fields $X, W$:
\begin{equation}
        g(X,W)_t = \E[X_t\epsilon * W_t\epsilon].
\end{equation}

For open $A \subset \mc{T}$ and $B \subset \R$ we are interested in the random variable
\begin{equation}
  \# \left\{(t,y): t \in A, dY_t = 0, Y_t \in B, \text{$t$ is a local maximum of $Y$}  \right\}.
  \end{equation}
Note that this random variable is independent of a chart: it is coordinate-free. The extra condition
about the local maximum may seem coordinate based as it would seem to depend on the Hessian
in that coordinate. However, the property of being a local maximum of a $C^2$ function is independent of any chart.

Hence, the expected measure  is also coordinate-free.
$$
(A, B) \mapsto \nu(A \times B) \overset{def}{=} \E \left[\# \left\{(t,y): t \in A, dY_t = 0, Y_t \in B, \text{$t$ is a local maximum of $Y$}  \right\}\right] 
$$
The Kac-Rice formula gives a formula for the intensity of this measure in any coordinate system. Under enough assumptions,
$\nu$ will have a Radon-Nikodym density with respect to the product measure on $\mc{T} \times \R$ equipped with the natural product metric.
This Radon-Nikodym density will of course also be coordinate-free. Let's compute it.

The computation relies first on the proper definition of the Hessian. For any $f \in C^2(\mc{T})$ and any smooth vector fields $X, W$, the
Hessian $\nabla^2 f(X, W): \mc{T} \rightarrow \R$ can be defined through 
\begin{equation}
\begin{aligned}
\nabla^2 f(X, W)_t = 
X_t(Wf) - g(\nabla_XW, \nabla f) = X_t(Wf) -  \sum_{i,j} \Cov(X_t(W\epsilon), V_{i,t}\epsilon) g^{ij}_{V,t} V_{j,t}f
\end{aligned}
  \end{equation}
with $Wf_t = W_tf$ and $V=(V_1, \dots, V_d)$ any set of frames with $g_{ij,V}(t) = g(V_i, V_j)_t$ and $g^{ij}_V =(g_{V})^{-1}$. We could
take the elements of $V$ to be coordinate vector fields in some chart if we wanted a local formula for the Hessian.
The Hessian is symmetric (expected) and it behaves tensorially (formally it's a module over smooth functions on $\mc{T}$). Let $M_t: T_t\mc{T} \rightarrow T_t\mc{T}$ be a section of linear maps on $T(\mc{T})$, i.e.
a set change of basis matrices, one for each $t$. Then
\begin{equation}
\nabla^2 (a \cdot X + b \cdot \bar{X},W) = a \cdot \nabla^2 (X, W) + b \cdot \nabla^2 (\bar{X}, W).
  \end{equation}
In fact, this even tells us that the vector fields $X$ and $W$ don't really need to be smooth!

As in Theorem 12.4.1 of ``Random fields and geometry'', let's now fix a set of orthonormal frames $(E_1, \dots, E_d)$. The measure $\nu$
can be written as
\begin{equation}
\begin{aligned}
\nu(A \times B) &= \int_A \E \left[\det(-\nabla^2 Y(E_i,E_j)_t) 1_{{\cal N}}(\nabla^2 Y(E_i,E_j)_t) 1_B(Y_t) \biggl \vert \nabla Y^E_t=0\right] \phi_{\nabla Y^E_t}(0) \text{Vol}_g(dt) \\
 &= \int_A \int_B \underbrace{ \E \left[\det(-\nabla^2 Y(E_i,E_j)_t) 1_{{\cal N}}(\nabla^2 Y(E_i,E_j)_t) \biggl \vert \nabla Y^E_t=0, Y_t=y\right] \phi_{Y_t|\nabla Y^E_t=0}(y) \; \phi_{\nabla Y^E_t}(0)}_{\rho(t,y)} \; dy \; \text{Vol}_g(dt) \\
\end{aligned}
  \end{equation}
with $\text{Vol}_g$ the Riemannian measure. Above, $\nabla Y^E_t = (E_{1,t}Y, \dots, E_{d,t}Y)$ is the gradient of $Y$ at $t$ read off in the frame $E$, with
$\nabla^2 Y(E_i,E_j)_t$ defined similarly. This expression $\rho(t,y)$ is coordinate-free: it does not even depend on our choice of orthonormal frames.

How does this relate to what we would get if we worked in a coordinate chart $\varphi(A)$ and worked with the coordinate $\partial t_i$ vector fields? In local coordinates $t=\varphi(u)$ for some chart
$\varphi:U \to \R^d$, we will
typically write
\begin{equation}
\Lambda_{ij,t} = g\left(\frac{\partial}{\partial t_i}, \frac{\partial}{\partial t_j}\right)_t.
  \end{equation}
Let's pick a square root $\Lambda_t^{1/2}$ such that $\Lambda_t^{-1/2} \Lambda_t (\Lambda_t^{-1/2})'=I_{d \times d}$. Then, we can find a specific set of orthonormal vector fields by
$$
E_{i,t} = \sum_j \Lambda^{-1/2}_{ij,t} \frac{\partial}{\partial t_j} \biggl|_t.
$$
With this frame, we see\footnote{A small note here: generally speaking
\begin{equation}
\nabla^2 Y(\partial t_i, \partial t_j)_t \neq \frac{\partial^2 Y}{\partial t_i \partial t_j}\biggl|_t
  \end{equation}
as the RHS is missing the part related to the connection, which makes the RHS non-tensorial. This relation is true
when $t$ is a critical point of $Y$. Therefore under the conditioning
$\nabla Y^E_t=0$ the conditional expectation of LHS is the same as the RHS.}
$$
\nabla^2 Y(E_i,E_j)_t = \Lambda^{-1/2}_t \nabla^2 Y(\partial/\partial t_i, \partial/\partial t_j)_t (\Lambda^{-1/2}_t)'
$$

Further, setting
$$
\partial Y_t = \left(\frac{\partial Y}{\partial t_1}\biggl|_t, \dots, \frac{\partial Y}{\partial t_d}\biggl|_t \right)
$$
we see that
$$
\phi_{\nabla Y_t^E}(0) = \det(\Lambda_t^{1/2}) \phi_{\partial Y_t}(0).
$$

Putting these these facts together, we see that the following expression is coordinate-free (and even valid with non-constant variance) and computable in any coordinate system we like:
$$
\begin{aligned}
\rho(t,y) = \det(\Lambda_t)^{-1/2} \E \left[\det\left(-\frac{\partial^2 Y}{\partial t_i \partial t_j} \biggl|_t\right) 1_{{\cal N}}\left(-\frac{\partial^2 Y}{\partial t_i \partial t_j} \biggl|_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] \phi_{Y_t|\partial Y_t=0}(y) \; \phi_{\partial Y_t}(0).
\end{aligned}
$$

Alternatively, we could write
$$
\begin{aligned}
\rho(t,y) &= (2\pi)^{-d/2} \det(\Lambda_t)^{-1} \E \left[\det\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t\right) 1_{{\cal N}}\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] \phi_{Y_t|\partial Y_t=0}(y) \; e^{-\frac{1}{2}g(\nabla \mu_t,\nabla \mu_t)} \\
 &= (2\pi)^{-d/2} \E \left[\det\left({\cal H}_t\Lambda_t^{-1}\right) 1_{{\cal N}}\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] \phi_{Y_t|\partial Y_t=0}(y) \; e^{-\frac{1}{2}g(\nabla \mu_t,\nabla \mu_t)} \\
\end{aligned}
$$
with ${\cal H}_{t,ij}$ the negative of the matrix of \nabla^2 Y_t$  in the basis $\partial t_i$ at $t$..

In the constant variance case (with variance 1), this simplifies to
$$
\begin{aligned}
\rho(t,y) = (2\pi)^{-(d+1)/2}  \E \left[\det\left({\cal H}_t\Lambda_t^{-1}\right) 1_{{\cal N}}\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] e^{-\frac{1}{2}(y-\mu_t)^2} \; e^{-\frac{1}{2}g(\nabla \mu_t,\nabla \mu_t)}
\end{aligned}
$$

\section{Taylor series on a Riemannian manifold}

%\\\\\\\ protect merge

We will want to expand $\rho(y,t)$ in some form of Taylor
expansion. On a manifold, using a Taylor expansion in a given
coordinate system can obscure things in different coordinate
systems. On a Riemannian manifold, it is simplest to derive a Taylor expansion
at $t$ in some small ball $B(t^*,r)$ by considering the geodesic connecting them.

Define $\log_{t^*}(t)$ to be the inverse of the exponential map's restriction to $T_{t^*}\mc{T}$:
$$
\exp_{\mc{T}}(t^*, \log_{t^*}(t)) = t
$$
and let $\gamma = \gamma_t:[0, \|\log_{t^*}(t)\|]$ be the geodesic connecting $t^*$ to $t$. The map $\log_{t^*}(t)$ will be well defined
on small balls around $t^*$.

For a smooth function $f:\mc{T} \rightarrow \R$ consider the map $\bar{f} = f \circ \gamma: [0, \|\log_{t^*}(t)\|] \rightarrow \R$.
The value $\bar{f}$ can be expressed using the usual one-dimensional Taylor's theorem. Up to order $k$ with a Taylor remainder it would be
$$
 \left|\bar{f}(r) - \sum_{j=0}^k \frac{r^j}{j!} \frac{d^j\bar{f}}{dr^j}(0) \right| \leq  \frac{r^{k+1}}{(k+1)!}\frac{d^{k+1}f}{dr^{k+1}} (\bar{r}(r))
$$
where $\bar{r} \in [0, r]$. For a H\"older type remainder for $f \in C^{k+\alpha}$ we would have
$$
 \left|\bar{f}(r) - \sum_{j=0}^k \frac{r^j}{j!} \frac{d^j\bar{f}}{dr^j}(0) \right| \leq  C_{\alpha}(\gamma) r^{\alpha}.
$$

 The key to extend this argument based on a geodesic to a more familiar form of Taylor series (and a natural one for Riemannian manifolds)
 is to note that along a geodesic, the derivatives implicated above will be expressible in terms of the $k$-th order covariant
 derivatives of $f$.

\subsection{Covariant differentiation}

The covariant derivatives of a function $f$ are tensor fields on $\mc{T}$, which means they behave nicely
as functions of $t$.\footnote{Specifically multilinearly as a module over smooth functions on $\mc{T}$. This,
in turn means they are easy to relate between different coordinate systems at any $t$.}
Let $X_i$ denote vector fields, however many as needed, with $\nabla_XY$ denoting
the covariant derivative of the vector field $Y$ with respect to $X$. The covariant
derivatives of a function are defined recursively:
$$
\begin{aligned}
  \nabla^1 f(X_1)_t &= g(\nabla f, X_1)_t \\
  &= X_{1,t}f \\
  \nabla^2 f(X_2, X_1)_t &= X_{2,t}(\nabla^1 f(X_1)) - \nabla^1 f(\nabla_{X_2}X_1)_t \\
  \nabla^k f(X_k, X_{k-1}, \dots, X_1)_t &= X_{k,t}(\nabla^{k-1} f(X_{k-1}, \dots, X_1)) -
  \nabla^{k-1} f(\nabla_{X_k}X_{k-1}, X_{k-2}, \dots, X_1)_t - \\
&  \qquad   \nabla^{k-1} f(X_{k-1}, \nabla_{X_k}X_{k-2}, \dots, X_1)_t - \\
  & \qquad \nabla^{k-1} f(X_{k-1}, \dots, \nabla_{X_k}X_1)_t.
  \end{aligned}
$$

In terms of symmetry, we note that beyond second order, covariant derivatives of a function are not symmetric. We can however project these derivatives
(or any tensor) 
onto the symmetric tensors. We denote this projection with a superscript
$$
A^{\mathrm{Sym}}(X_1, \dots, X_k) = \frac{1}{k!} \sum_{\sigma \in S_k} A(X_{\sigma(1)}, \dots, X_{\sigma(k)}).
$$

\begin{lemma}
\label{lem:geodesic}
  For $r \in [0, \|\log_{t^*}(t)\|]$ the following holds:
  $$
\frac{d^j\bar{f}}{dr^j}(r) = \nabla^j f_{\gamma(r)}(\dot{\gamma}(r), \dots, \dot{\gamma}(r)) =  \nabla^j f_{\gamma(r)}^{\mathrm{Sym}}(\dot{\gamma}(r), \dots, \dot{\gamma}(r))
  $$
  \end{lemma}
The proof of this is by induction starting at $j=2$ combined with the fact that $(\nabla_{\dot{\gamma}}\dot{\gamma})_{\gamma(r)}=0$ for $r \in [0, \|\log_{t^*}(t)\|]$.

We can now state a coordinate-free version of Taylor's theorem.
\begin{theorem}[Taylor's theorem]
\label{thm:taylor}
  Suppose $f \in C^{k+\alpha}(\mc{T})$ for $\alpha \in (0, 1]$ then, for $t \in B_{t^*}(r)$ with $r$ sufficiently small such that $\log_{t^*}$ is well-defined
    over $B_{t^*}(r)$. Then,
  \begin{equation}
    \label{eq:taylor:theorem}
\begin{aligned}
  \lefteqn{    \left|f(t) - \sum_{j=0}^k \frac{1}{j!} \nabla^j f^{\mathrm{Sym}}_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)) \right| } \\
  & \qquad =     \left|f(t) - \sum_{j=0}^k \frac{1}{j!} \nabla^j f_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)) \right| \\
  & \qquad \leq C(f,r) \|\log_{t^*}(t)\|^{k+\alpha}.
\end{aligned}
    \end{equation}
The constant $C(f, r)$ can be taken to be (up to universal constants)
\begin{equation}
  C(f,r) = \begin{cases} \frac{1}{(k+1)!}\sup_{t \in B(t^*,r)} \|\nabla^{k+1} f^{\mathrm{Sym}}_t\| & \alpha=1 \\
    \inf_{E_{t^*}}  C_{\alpha,E_{t^*}}(\bar{f}^{E_{t^*}}, r) & \alpha \in (0, 1)
\end{cases}
\end{equation}
  with $E_{t^*}$ denoting a choice of orthonormal basis for $T_{t^*}\mc{T}$ with corresponding
  normal coordinates $\varphi^{E_{t^*}}:\mc{T} \rightarrow \R^d$ at $t^*$. The quantity $C_{\alpha,E_{t^*}}(\bar{f}^{E_{t^*}}, r)$ is a coordinate-based H\"older constant.
  That is, if we define $\bar{f}^{E_{t^*}} = f\circ (\varphi^{E_{t^*}})^{-1}: \R^d \rightarrow \R$, then for each choice $E_{t^*}$ we can pick a H\"older constant:
  $$
\left| \bar{f}^{E_{t^*}}(h) - \sum_{j=0}^k \frac{1}{j!} \nabla^j \bar{f}^{E_{t^*}}_0(h, \dots, h) \right| \leq \|h\|^{\alpha} C_{\alpha, E_{t^*}}(\bar{f}^{E_{t^*}}, r) \ \forall \; h \in \varphi^{E_{t^*}}(B(t^*,r)).
  $$
  \end{theorem}

\subsection{Taylor's theorem for tensor fields}

Covariant derivatives of a tensor field are similary defined. Suppose $A$ is a $k$-th order (covariant) tensor field
$$
A_t:\otimes^k T_t\mc{T} \to \R
$$
then we can define a $(k+1)$ order covariant tensor field by
$$
\begin{aligned}
  \nabla A(Y, X_k, \dots, X_1)_t &= Y_{t}(A(X_k, \dots, X_1)) - A(\nabla_{Y}X_k, \dots, X_1)_t  \\
  &  \qquad  A(X_{k}, \nabla_{Y}X_{k-1}, \dots, X_1)_t - \\
  & \qquad A(X_{k}, \dots, \nabla_{Y}X_1)_t.
\end{aligned}
$$

Viewing $\nabla^{k} f$ as a tensor field, we see that $\nabla ( \nabla^k f)(X)$, to be read as the covariant derivative of the tensor $\nabla^k f$ in the direction
$X$, would be a tensor field of order $k$ satisfying
$$
\nabla(\nabla^k f)(Y)(X_1, \dots, X_k) = (\nabla^{k+1} f)(Y, X_1, \dots, X_k).
$$
In other words $\nabla (\nabla^k f) = \nabla^{k+1} f$.

We conclude by writing out a corresponding version of Taylor's theorem for tensor fields. For functions, we simply applied
the usual Taylor theorem on a curve. Repeated differentiation of a function is the same
as repeated application of the connection on functions. That is,
$$
\frac{d^k \bar{f}}{dr^k}\biggl|_r = \bar{\nabla}_{\dot{\gamma}} \left(\bar{\nabla}_{\dot{\gamma}}\left(\dots \left(\bar{\nabla}_{\dot{\gamma}}f\right) \right)\right)
$$
where $(\bar{\nabla}_{X}f)$ is  the function $t \mapsto \nabla f(X)_t$. We will write repeated applications of the connection\footnote{We will not use this notion
of repeated application of the connection elsewhere. It is used here to differentiate it from covariant differentiation.}as
$$
\bar{\nabla}^k_{(X_k, \dots, X_1)} f = \bar{\nabla}_{X_k}\left(\bar{\nabla}_{X_{k-q}} \left( \dots \left(\bar{\nabla}_{X_1}f\right)\right)\right).
$$
Importantly, the above expression is not tensorial in the $X_i$'s.

This repeated application of the connection is the most
natural way to differentiate a tensor of the form
$$
\sum_{s \in S} A_s \otimes_{j \in s} U_j
$$
for some subsets $s$ of fixed size, say $k$ for some set of vector fields ${\cal U} = \{U_1, \dots\}$. Indeed, this is how one would proceed in the Euclidean case. In the Euclidean case, of course one can express the tensor in some fixed basis, which is
not possible in general for a Riemmanian manifold.

Let's consider the simplest case, a 2-tensor of the form
$$
A = \sum_{i,j} a_{ij} U_i \otimes U_j.
$$
Now, consider directions we might want to differentiate this tensor. Even in the Euclidean case, the set of vector fields ${\cal U}$ may not be the standard
coordinate vector fields.

In the Euclidean case, we would likely look to differentiate $a_{ij}$ as well as $U_i$ and $U_j$ in the standard coordinate vector fields.
This differentiation corresponds exactly to repeated applications of the (Euclidean) connection to the terms $a_{ij}, U_i, U_j$.
After having
differentiated sufficiently we would expect to have some form of Taylor's theorem to evaluate the remainder.

Hence, for a tensor $A$, let's consider, for $r \in [0, \|\log_{t^*}(t)\|]$ the term
$$
A(r) - \sum_{j=0}^k \bar{\nabla}^k_{(\dot{\gamma}(r), \dots, \dot{\gamma}(r))}A(r).
$$
The following is a straightforward generalization of Lemma \cite{lem:geodesic}.
\begin{lemma}
$$
\bar{\nabla}^k_{(\dot{\gamma}(r), \dots, \dot{\gamma}(r))}A= \nabla^k A_{\gamma(r)}\left(\dot{\gamma}(r), \dots, \dot{\gamma}(r)\right).
$$
Or, repeated applications of the connection to a tensor along the geodesic computes covariant derivatives of the tensor. 
\end{lemma}
The proof is again inductive, starting with the identity $\nabla^2 A(X, Y) = (\nabla_X \nabla_Y - \nabla_{\nabla_XY})A$ and applying the geodesic condition.

In order to state our version of Taylor's thereom, it is useful to define a partial symmetrization of a $k$-tensor. For $q \leq k$, define the first $q$ symmetrization
$$
A^{\mathrm{Sym,q}}(X_k,\dots, X_1) = \sum_{\sigma \in S(q)} A(X_{k+1-\sigma(1)}, \dots, A_{k+1-\sigma(q)}, X_{k-q}, \dots, X_1).
$$

\begin{theorem}[Taylor's theorem for tensors]
\label{thm:taylor:tensor}
  Suppose $A \in C^{k+\alpha}(\mc{T})$ is a tensor for $\alpha \in (0, 1]$ then, for $t \in B_{t^*}(r)$ with $r$ sufficiently small such that $\log_{t^*}$ is well-defined
    over $B_{t^*}(r)$. Then,
  \begin{equation}
    \label{eq:taylor:theorem}
\begin{aligned}
  \lefteqn{    \left|A(t) - \sum_{j=0}^k \frac{1}{j!} \nabla^j A^{\mathrm{Sym},j}_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)) \right| } \\
  & \qquad =     \left|A(t) - \sum_{j=0}^k \frac{1}{j!} \nabla^j A_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)) \right| \\
  & \qquad \leq C(f,r) \|\log_{t^*}(t)\|^{k+\alpha}.
\end{aligned}
    \end{equation}
The constant $C(f, r)$ is similarly defined to Theorem \ref{thm:taylor}.
  \end{theorem}

%\\\\\\\\\\\\\\\

Above, the $\nabla$ operator raises the order of the tensor but we can turn this into a connection
on tensor fields, i.e. define it so that the derivative is itself order $k$.
For this, we use the abuse of notation $\nabla A(Y)$ or, more properly,
$\nabla_YA$ which is defined by
$$
(\nabla_YA)(X_1,\dots, X_1) = (\nabla A(Y))(X_k,\dots, X_1) = \nabla A(Y, X_k, \dots, X_1).
$$
Using the $\nabla_YA$ notation, we can define the second order covariant derivative of $A$ in the natural way
$$
\nabla^2 A(Z,Y)(X_k, \dots, X_1)_t = (\nabla_Z(\nabla_YA) - \nabla_{\nabla_ZY}A)(X_k, \dots, X_1)_t = \nabla^2 A(Z, Y, X_k, \dots, X_1)
$$

Given vector fields $X, W$ and a covariate 2-tensor field $A$ we'll want to compute the gradient of the functions
$$
t \mapsto A(X,W)_t \overset{def}{=} \mathfrak{a}_t
$$
We see the gradient is computed as
$$
\nabla \mathfrak{a}(Z)_t = (\nabla A(Z))(X, W) + A(\nabla X(Z), W) + A(X, \nabla W(Z)).
$$
Its Hessian is
$$
\begin{aligned}
  \nabla^2 \mathfrak{a}(Y, Z)_t &= (\nabla^2 A(Y,Z))(X, W) + (\nabla A(Z))(\nabla X(Y), W) + (\nabla A(Z))(X, \nabla W(Y)) + \\
& \qquad  (\nabla A(Y))(\nabla X(Z), W) + A(\nabla^2 X(Y, Z), W) + A(\nabla X(Z), \nabla W(Y)) + \\
& \qquad  (\nabla A(Y))(X, \nabla W(Z)) + A(\nabla X(Y), \nabla W(Z)) + A(X, \nabla^2 W(Y, Z))  \\
\end{aligned}
$$

\subsection{Higher order case when $A=g$ and $X=W$}

In the special case $A=g$ and $X=W$ we see that $\mathfrak{a}_t = \frac{1}{2}g(X,X)_t$, half the squared norm
of a vector field. For this special case, which we'll write $\mathfrak{g}$, we'll also want third and fourth order derivatives. Our work
is simplified in this case as $\nabla g=0$, so all terms involving covariant derivatives of the metric vanish:
$$
\begin{aligned}
\nabla \mathfrak{g}(V_1) &= g(\nabla X(V_1), X) \\
\nabla^2 \mathfrak{g}(V_2, V_1) &= g(\nabla^2 X(V_2, V_1), X) + g(\nabla X(V_2), \nabla X(V_1)) \\
  \nabla^3 \mathfrak{g}(V_3, V_2, V_1)& = g(\nabla^3 X(V_3, V_2, V_1), X) + g(\nabla^2  X(V_2, V_1), \nabla X(V_3)) +  \\
  & \qquad g(\nabla^2 X(V_3, V_2), \nabla X(V_1)) + g(\nabla X(V_2), \nabla^2 X(V_3, V_1)). \\
\end{aligned}
$$
In our application of this later, we will have $X=\nabla \mu$ hence, with $E=(E_1, \dots, E_d)$ an orthonormal frame,
the above simplifies to
$$
\begin{aligned}
\nabla \mathfrak{g}(V_1) &= \sum_{i=1}^d \nabla^2 \mu(V_1, E_i) \nabla \mu(E_i) \\
\nabla^2 \mathfrak{g}(V_2, V_1) &= \sum_{i=1}^d 
\biggl(\nabla^3 \mu(V_2, V_1, E_i) \nabla \mu(E_i) + \nabla^2 \mu(V_2, E_i) \nabla^2 \mu(E_i, V_1) \biggr) \\
  \nabla^3 \mathfrak{g}(V_3, V_2, V_1)& = \sum_{i=1}^d
\biggl(\nabla^4 \mu(V_3, V_2, V_1, E_i) \nabla \mu(E_i) + \nabla^3  \mu(V_2, V_1, E_i) \nabla^2 \mu(V_3, E_i) +  \\
  & \qquad \nabla^3 \mu(V_3, V_2, E_i) \nabla^2 \mu(V_1, E_i) +  \nabla^2 \mu(V_2, E_i) \nabla^3 \mu(V_3, V_1, E_i) \biggr)
\end{aligned}
$$

Finally, when evaluating at $t^*$ under the assumption $\nabla \mu_{t^*}=0$ we see
\begin{equation}
  \label{eq:grad:density}
\begin{aligned}
\nabla \mathfrak{g}(V_1)_{t^*} &= 0 \\
\nabla^2 \mathfrak{g}(V_2, V_1)_{t^*} &= \sum_{i=1}^d 
 \nabla^2 \mu(V_2, E_i)_{t^*} \nabla^2 \mu(E_i, V_1)_{t^*} \\
  \nabla^3 \mathfrak{g}(V_3, V_2, V_1)_{t^*}& = \sum_{i=1}^d
\biggl(\nabla^3  \mu(V_2, V_1, E_i)_{t^*} \nabla^2 \mu(V_3, E_i)_{t^*} +  \\
  & \qquad \nabla^3 \mu(V_3, V_2, E_i)_{t^*} \nabla^2 \mu(V_1, E_i)_{t^*} +  \nabla^2 \mu(V_2, E_i)_{t^*} \nabla^3 \mu(V_3, V_1, E_i)_{t^*} \biggr)
\end{aligned}
\end{equation}


\subsection{Handling the remainders}

\subsubsection{Remainder relating derivatives in a chart to covariant derivatives}

Away from $h=0$, the difference between the $k$-th order derivatives of $f$
and the corresponding covariant term will involve 
the vector fields $\nabla_{\partial h_i} \partial h_j, 1 \leq i,j \leq d$ for $h$ in a ball around 0 as well as the lower order covariant derivatives of $f$. Terms like this can be bounded (for suitable norms) over a small ball as follows:
\begin{equation}
  \label{eq:lower:covariant}
\left|\sum_{(i_1,\dots,i_k)} \nabla^{k-1} f(\nabla_{\partial h_{i_k}}\partial h_{i_{k-1}}, \partial h_{i_{k-2}}, \dots)_{t=\exp_{\mc{T}}(t^*, h)} \prod_{l=1}^k h_{i_l}\right|
\leq \|h\|^{k+1} \|\nabla^{k-1}f\|_h \max_{i,j} (\|h\|^{-1}\|(\nabla_{\partial h_i}\partial h_j)_h\|)
\end{equation}
and then maximized over $h$ in some ball around 0. This bound depends (weakly) on the choice of basis $E_{t^*}$ through the terms $\|(\nabla_{\partial h_i}\partial h_j)_h\|/\|h\|$ (which will be $O(1)$ due to
the vanishing Christoffel symbols at $h=0$). Maximizing
over the set of orthonormal bases, parameterized by $O(d)$, would yield a bound independent of the choice of
basis determining our normal
coordinate system. That is, for any $O \in O(d)$ we can find another orthonormal basis
$$
E^O_{t^*} = \left\{\sum_{j=1}^d O_{1j}E_{j,t^*}, \dots, \sum_{j=1}^d O_{dj}E_{j,t^*} \right\}
$$
with corresponding normal coordinates $h^O$. We can minimize over $O$ to get a ``better'' coordinate free bound.

\subsubsection{Remainders with chartwise H\"older ounds}

Assuming H\"older for the last derivative, the usual remainder in normal coordinates
will have a constant based on the choice of orthonormal basis. We could similarly minimize over $O$ here as well.

\subsection{Putting the pieces together}

So what does a Taylor expansion finally look like? We now have an answer. Let $\log_{t^*}:\mc{T} \rightarrow
T_{t^*}\mc{T}$ denote the inverse of the map
$$
X_{t^*} \to \exp_{\mc{T}}(t^*, X_{t^*})
$$
Then, we can write, for $t$ in some ball $B(t^*,r)$
$$
f(t) = f(t^*) + \nabla^1 f_t(\log_{t^*}(t)) + \frac{1}{2} \nabla^2 f_t(\log_{t^*}(t), \log_{t^*}(t))
+ \dots + \frac{1}{k!} \nabla^k f_t(\log_{t^*}(t), \dots, \log_{t^*}(t)) + R(t,t^*).
$$
The remainder $R$ has two contributions. The first comes from lower order
covariant derivatives \eqref{eq:lower:covariant} and the second comes from a chartwise H\"older constants
in balls around $t^*$. The first contribution is of order $\|\log_{t^*}(t)\|^{k+1}$ and
the second is of order $\|\log_{t^*}(t)\|^{k+\alpha}$ if the function is $C^{k+\alpha}$.
>>>>>>> b9092d2 (coordinate free form of density, later tensor derivatives -- need to swap in the correct Taylor series section)

\section{Coordinate-free versions of (10),(11),(14),(15)}

\subsection{Determinant term (11)}

The first parts of Lemma 1 should be almost identical. That is, the ignorability
of the local maxima condition should be able to be pushed through as coordinate-free,
as well as the Gaussian moment calculation, which can similarly be done coordinate-free. What remains is the determinant term:
$$
\det(\Lambda_t^{-1} H_t(y)) \overset{def}{=} \det(A_t(y))
$$
Recall that this can be evaluated in any basis without changing its value. More precisely, the eigenvalues
of the matrix above do not depend on our choice of basis.
We might as well use an orthonormal frame field $E=(E_1, \dots, E_d)$.
Above, when the variance is constant and equal to 1, the entries of the matrix\footnote{Strictly speaking, this is a matrix valued function whose $\det$ agrees with $\det(A_t(y))$ at every $(t,y)$} can be taken to be
$$
\begin{aligned}
A_{t,ij}(y) &= \E \left[-\nabla^2 Y(E_i, E_j)_t | Y_h=y, \partial Y_h=0\right] \\
&= \E \left[-\nabla^2 Y(E_i, E_j)_h | Y_h=y\right] \\
&= - \nabla^2 \mu(E_i, E_j)_t + (y - \mu_t) I \\
\end{aligned}
$$
For non-constant variance the matrix $\Lambda_h$ above is replaced by something different, but not terrible and we defer this to paper \# 2.

For a matrix valued function $M$ we can write
$$
\det(M_{x+\Delta}) = \det(M_{x}) \exp(\log \det(M_{x+\Delta}) - \log \det( M_{x}))
$$
hence, for our matrix valued function on $\mc{T}$
$$
\nabla (\det A)(E_k)_t = \det(A_{t}) \cdot \text{Tr}\left(\left(\frac{\partial \log \det}{\partial M}\biggl|_{M=A_t}\right) \nabla A(E_i)_t\right) = \det(A_t) \cdot
\text{Tr}(A_t^{-1} \nabla A(E_k)_t)
$$
with
$$
\nabla A(E_k) = -\nabla^3 \mu(E_k, E_i, E_j) - \nabla \mu(E_k) I.
$$

 To get a proper Taylor remainder we should differentiate again.
 The Hessian we're after is
 $$
\begin{aligned}
  \nabla^2 \det (A)(X, Y)_t &= \det(A_t) \cdot \biggl(\text{Tr}(A^{-1}\nabla A(X))_t \text{Tr}(A^{-1}\nabla A(Y))_t - \text{Tr}(A^{-1}\nabla A(X)A^{-1}\nabla A(Y))_t + \\
  & \qquad \qquad \text{Tr}(A^{-1}\nabla^2 A(X, Y))_t \biggr) \\
\end{aligned}
 $$
Dividing through by $\det(A_t)$, the relative error
for the first two terms can be bounded by a constant involving a bound on the operator norm of $A^{-1}_t$
$$
\sup_{t \in B(t^*,r)} \|A^{-1}_t\| \leq \inf_O  \|H^O_h(y)^{-1}\|
$$
with the $O$ denoting a choice of orthogonal matrix.
Another term in the error for the first two terms will involve (with suitable norms)
$$
\sup_{t \in B(t^*,r)} \|\nabla \mu_t\| + \|\nabla^3 \mu_t\|.
$$

In the last term
$$
\nabla^2 A(X, Y)_{ij} = -\nabla^4 \mu(X, Y, E_i, E_j) - \nabla^2 \mu(X, Y) \cdot I
$$
hence the product $A^{-1} \nabla^2 A(E_k, E_l)$ for orthonormal $E_k, E_l$ should be bounded over $B(t^*,r)$ as long as
$-\nabla^4 \mu$ does not grow faster than $-\nabla^2 \mu$. Of the terms above in the Hessian, this is really the only one that is obviously constant in the regime that $-\nabla^2\mu$ dominates.

\subsubsection{Conclusion on (11)}

We can conclude that, ignoring the lower order term from \eqref{eq:lower:covariant}
$$
\left|\frac{\det(\Lambda_t^{-1}H_t(y))}{\det(\Lambda_{t^*}^{-1}H_{t^*}(y))} - \text{Tr}(A_{t^*}(y)^{-1} \nabla^3 \mu(\cdot, \cdot, \log_{t^*}(t))) \right| \leq C \| \log_{t^*}(t)\|^2  
 $$
where
$$
\nabla^3 \mu(\cdot, \cdot, \log_{t^*}(t))_{ij} = \nabla^3 \mu_{t^*}(E_{i,t^*}, E_{j,t^*}, \log_{t^*}(t))
$$
and
 $C$ can be chosen to be coordinate-free.

\end{document}


{\bf For tilt method: look at difference of regressions Taylor series when switching base and evaluation points from $(t,t^*)$ to  $(t^*,t)$. Zeroth-order terms are
  identical, linear term should be 0. Second order is hessian at $t$ and vectors $\log_tt^*$ the other is at $t^*$ with vectors $\log_{t^*}t$.}
