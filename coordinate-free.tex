\documentclass{article}

\input{../preamble.sty}
\usepackage{lmodern}

\newcommand{\ag}[1]{{\bf{{\red{[{AG: #1}]}}}}}
\newcommand{\jt}[1]{{\bf{{\red{[{JT: #1}]}}}}}
\newcommand{\InnerProduct}[2]{\langle #1,#2 \rangle}
\newcommand{\Norm}[1]{\|#1\|}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\br}{{\bf r}}
\newcommand{\bs}{{\bf s}}
\newcommand{\bt}{{\boldsymbol t}}
\newcommand{\bH}{{\boldsymbol H}}
\newcommand{\Newton}{\mathrm{Newton}}
\newcommand{\DummyField}{{\tt f}}
\newcommand{\DummyGradient}{{\bf g}}
\newcommand{\InformationMatrix}{{\boldsymbol J}}
\newcommand{\lin}{\mathrm{lin}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\newcommand{\Exp}{\mathrm{Exp}}

\newcommand{\RandomField}{Y}
\newcommand{\bLambda}{{\boldsymbol \Lambda}}
\newcommand{\bGamma}{{\boldsymbol \Gamma}}
\newcommand{\Err}{\mathrm{Err}}
\newcommand{\bQ}{{\boldsymbol Q}}
\newcommand{\bJ}{{\boldsymbol J}}
\newcommand{\bV}{{\boldsymbol V}}
\newcommand{\bI}{{\boldsymbol I}}
\newcommand{\bC}{{\boldsymbol C}}
\newcommand{\convweak}{\overset{d}{\to}}

\newcommand{\appropto}{\mathrel{\vcenter{
			\offinterlineskip\halign{\hfil$##$\cr
				\propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

\renewcommand{\thealgorithm}{\arabic{algorithm}}

\title{ {\bf Inference for Local Maxima of a Gaussian Random Field} \\ Coordinate free intensity}

\begin{document}
	
	\maketitle
	\RaggedRight
	
	Consider a generic signal-plus-noise random field
	\begin{equation}
		\label{eqn:signal-plus-noise}
		Y_t = \mu_t + \epsilon_t, \quad {\rm for} ~ t \in \mc{T}.
	\end{equation}

        We're going to consider $\mc{T}$ an abstract manifold for the moment, meaning that any {\em local calculations}
        must be done in a chart $\varphi: U \rightarrow \R^d$ with $U \subset \mc{T}$.
        It is also a Riemannian manifold with metric: given two vector fields $X, W$:
\begin{equation}
        g(X,W)_t = \E[X_t\epsilon * W_t\epsilon].
\end{equation}

For open $A \subset \mc{T}$ and $B \subset \R$ we are interested in the random variable
\begin{equation}
  \# \left\{(t,y): t \in A, dY_t = 0, Y_t \in B, \text{$t$ is a local maximum of $Y$}  \right\}.
  \end{equation}
Note that this random variable is independent of a chart: it is coordinate-free. The extra condition
about the local maximum may seem coordinate based as it would seem to depend on the Hessian
in that coordinate. However, the property of being a local maximum of a $C^2$ function is independent of any chart.

Hence, the expected measure  is also coordinate-free.
$$
(A, B) \mapsto \nu(A \times B) \overset{def}{=} \E \left[\# \left\{(t,y): t \in A, dY_t = 0, Y_t \in B, \text{$t$ is a local maximum of $Y$}  \right\}\right] 
$$
The Kac-Rice formula gives a formula for the intensity of this measure in any coordinate system. Under enough assumptions,
$\nu$ will have a Radon-Nikodym density with respect to the product measure on $\mc{T} \times \R$ equipped with the natural product metric.
This Radon-Nikodym density will of course also be coordinate-free. Let's compute it.

The computation relies first on the proper definition of the Hessian. For any $f \in C^2(\mc{T})$ and any smooth vector fields $X, W$, the
Hessian $\nabla^2 f(X, W): \mc{T} \rightarrow \R$ can be defined through 
\begin{equation}
\begin{aligned}
\nabla^2 f(X, W)_t = 
X_t(Wf) - g(\nabla_XW, \nabla f) = X_t(Wf) -  \sum_{i,j} \Cov(X_t(W\epsilon), V_{i,t}\epsilon) g^{ij}_{V,t} V_{j,t}f
\end{aligned}
  \end{equation}
with $Wf_t = W_tf$ and $V=(V_1, \dots, V_d)$ any set of frames with $g_{ij,V}(t) = g(V_i, V_j)_t$ and $g^{ij}_V =(g_{V})^{-1}$. We could
take the elements of $V$ to be coordinate vector fields in some chart if we wanted a local formula for the Hessian.
The Hessian is symmetric (expected) and it behaves tensorially (formally it's a module over smooth functions on $\mc{T}$). Let $M_t: T_t\mc{T} \rightarrow T_t\mc{T}$ be a section of linear maps on $T(\mc{T})$, i.e.
a set change of basis matrices, one for each $t$. Then
\begin{equation}
\nabla^2 (a \cdot X + b \cdot \bar{X},W) = a \cdot \nabla^2 (X, W) + b \cdot \nabla^2 (\bar{X}, W).
  \end{equation}
In fact, this even tells us that the vector fields $X$ and $W$ don't really need to be smooth!

As in Theorem 12.4.1 of ``Random fields and geometry'', let's now fix a set of orthonormal frames $(E_1, \dots, E_d)$. The measure $\nu$
can be written as
\begin{equation}
\begin{aligned}
\nu(A \times B) &= \int_A \E \left[\det(-\nabla^2 Y(E_i,E_j)_t) 1_{{\cal N}}(\nabla^2 Y(E_i,E_j)_t) 1_B(Y_t) \biggl \vert \nabla Y^E_t=0\right] \phi_{\nabla Y^E_t}(0) \text{Vol}_g(dt) \\
 &= \int_A \int_B \underbrace{ \E \left[\det(-\nabla^2 Y(E_i,E_j)_t) 1_{{\cal N}}(\nabla^2 Y(E_i,E_j)_t) \biggl \vert \nabla Y^E_t=0, Y_t=y\right] \phi_{Y_t|\nabla Y^E_t=0}(y) \; \phi_{\nabla Y^E_t}(0)}_{\rho(t,y)} \; dy \; \text{Vol}_g(dt) \\
\end{aligned}
  \end{equation}
with $\text{Vol}_g$ the Riemannian measure. Above, $\nabla Y^E_t = (E_{1,t}Y, \dots, E_{d,t}Y)$ is the gradient of $Y$ at $t$ read off in the frame $E$, with
$\nabla^2 Y(E_i,E_j)_t$ defined similarly. This expression $\rho(t,y)$ is coordinate-free: it does not even depend on our choice of orthonormal frames.

How does this relate to what we would get if we worked in a coordinate chart $\varphi(A)$ and worked with the coordinate $\partial t_i$ vector fields? In local coordinates $t=\varphi(u)$ for some chart
$\varphi:U \to \R^d$, we will
typically write
\begin{equation}
\Lambda_{ij,t} = g\left(\frac{\partial}{\partial t_i}, \frac{\partial}{\partial t_j}\right)_t.
  \end{equation}
Let's pick a square root $\Lambda_t^{1/2}$ such that $\Lambda_t^{-1/2} \Lambda_t (\Lambda_t^{-1/2})'=I_{d \times d}$. Then, we can find a specific set of orthonormal vector fields by
$$
E_{i,t} = \sum_j \Lambda^{-1/2}_{ij,t} \frac{\partial}{\partial t_j} \biggl|_t.
$$
With this frame, we see\footnote{A small note here: generally speaking
\begin{equation}
\nabla^2 Y(\partial t_i, \partial t_j)_t \neq \frac{\partial^2 Y}{\partial t_i \partial t_j}\biggl|_t
  \end{equation}
as the RHS is missing the part related to the connection, which makes the RHS non-tensorial. This relation is true
when $t$ is a critical point of $Y$. Therefore under the conditioning
$\nabla Y^E_t=0$ the conditional expectation of LHS is the same as the RHS.}
$$
\nabla^2 Y(E_i,E_j)_t = \Lambda^{-1/2}_t \nabla^2 Y(\partial/\partial t_i, \partial/\partial t_j)_t (\Lambda^{-1/2}_t)'
$$

Further, setting
$$
\partial Y_t = \left(\frac{\partial Y}{\partial t_1}\biggl|_t, \dots, \frac{\partial Y}{\partial t_d}\biggl|_t \right)
$$
we see that
$$
\phi_{\nabla Y_t^E}(0) = \det(\Lambda_t^{1/2}) \phi_{\partial Y_t}(0).
$$

Putting these these facts together, we see that the following expression is coordinate-free (and even valid with non-constant variance) and computable in any coordinate system we like:
$$
\begin{aligned}
\rho(t,y) = \det(\Lambda_t)^{-1/2} \E \left[\det\left(-\frac{\partial^2 Y}{\partial t_i \partial t_j} \biggl|_t\right) 1_{{\cal N}}\left(-\frac{\partial^2 Y}{\partial t_i \partial t_j} \biggl|_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] \phi_{Y_t|\partial Y_t=0}(y) \; \phi_{\partial Y_t}(0).
\end{aligned}
$$

Alternatively, we could write
$$
\begin{aligned}
\rho(t,y) &= (2\pi)^{-d/2} \det(\Lambda_t)^{-1} \E \left[\det\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t\right) 1_{{\cal N}}\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] \phi_{Y_t|\partial Y_t=0}(y) \; e^{-\frac{1}{2}g(\nabla \mu_t,\nabla \mu_t)} \\
 &= (2\pi)^{-d/2} \E \left[\det\left({\cal H}_t\Lambda_t^{-1}\right) 1_{{\cal N}}\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] \phi_{Y_t|\partial Y_t=0}(y) \; e^{-\frac{1}{2}g(\nabla \mu_t,\nabla \mu_t)} \\
\end{aligned}
$$
with ${\cal H}_{t,ij}$ the negative of the matrix of \nabla^2 Y_t$  in the basis $\partial t_i$ at $t$..

In the constant variance case (with variance 1), this simplifies to
$$
\begin{aligned}
\rho(t,y) = (2\pi)^{-(d+1)/2}  \E \left[\det\left({\cal H}_t\Lambda_t^{-1}\right) 1_{{\cal N}}\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] e^{-\frac{1}{2}(y-\mu_t)^2} \; e^{-\frac{1}{2}g(\nabla \mu_t,\nabla \mu_t)}
\end{aligned}
$$

\section{Taylor series on a Riemannian manifold}

%\\\\\\\ protect merge

We will want to expand $\rho(y,t)$ in some form of Taylor
expansion. On a manifold, using a Taylor expansion in a given
coordinate system can obscure things in different coordinate
systems. On a Riemannian manifold, it is simplest to derive a Taylor expansion
at $t$ in some small ball $B(t^*,r)$ by considering the geodesic connecting them.

Define $\log_{t^*}(t)$ to be the inverse of the exponential map's restriction to $T_{t^*}\mc{T}$:
$$
\exp_{\mc{T}}(t^*, \log_{t^*}(t)) = t
$$
and let $\gamma = \gamma_t:[0, d(t^*,t)]$ be the geodesic connecting $t^*$ to $t$. The map $\log_{t^*}(t)$ will be well defined
on small balls around $t^*$.

For a smooth function $f:\mc{T} \rightarrow \R$ consider the map $\bar{f} = f \circ \gamma: [0, d(t^*,t)] \rightarrow \R$.
The value $\bar{f}$ can be expressed using the usual one-dimensional Taylor's theorem. Up to order $k$ with a Taylor remainder it would be
$$
 \left|\bar{f}(r) - \sum_{j=0}^k \frac{r^j}{j!} \frac{d^j\bar{f}}{dr^j}(0) \right| \leq  \frac{r^{k+1}}{(k+1)!}\frac{d^{k+1}f}{dr^{k+1}} (\bar{r}(r))
$$
where $\bar{r} \in [0, r]$. For a H\"older type remainder for $f \in C^{k+\alpha}$ we would have
$$
 \left|\bar{f}(r) - \sum_{j=0}^k \frac{r^j}{j!} \frac{d^j\bar{f}}{dr^j}(0) \right| \leq  C_{\alpha}(\gamma) r^{\alpha}.
$$

 The key to extend this argument based on a geodesic to a more familiar form of Taylor series (and a natural one for Riemannian manifolds)
 is to note that along a geodesic, the derivatives implicated above will be expressible in terms of the $k$-th order covariant
 derivatives of $f$.

\subsection{Covariant differentiation}

The covariant derivatives of a function $f$ are tensor fields on $\mc{T}$, which means they behave nicely
as functions of $t$.\footnote{Specifically multilinearly as a module over smooth functions on $\mc{T}$. This,
in turn means they are easy to relate between different coordinate systems at any $t$.}
Let $X_i$ denote vector fields, however many as needed, with $\nabla_XY$ denoting
the covariant derivative of the vector field $Y$ with respect to $X$. The covariant
derivatives of a function are defined recursively:
$$
\begin{aligned}
  \nabla^1 f(X_1)_t &= g(\nabla f, X_1)_t \\
  &= X_{1,t}f \\
  \nabla^2 f(X_2, X_1)_t &= X_{2,t}(\nabla^1 f(X_1)) - \nabla^1 f(\nabla_{X_2}X_1)_t \\
  \nabla^k f(X_k, X_{k-1}, \dots, X_1)_t &= X_{k,t}(\nabla^{k-1} f(X_{k-1}, \dots, X_1)) -
  \nabla^{k-1} f(\nabla_{X_k}X_{k-1}, X_{k-2}, \dots, X_1)_t - \\
&  \qquad   \nabla^{k-1} f(X_{k-1}, \nabla_{X_k}X_{k-2}, \dots, X_1)_t - \\
  & \qquad \nabla^{k-1} f(X_{k-1}, \dots, \nabla_{X_k}X_1)_t.
  \end{aligned}
$$

In terms of symmetry, we note that beyond second order, covariant derivatives of a function are not generally symmetric. We can however project these derivatives
(or any tensor) 
onto the symmetric tensors. We denote this projection with a superscript
$$
A^{\mathrm{Sym}}(X_1, \dots, X_k) = \frac{1}{k!} \sum_{\sigma \in S_k} A(X_{\sigma(1)}, \dots, X_{\sigma(k)}).
$$

\begin{lemma}
\label{lem:geodesic}
  For $r \in [0, d(t^*,t)]$ the following holds:
  $$
\frac{d^j\bar{f}}{dr^j}(r) = \nabla^j f_{\gamma(r)}(\dot{\gamma}(r), \dots, \dot{\gamma}(r)) =  \nabla^j f_{\gamma(r)}^{\mathrm{Sym}}(\dot{\gamma}(r), \dots, \dot{\gamma}(r))
  $$
  \end{lemma}
The proof of this is by induction starting at $j=2$ combined with the fact that $(\nabla_{\dot{\gamma}}\dot{\gamma})_{\gamma(r)}=0$ for $r \in [0, d(t^*,t)\|]$.

We can now state a coordinate-free version of Taylor's theorem.
\begin{theorem}[Taylor's theorem]
\label{thm:taylor}
  Suppose $f \in C^{k+\alpha}(\mc{T})$ for $\alpha \in (0, 1]$ then, for $t \in B_{t^*}(r)$ with $r$ sufficiently small such that $\log_{t^*}$ is well-defined
    over $B_{t^*}(r)$. Then,
  \begin{equation}
    \label{eq:taylor:theorem}
\begin{aligned}
  \lefteqn{    \left|f(t) - \sum_{j=0}^k \frac{1}{j!} \nabla^j f^{\mathrm{Sym}}_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)) \right| } \\
  & \qquad =     \left|f(t) - \sum_{j=0}^k \frac{1}{j!} \nabla^j f_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)) \right| \\
  & \qquad \leq C(f,r) d(t^*,t)^{k+\alpha}.
\end{aligned}
    \end{equation}
The constant $C(f, r)$ can be taken to be (up to a universal multiplicative constant $\bar{C}$)
\begin{equation}
  C(f,r) = \bar{C} \cdot  \begin{cases} \frac{1}{(k+1)!}\sup_{t \in B(t^*,r)} \|\nabla^{k+1} f^{\mathrm{Sym}}_t\| & \alpha=1 \\
    \inf_{E_{t^*}}  C_{\alpha,E_{t^*}}(\bar{f}^{E_{t^*}}, r) & \alpha \in (0, 1)
\end{cases}
\end{equation}
  with $E_{t^*}$ denoting a choice of orthonormal basis for $T_{t^*}\mc{T}$ with corresponding
  normal coordinates $\varphi^{E_{t^*}}:\mc{T} \rightarrow \R^d$ at $t^*$. The quantity $C_{\alpha,E_{t^*}}(\bar{f}^{E_{t^*}}, r)$ is a coordinate-based H\"older constant.
  That is, if we define $\bar{f}^{E_{t^*}} = f\circ (\varphi^{E_{t^*}})^{-1}: \R^d \rightarrow \R$, then for each choice $E_{t^*}$ we can pick a H\"older constant:
  $$
\left| \bar{f}^{E_{t^*}}(h) - \sum_{j=0}^k \frac{1}{j!} \nabla^j \bar{f}^{E_{t^*}}_0(h, \dots, h) \right| \leq \|h\|^{k+\alpha} C_{\alpha, E_{t^*}}(\bar{f}^{E_{t^*}}, r) \ \forall \; h \in \varphi^{E_{t^*}}(B(t^*,r)).
  $$
  \end{theorem}

\subsection{Covariant differntiation of tensor fields}

Covariant derivatives of a tensor field are similary defined. Suppose $A$ is a $k$-th order (covariant) tensor field
$$
A_t:\otimes^k T_t\mc{T} \to \R
$$
then we can define a $(k+1)$ order covariant tensor field by
$$
\begin{aligned}
  \nabla A(Y, X_k, \dots, X_1)_t &= Y_{t}(A(X_k, \dots, X_1)) - A(\nabla_{Y}X_k, \dots, X_1)_t  \\
  &  \qquad  A(X_{k}, \nabla_{Y}X_{k-1}, \dots, X_1)_t - \\
  & \qquad A(X_{k}, \dots, \nabla_{Y}X_1)_t.
\end{aligned}
$$

Viewing $\nabla^{k} f$ as a tensor field, we see that $\nabla ( \nabla^k f)(X)$, to be read as the covariant derivative of the tensor $\nabla^k f$ in the direction
$X$, would be a tensor field of order $k$ satisfying
$$
\nabla(\nabla^k f)(Y)(X_1, \dots, X_k) = (\nabla^{k+1} f)(Y, X_1, \dots, X_k).
$$
In other words $\nabla (\nabla^k f) = \nabla^{k+1} f$.

\subsection{Assumptions on $\mu$}

Having defined the form of Taylor's theorem on Riemannian manifolds, 
we now make the following assumptions on $\mu \in C^{4+\alpha}$, with asymptotic parameter $\delta_{t^*}$:
$$
\inf_{t \in B_{t^*}(-\log(\delta(t^*)) \delta(t^*))} \inf_{X_t \in T_t\mc{T}: g_t(X_t,X_t)=1} (-\nabla^2 \mu_{t^*})(X_t, X_t) \geq C (\delta_{t^*})^{-1}
$$
for some fixed $C$. That is, we'll be applying our Taylor theorem $r=\delta_{t^*}(-\log(\delta_{t^*}))$.

  We also make a well-conditioned assumption 
  $$
\sup_{t \in B_{t^*}(-\log(\delta(t^*)))} \|\nabla^2 \mu_t\| + \|\nabla^3 \mu_t\| + \|\nabla^4 \mu_t\| \leq \kappa C \delta_{t^*}^{-1}
  $$
for some fixed $\kappa$. As for the H\"older coefficient, we assume
$$\inf_{E_{t^*}} C_{\alpha, E_{t^*}}(\bar{f}^{E_{t^*}}, r) \leq \kappa \cdot C \cdot \delta_{t^*}^{-1}.$$


\subsection{Taylor's theorem for evaluations of tensors}

Rather than providing a version of Taylor's theorem for tensor fields, we focus on Taylor's theorem for functions of
the form
$$
t \mapsto A(X,W)_t \overset{def}{=} \mathfrak{a}_t
$$
for 2-tensors $A$ and fixed vector fields $X, W$. The canonical example here being the coordinates
of a tensor read out in an orthonormal frame, or perhaps in normal coordinates.

We see the gradient is computed as
$$
\nabla \mathfrak{a}(Z)_t = (\nabla A(Z))(X, W) + A(\nabla X(Z), W) + A(X, \nabla W(Z)).
$$
Its Hessian is
$$
\begin{aligned}
  \nabla^2 \mathfrak{a}(Y, Z)_t &= (\nabla^2 A(Y,Z))(X, W) + (\nabla A(Z))(\nabla X(Y), W) + (\nabla A(Z))(X, \nabla W(Y)) + \\
& \qquad  (\nabla A(Y))(\nabla X(Z), W) + A(\nabla^2 X(Y, Z), W) + A(\nabla X(Z), \nabla W(Y)) + \\
& \qquad  (\nabla A(Y))(X, \nabla W(Z)) + A(\nabla X(Y), \nabla W(Z)) + A(X, \nabla^2 W(Y, Z)).  \\
\end{aligned}
$$

We will have some flexibility in choosing $X, Y, Z, W$ below. Specifically, it will be convenient
to choose them to be normal coordinate vector fields.

\begin{lemma}
\label{lem:normal:taylor:tensor}
  Let $\partial v_i, 1 \leq i \leq d$ denote normal coordinate vector fields for
  a fixed orthonormal basis $E_{t^*}=\{E_{1,t^*},\dots, E_{d,t^*}\}$ of $T_{t^*}\mc{T}$ and let
  $$\mathfrak{a}=\nabla^2 \mu(\partial v_i, \partial v_j) + (y - \mu_t) \cdot g(\partial v_i, \partial v_j)_t$$
  for some $\mu \in C^{4+\alpha}(\mc{T},\R)$ with $\nabla \mu_{t^*}=0$. Then,
  \begin{equation}
\partial v_k(\nabla \mathfrak{a})_{t^*} = \nabla^3 \mu(\partial v_k, \partial v_i, \partial v_j)_{t^*}
    \end{equation}
  Further, up to a universal constant $\bar{C}$
  \begin{equation}
    \left|\mathfrak{a}_t - \mathfrak{a}_{t^*} - \nabla^3 \mu(\log_{t^*}(t), \partial v_i, \partial v_j)_{t^*}
    \right| \leq \kappa \cdot \bar{C} \cdot d(t^*,t)^2 \delta_{t^*}^{-1}
    \end{equation}
  The quantity $\|\nabla^2 \mathfrak{a}^{E_{t^*}}_t\|$ depends on the choice of basis, and can be bounded in terms of
  derivatives the Christoffel symbols in the basis $E_{t^*}$ and various norms of $A_t$, $\nabla A_t$ and $\nabla^2 A_t$. Maximizing over $t \in B_{t^*}(r)$ followed by minimizing over $E_{t^*}$ yields a coordinate-free bound.
  \end{lemma}

We'll also be interested in the special case $A=g$ and $X=W=\nabla \mu$.
In this case, we see that $\mathfrak{a}_t = \frac{1}{2}g(X,X)_t$, half the squared norm
of a vector field. For this special case, which we'll write $\mathfrak{g}$, we'll also want third and fourth order derivatives. Calculations simplify greatly here
as $\nabla g=0$, so all terms involving covariant derivatives of the metric vanish:
$$
\begin{aligned}
\nabla \mathfrak{g}(V_1) &= g(\nabla X(V_1), X) \\
\nabla^2 \mathfrak{g}(V_2, V_1) &= g(\nabla^2 X(V_2, V_1), X) + g(\nabla X(V_2), \nabla X(V_1)) \\
  \nabla^3 \mathfrak{g}(V_3, V_2, V_1)& = g(\nabla^3 X(V_3, V_2, V_1), X) + g(\nabla^2  X(V_2, V_1), \nabla X(V_3)) +  \\
  & \qquad g(\nabla^2 X(V_3, V_2), \nabla X(V_1)) + g(\nabla X(V_2), \nabla^2 X(V_3, V_1)). \\
\end{aligned}
$$

As noted above, in our application of this later, we will have $X=\nabla \mu$. With $E=(E_1, \dots, E_d)$ an orthonormal frame, the above simplifies to
$$
\begin{aligned}
\nabla \mathfrak{g}(V_1) &= \sum_{i=1}^d \nabla^2 \mu(V_1, E_i) \nabla \mu(E_i) \\
\nabla^2 \mathfrak{g}(V_2, V_1) &= \sum_{i=1}^d 
\biggl(\nabla^3 \mu(V_2, V_1, E_i) \nabla \mu(E_i) + \nabla^2 \mu(V_2, E_i) \nabla^2 \mu(E_i, V_1) \biggr) \\
  \nabla^3 \mathfrak{g}(V_3, V_2, V_1)& = \sum_{i=1}^d
\biggl(\nabla^4 \mu(V_3, V_2, V_1, E_i) \nabla \mu(E_i) + \nabla^3  \mu(V_2, V_1, E_i) \nabla^2 \mu(V_3, E_i) +  \\
  & \qquad \nabla^3 \mu(V_3, V_2, E_i) \nabla^2 \mu(V_1, E_i) +  \nabla^2 \mu(V_2, E_i) \nabla^3 \mu(V_3, V_1, E_i) \biggr)
\end{aligned}
$$

Finally, when evaluating at $t^*$ under the assumption $\nabla \mu_{t^*}=0$ we see
\begin{equation}
  \label{eq:grad:density}
\begin{aligned}
\nabla \mathfrak{g}(V_1)_{t^*} &= 0 \\
\nabla^2 \mathfrak{g}(V_2, V_1)_{t^*} &= \sum_{i=1}^d 
 \nabla^2 \mu(V_2, E_i)_{t^*} \nabla^2 \mu(E_i, V_1)_{t^*} \\
  \nabla^3 \mathfrak{g}(V_3, V_2, V_1)_{t^*}& = \sum_{i=1}^d
\biggl(\nabla^3  \mu(V_2, V_1, E_i)_{t^*} \nabla^2 \mu(V_3, E_i)_{t^*} +  \\
  & \qquad \nabla^3 \mu(V_3, V_2, E_i)_{t^*} \nabla^2 \mu(V_1, E_i)_{t^*} +  \nabla^2 \mu(V_2, E_i)_{t^*} \nabla^3 \mu(V_3, V_1, E_i)_{t^*} \biggr).
\end{aligned}
\end{equation}

Summarizing, we can state the following lemma.
\begin{lemma}
\label{lem:grad:density}
  Suppose $\mu \in C^{4+\alpha}(\mc{T};\R)$ and $\nabla \mu_{t^*}=0$. Then, with H\"older remainder term,
  \begin{equation}
\begin{aligned}
\lefteqn{    \biggl| \frac{1}{2} g(\nabla \mu, \nabla \mu)_t -  } \\
& \qquad  \frac{1}{2}\sum_{i=1}^d \biggl[ (\nabla^2 \mu_{t^*}(\log_{t^*}(t), E_{i,t^*}))^2 + \\
    & \qquad \qquad 
      \nabla^3 \mu_{t^*}(\log_{t^*}(t), \log_{t^*}(t), E_{i,t^*}) \nabla^2 \mu_{t^*}(E_{i,t^*}, \log_{t^*}(t)) \biggr] \biggr| \\
& \qquad \leq \kappa \cdot \bar{C} d(t^*,t)^{3+\alpha} \delta_{t^*}(-\log(\delta_{t^*})) +
\kappa^2 \cdot \bar{C}' d(t^*,t)^4 \delta_{t^*}^{-2}
\end{aligned}
    \end{equation}
The Taylor expansion does not depend on the choice of orthonormal basis.
  \end{lemma}
The two terms in the bound appear  based on analyzing terms of $\nabla^3 \mathfrak{g}$ with $\nabla^4 \mu(\cdot) \nabla \mu(\cdot)$ separately from
terms with $\nabla^3 \mu(\cdot) \nabla^2 \mu(\cdot)$. As we have only assumed $\mu \in C^{4+\alpha}$ we will have to consider these terms separately. Fortunately $\nabla \mu$ can only grow to size $\delta_{t^*}(-\log (\delta_{t^*}))$ on our ball.



\section{Coordinate-free versions of (10),(11),(14),(15)}

\subsection{Determinant term (11)}

The first parts of Lemma 1 should be almost identical. That is, the ignorability
of the local maxima condition should be able to be pushed through as coordinate-free,
as well as the Gaussian moment calculation, which can similarly be done coordinate-free. What remains is the determinant term:
$$
\det(\Lambda_t^{-1} H_t(y)) \overset{def}{=} \det(A_t(y))
$$
Recall that this can be evaluated in any basis without changing its value. More precisely, the eigenvalues
of the matrix above do not depend on our choice of basis.
We might as well use normal coordinates $(\partial v_i)_{1 \leq i \leq d}$ at $t^*$ with basis $E_{t^*}=\{E_{1,t^*}, \dots, E_{d,t^*}\}$.
Above, when the variance is constant and equal to 1, the entries of the matrix\footnote{Strictly speaking, this is a matrix valued function whose $\det$ agrees with $\det(A_t(y))$ at every $(t,y)$} can be taken to be
$$
\begin{aligned}
A_{t,ij}(y) &= \E \left[-\nabla^2 Y(E_i, E_j)_t | Y_h=y, \partial Y_h=0\right] \\
&= \E \left[-\nabla^2 Y(E_i, E_j)_h | Y_h=y\right] \\
&= - \nabla^2 \mu(E_i, E_j)_t + (y - \mu_t) I \\
\end{aligned}
$$
For non-constant variance the matrix $\Lambda_h$ above is replaced by something different, but not terrible and we defer this to paper \# 2.

For a matrix valued function $M$ in local coordinates $x$ we can write
$$
\det(M_{x+\Delta}) = \det(M_{x}) \exp(\log \det(M_{x+\Delta}) - \log \det( M_{x}))
$$
hence, $M_x = \Lambda_x^{-1}H_x$
$$
\det(M_{x+\Delta}) = \det(M_{x}) \exp(\log \det(H_{x+\Delta}) - \log \det( H_{x}) - \log \det(\Lambda_{x+\Delta}) + \log \det(\Lambda_x)).
$$
Differentiating in our normal coordinate system, and applying Lemma \ref{lem:normal:taylor:tensor} yields
$$
\begin{aligned}
  \nabla (\det A)(\partial v_k)_{t^*} &= \det(A_{t}) \cdot \text{Tr}\left(\left(\frac{\partial \log \det}{\partial M}\biggl|_{M=H_t}\right) \nabla H(\partial v_k)_t - \left(\frac{\partial \log \det}{\partial M}\biggl|_{M=\Lambda_t}\right) \nabla \Lambda(\partial v_k)_t\right) \\
  &= \det(H_{t^*}) \cdot \left(\sum_{i,j} \nabla^3 \mu(\partial v_k, \partial v_i, \partial v_j)_{t^*} (H_{t^*}^{-1})_{ji} \right)
\end{aligned}
$$

 To get a proper Taylor remainder we should differentiate again.
 The Hessian we're after is
 $$
\begin{aligned}
  \nabla^2 \det (A)(X, Y)_t &= \det(A_t) \cdot \biggl(\text{Tr}(A^{-1}\nabla A(X))_t \text{Tr}(A^{-1}\nabla A(Y))_t - \text{Tr}(A^{-1}\nabla A(X)A^{-1}\nabla A(Y))_t + \\
  & \qquad \qquad \text{Tr}(A^{-1}\nabla^2 A(X, Y))_t \biggr) \\
\end{aligned}
 $$
Our well-conditioned assumption will imply that all terms in the parentheses are bounded even as $\delta_{t^*} \downarrow 0$.
We can conclude that
$$
\left|\det( A_t) - \det (A_{t^*}) \left(1 + \sum_{i,j} \nabla^3 \mu_{t^*}(\log_{t^*}(t), E_{i,t^*}, E_{j,t^*}) (H_{t^*}^{-1})_{ji}\right) \right| \leq \bar{C}(\kappa) \det(H_{t^*}) d(t^*,t)^2.
$$
That is, the relative error's size does not grow as $\delta_{t^*} \downarrow 0$.

\subsection{Density term: gradient}

Lemma \ref{lem:grad:density} can be used to get a relative error bound on
$$
e^{-\frac{1}{2}g(\nabla \mu, \nabla \mu)_t}.
$$
That is, for $r$ sufficiently small, and assuming that neither $\mu^3$ nor $\mu^4$ dominate $\mu^2$ we can almost directly conclude 
\begin{equation}
\begin{aligned}
\lefteqn{  \biggl|\exp\left(-\frac{1}{2}g(\nabla \mu, \nabla \mu)_t\right) - \exp\left(-\frac{1}{2} \sum_{i=1}^d \nabla^2 \mu_{t^*}(\log_{t^*}(t), E_{i,t^*})^2\right) \times} \\
& \qquad   \left(1 + \frac{1}{2} \sum_{i=1}^d \nabla^3 \mu_{t^*}(\log_{t^*}(t), \log_{t^*}(t), E_{i,t^*}) \nabla^2 \mu_{t^*}(E_{i,t^*}, \log_{t^*}(t)) \right) \biggr|  \\
&  \leq \qquad (1 + \kappa \cdot \bar{C} d(t^*,t)^{3+\alpha} \delta_{t^*}^{-1} + \kappa^2 \cdot \bar{C}' d(t^*,t)^4 \delta_{t^*}^{-2}) \cdot \exp\left(-\frac{1}{2} \sum_{i=1}^d \nabla^2 \mu_{t^*}(\log_{t^*}(t), E_{i,t^*})^2\right) .
\end{aligned}
\end{equation}
The actual relative error can be bounded by the square of the  remainder from the 2nd order Taylor expansion
times the exponential of the absolute value of the remainder, i.e. it is bounded by
$$
e^{|R(t,t^*)|} R(t,t^*)^2 < 3 R(t,t^*)^2, \qquad |R(t,t^*)|<1.
$$

\subsection{Value of the field term}

The final term to apply our version Taylor's theorem to is (with variance constant and equal to 1)
$$
\exp\left(-\frac{1}{2}(y-\mu_t)^2\right) \overset{def}{=} e^{a_t}.
$$
It is by now straightforward to verify
$$
\begin{aligned}
\nabla a(Z) &= (y-\mu) \nabla \mu(Z)\\
\nabla a^2(Y, Z) &= -\nabla \mu(Y) \nabla \mu(Z) + (y-\mu) \nabla^2 \mu(Y, Z)\\
\nabla a^3(X, Y, Z) &= -\nabla^2 \mu(X, Y) \nabla \mu(Z)\\
& \qquad -\nabla \mu(Y) \nabla^2 \mu(X, Z) \\
& \qquad -\nabla \mu(Z) \nabla^2 \mu(X, Y) \\
& \qquad + (y - \mu) \nabla^3 \mu(X, Y, Z). \\
\end{aligned}
$$
Evaluating at $t^*$ yields
$$
\begin{aligned}
\nabla a(Z)_{t^*} &= 0 \\
\nabla a^2(Y, Z)_{t^*} &=  (y-\mu_{t^*}) \nabla^2 \mu(Y, Z)_{t^*}\\
\nabla a^3(X, Y, Z)_{t^*} &=   (y - \mu_{t^*}) \nabla^3 \mu(X, Y, Z)_{t^*}. \\
\end{aligned}
$$

We could carry on to the 4th derivative but the only interesting thing about it is its dependence on $\mu$ (and hence $\delta_{t^*}$. We see that $\|\nabla^4 a\| \leq \kappa^2 \cdot \bar{C} \delta_{t^*}^{-2}$.
Similar to the gradient term, we see
$$
\begin{aligned}
  \lefteqn{\biggl|\exp(a(t)) - \exp\left(\frac{1}{2}(y-\mu_{t^*}) \nabla^2 \mu_{t^*}(\log_{t^*}(t), \log_{t^*}(t))\right) \times} \\
  & \qquad \left(1 + (y - \mu_{t^*})( \nabla^3 \mu(\log_{t^*}(t), \log_{t^*}(t) ,\log_{t^*}(t))) \right)\biggr|
  \leq \exp\left(\frac{1}{2}(y-\mu_{t^*}) \nabla^2 \mu_{t^*}(\log_{t^*}(t), \log_{t^*}(t))\right) \kappa^2 \bar{C} d(t,t^*)^4 \delta_{t^*}^{-2}.
\end{aligned}
$$
Moving the first order term in to the relative error yields a different bound
$$
\begin{aligned}
  \lefteqn{\biggl|\exp(a(t)) - \exp\left(\frac{1}{2}(y-\mu_{t^*}) \nabla^2 \mu_{t^*}(\log_{t^*}(t), \log_{t^*}(t))\right)\biggr|} \\
  & \qquad 
  \leq \exp\left(\frac{1}{2}(y-\mu_{t^*}) \nabla^2 \mu_{t^*}(\log_{t^*}(t), \log_{t^*}(t))\right) \left(\kappa^2 \cdot \bar{C} \cdot d(t,t^*)^4 \delta_{t^*}^{-2} +  \kappa \cdot \bar{C}' \cdot d(t,t^*)^3 |y - \mu_{t^*}| \delta_{t^*}^{-1} \right)
\end{aligned}
$$

\section{Finally, pick some coordinates}

After having carefully found bounds that do not depend on a coordinate system, it's now time to pick one. Choose
normal coordinates $v=v^{t^*,E_{t^*}}$ based on an orthonormal basis $E_{t^*}$ and rescale by $\delta_t^{-1}$ denoting this by $z$. In these coordinates, the terms $\log_{t^*}(t)$ are replaced by
$$
\log_{t^^*}(t) = \sum_{i=1}^d z_i(t) \partial z_i(z(t)).
$$
The derivatives Taylor expansions are now simply linear in $z$ scaled by $\delta_{t^*}^{-1}$
(for the determinant term) and $\delta_{t^*}^{-3}$ for the density of the gradient term. This choice of coordinates is not natural for 
pivots, but it is indeed a clean way to express the intensity function as a function on $\mc{T}$. The resulting
linear and cubic terms are such that they are $O(\delta_{t^*})$ in size, as expected. 

 

\section{Appendix: Taylor's theorem for tensor fields}

For completeness, we write  out a corresponding version of Taylor's theorem for tensor fields. For functions, we simply applied
the usual Taylor theorem on a curve. Repeated differentiation of a function is the same
as repeated application of the connection on functions. That is,
$$
\frac{d^k \bar{f}}{dr^k}\biggl|_r = \bar{\nabla}_{\dot{\gamma}} \left(\bar{\nabla}_{\dot{\gamma}}\left(\dots \left(\bar{\nabla}_{\dot{\gamma}}f\right) \right)\right)
$$
where $(\bar{\nabla}_{X}f)$ is  the function $t \mapsto \nabla f(X)_t$. We will write repeated applications of the connection\footnote{We will not use this notion
of repeated application of the connection elsewhere. It is used here to differentiate it from covariant differentiation.}as
$$
\bar{\nabla}^k_{(X_k, \dots, X_1)} f = \bar{\nabla}_{X_k}\left(\bar{\nabla}_{X_{k-q}} \left( \dots \left(\bar{\nabla}_{X_1}f\right)\right)\right).
$$
Importantly, the above expression is not tensorial in the $X_i$'s.

This repeated application of the connection is the most
natural way to differentiate a tensor of the form
$$
\sum_{s \in S} A_s \otimes_{j \in s} U_j
$$
for some subsets $s$ of fixed size, say $k$ for some set of vector fields ${\cal U} = \{U_1, \dots\}$. Indeed, this is how one would proceed in the Euclidean case. In the Euclidean case, of course one can express the tensor in some fixed basis, which is
not possible in general for a Riemmanian manifold.

Let's consider the simplest case, a 2-tensor of the form
$$
A = \sum_{i,j} a_{ij} U_i \otimes U_j.
$$
Now, consider directions we might want to differentiate this tensor. Even in the Euclidean case, the set of vector fields ${\cal U}$ may not be the standard
coordinate vector fields.

In the Euclidean case, we would likely look to differentiate $a_{ij}$ as well as $U_i$ and $U_j$ in the standard coordinate vector fields.
This differentiation corresponds exactly to repeated applications of the (Euclidean) connection to the terms $a_{ij}, U_i, U_j$.
After having
differentiated sufficiently we would expect to have some form of Taylor's theorem to evaluate the remainder.

The following is a straightforward generalization of Lemma \cite{lem:geodesic}.
\begin{lemma}
$$
(\bar{\nabla}^k_{(\dot{\gamma}(r), \dots, \dot{\gamma}(r))}A)_{\gamma(r)}= \nabla^k A_{\gamma(r)}\left(\dot{\gamma}(r), \dots, \dot{\gamma}(r)\right).
$$
Or, repeated applications of the connection to a tensor along the geodesic computes covariant derivatives of the tensor. 
\end{lemma}
The proof is again inductive, starting with the identity $\nabla^2 A(X, Y) = (\nabla_X \nabla_Y - \nabla_{\nabla_XY})A$ and applying the geodesic condition.

We still have a problem, however. Consider, for $r \in [0, d(t^*,t)]$ the quantity $A_{\gamma(r)}$ and the natural
candidate for a Taylor expansion based at $t^*=\gamma(0)$:
$$
\sum_{j=0}^k \frac{1}{j!}\nabla^j A_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)).
$$
The tensor $A_{\gamma(r)}$ is multilinear on tensor products of $T_{\gamma(r)}\mc{T}$ while $A_{t^*}$ and its
covariant derivatives are multilinear on tensor products of $T_{t^*}\mc{T}$. This can be resolved
through parallel transport along $\gamma$ from $T_{\gamma(r)}\mc{T}$ to $T_{\gamma(0)}\mc{T} = T_{t^*}\mc{T}$.

\jt{A site that was somewhat helpful: https://www.math.uni-hamburg.de/home/lindemann/diffgeo\string_SS2020\string_lindemann.html,
particularly last 5 or 6 lectures.}

In order to state our version of Taylor's thereom, it is useful to define a partial symmetrization of a $k$-tensor. For $q \leq k$, define the first $q$ symmetrization
$$
A^{\mathrm{Sym,q}}(X_k,\dots, X_1) = \sum_{\sigma \in S(q)} A(X_{k+1-\sigma(1)}, \dots, A_{k+1-\sigma(q)}, X_{k-q}, \dots, X_1).
$$

\jt{I think this last bit checks out though we don't really need it as we're going to really
  only Taylor expand functions.}

\begin{theorem}[Taylor's theorem for tensors]
\label{thm:taylor:tensor}
  Suppose $A \in C^{k+\alpha}(\mc{T})$ is an $m$-tensor for $\alpha \in (0, 1]$ then, for $t \in B_{t^*}(r)$ with $r$ sufficiently small such that $\log_{t^*}$ is well-defined
    over $B_{t^*}(r)$. Let $P_t^{t^*}:T_{t}\mc{T} \to T_{t^*} \mc{T}$ denote
    parallel transport along $\gamma$. Then,
  \begin{equation}
    \label{eq:taylor:theorem}
\begin{aligned}
  \lefteqn{    \left|A(t)(V_{1,t}, \dots, V_{m,t}) - \sum_{j=0}^k \frac{1}{j!} \nabla^j A^{\mathrm{Sym},j}_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t), P_t^{t^*}(V_{1,t}), \dots, P_t^{t^*}(V_{m,t})) \right| } \\
  & \qquad =     \left|A(t)(V_{1,t}, \dots, V_{m,t}) - \sum_{j=0}^k \frac{1}{j!} \nabla^j A_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t), P_t^{t^*}(V_{1,t}), \dots, P_t^{t^*}(V_{m,t})) \right| \\
  & \qquad \leq C(f,r) d(t^*,t)^{k+\alpha}.
\end{aligned}
    \end{equation}
The constant $C(f, r)$ is similarly defined to Theorem \ref{thm:taylor}.
  \end{theorem}


\end{document}


{\bf For tilt method: look at difference of regressions Taylor series when switching base and evaluation points from $(t,t^*)$ to  $(t^*,t)$. Zeroth-order terms are
  identical, linear term should be 0. Second order is hessian at $t$ and vectors $\log_tt^*$ the other is at $t^*$ with vectors $\log_{t^*}t$.}
