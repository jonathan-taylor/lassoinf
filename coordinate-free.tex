\documentclass{article}

\input{../preamble.sty}
\usepackage{lmodern}

\newcommand{\ag}[1]{{\bf{{\red{[{AG: #1}]}}}}}
\newcommand{\InnerProduct}[2]{\langle #1,#2 \rangle}
\newcommand{\Norm}[1]{\|#1\|}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\M}{\mathbb{M}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\br}{{\bf r}}
\newcommand{\bs}{{\bf s}}
\newcommand{\bt}{{\boldsymbol t}}
\newcommand{\bH}{{\boldsymbol H}}
\newcommand{\Newton}{\mathrm{Newton}}
\newcommand{\DummyField}{{\tt f}}
\newcommand{\DummyGradient}{{\bf g}}
\newcommand{\InformationMatrix}{{\boldsymbol J}}
\newcommand{\lin}{\mathrm{lin}}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\newcommand{\Exp}{\mathrm{Exp}}

\newcommand{\RandomField}{Y}
\newcommand{\bLambda}{{\boldsymbol \Lambda}}
\newcommand{\bGamma}{{\boldsymbol \Gamma}}
\newcommand{\Err}{\mathrm{Err}}
\newcommand{\bQ}{{\boldsymbol Q}}
\newcommand{\bJ}{{\boldsymbol J}}
\newcommand{\bV}{{\boldsymbol V}}
\newcommand{\bI}{{\boldsymbol I}}
\newcommand{\bC}{{\boldsymbol C}}
\newcommand{\convweak}{\overset{d}{\to}}

\newcommand{\appropto}{\mathrel{\vcenter{
			\offinterlineskip\halign{\hfil$##$\cr
				\propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}

\renewcommand{\thealgorithm}{\arabic{algorithm}}

\title{ {\bf Inference for Local Maxima of a Gaussian Random Field} \\ Coordinate free intensity}

\begin{document}
	
	\maketitle
	\RaggedRight
	
	Consider a generic signal-plus-noise random field
	\begin{equation}
		\label{eqn:signal-plus-noise}
		Y_t = \mu_t + \epsilon_t, \quad {\rm for} ~ t \in \mc{T}.
	\end{equation}

        We're going to consider $\mc{T}$ an abstract manifold for the moment, meaning that any {\em local calculations}
        must be done in a chart $\varphi: U \rightarrow \R^d$ with $U \subset \mc{T}$.
        It is also a Riemannian manifold with metric: given two vector fields $X, W$:
\begin{equation}
        g(X,W)_t = \E[X_t\epsilon * W_t\epsilon].
\end{equation}

For open $A \subset \mc{T}$ and $B \subset \R$ we are interested in the random variable
\begin{equation}
  \# \left\{(t,y): t \in A, dY_t = 0, Y_t \in B, \text{$t$ is a local maximum of $Y$}  \right\}.
  \end{equation}
Note that this random variable is independent of a chart: it is coordinate-free. The extra condition
about the local maximum may seem coordinate based as it would seem to depend on the Hessian
in that coordinate. However, the property of being a local maximum of a $C^2$ function is independent of any chart.

Hence, the expected measure  is also coordinate-free.
$$
(A, B) \mapsto \nu(A \times B) \overset{def}{=} \E \left[\# \left\{(t,y): t \in A, dY_t = 0, Y_t \in B, \text{$t$ is a local maximum of $Y$}  \right\}\right] 
$$
The Kac-Rice formula gives a formula for the intensity of this measure in any coordinate system. Under enough assumptions,
$\nu$ will have a Radon-Nikodym density with respect to the product measure on $\mc{T} \times \R$ equipped with the natural product metric.
This Radon-Nikodym density will of course also be coordinate-free. Let's compute it.

The computation relies first on the proper definition of the Hessian. For any $f \in C^2(\mc{T})$ and any smooth vector fields $X, W$, the
Hessian $\nabla^2 f(X, W): \mc{T} \rightarrow \R$ can be defined through 
\begin{equation}
\begin{aligned}
\nabla^2 f(X, W)_t = 
X_t(Wf) - g(\nabla_XW, \nabla f) = X_t(Wf) -  \sum_{i,j} \Cov(X_t(W\epsilon), V_{i,t}\epsilon) g^{ij}_{V,t} V_{j,t}f
\end{aligned}
  \end{equation}
with $Wf_t = W_tf$ and $V=(V_1, \dots, V_d)$ any set of frames with $g_{ij,V}(t) = g(V_i, V_j)_t$ and $g^{ij}_V =(g_{V})^{-1}$. We could
take the elements of $V$ to be coordinate vector fields in some chart if we wanted a local formula for the Hessian.
The Hessian is symmetric (expected) and it behaves tensorially (formally it's a module over smooth functions on $\mc{T}$). Let $M_t: T_t\mc{T} \rightarrow T_t\mc{T}$ be a section of linear maps on $T(\mc{T})$, i.e.
a set change of basis matrices, one for each $t$. Then
\begin{equation}
\nabla^2 (a \cdot X + b \cdot \bar{X},W) = a \cdot \nabla^2 (X, W) + b \cdot \nabla^2 (\bar{X}, W).
  \end{equation}
In fact, this even tells us that the vector fields $X$ and $W$ don't really need to be smooth!

As in Theorem 12.4.1 of ``Random fields and geometry'', let's now fix a set of orthonormal frames $(E_1, \dots, E_d)$. The measure $\nu$
can be written as
\begin{equation}
\begin{aligned}
\nu(A \times B) &= \int_A \E \left[\det(-\nabla^2 Y(E_i,E_j)_t) 1_{{\cal N}}(\nabla^2 Y(E_i,E_j)_t) 1_B(Y_t) \biggl \vert \nabla Y^E_t=0\right] \phi_{\nabla Y^E_t}(0) \text{Vol}_g(dt) \\
 &= \int_A \int_B \underbrace{ \E \left[\det(-\nabla^2 Y(E_i,E_j)_t) 1_{{\cal N}}(\nabla^2 Y(E_i,E_j)_t) \biggl \vert \nabla Y^E_t=0, Y_t=y\right] \phi_{Y_t|\nabla Y^E_t=0}(y) \; \phi_{\nabla Y^E_t}(0)}_{\rho(t,y)} \; dy \; \text{Vol}_g(dt) \\
\end{aligned}
  \end{equation}
with $\text{Vol}_g$ the Riemannian measure. Above, $\nabla Y^E_t = (E_{1,t}Y, \dots, E_{d,t}Y)$ is the gradient of $Y$ at $t$ read off in the frame $E$, with
$\nabla^2 Y(E_i,E_j)_t$ defined similarly. This expression $\rho(t,y)$ is coordinate-free: it does not even depend on our choice of orthonormal frames.

How does this relate to what we would get if we worked in a coordinate chart $\varphi(A)$ and worked with the coordinate $\partial t_i$ vector fields? In local coordinates $t=\varphi(u)$ for some chart
$\varphi:U \to \R^d$, we will
typically write
\begin{equation}
\Lambda_{ij,t} = g\left(\frac{\partial}{\partial t_i}, \frac{\partial}{\partial t_j}\right)_t.
  \end{equation}
Let's pick a square root $\Lambda_t^{1/2}$ such that $\Lambda_t^{-1/2} \Lambda_t (\Lambda_t^{-1/2})'=I_{d \times d}$. Then, we can find a specific set of orthonormal vector fields by
$$
E_{i,t} = \sum_j \Lambda^{-1/2}_{ij,t} \frac{\partial}{\partial t_j} \biggl|_t.
$$
With this frame, we see\footnote{A small note here: generally speaking
\begin{equation}
\nabla^2 Y(\partial t_i, \partial t_j)_t \neq \frac{\partial^2 Y}{\partial t_i \partial t_j}\biggl|_t
  \end{equation}
as the RHS is missing the part related to the connection, which makes the RHS non-tensorial. This relation is true
when $t$ is a critical point of $Y$. Therefore under the conditioning
$\nabla Y^E_t=0$ the conditional expectation of LHS is the same as the RHS.}
$$
\nabla^2 Y(E_i,E_j)_t = \Lambda^{-1/2}_t \nabla^2 Y(\partial/\partial t_i, \partial/\partial t_j)_t (\Lambda^{-1/2}_t)'
$$

Further, setting
$$
\partial Y_t = \left(\frac{\partial Y}{\partial t_1}\biggl|_t, \dots, \frac{\partial Y}{\partial t_d}\biggl|_t \right)
$$
we see that
$$
\phi_{\nabla Y_t^E}(0) = \det(\Lambda_t^{1/2}) \phi_{\partial Y_t}(0).
$$

Putting these these facts together, we see that the following expression is coordinate-free (and even valid with non-constant variance) and computable in any coordinate system we like:
$$
\begin{aligned}
\rho(t,y) = \det(\Lambda_t)^{-1/2} \E \left[\det\left(-\frac{\partial^2 Y}{\partial t_i \partial t_j} \biggl|_t\right) 1_{{\cal N}}\left(-\frac{\partial^2 Y}{\partial t_i \partial t_j} \biggl|_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] \phi_{Y_t|\partial Y_t=0}(y) \; \phi_{\partial Y_t}(0).
\end{aligned}
$$

Alternatively, we could write
$$
\begin{aligned}
\rho(t,y) = (2\pi)^{-d/2} \det(\Lambda_t)^{-1/2} \E \left[\det\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t\right) 1_{{\cal N}}\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] \phi_{Y_t|\partial Y_t=0}(y) \; e^{-\frac{1}{2}g(\nabla \mu_t,\nabla \mu_t)}
\end{aligned}
$$

In the constant variance case (with variance 1), this simplifies to
$$
\begin{aligned}
\rho(t,y) = (2\pi)^{-(d+1)/2} \det(\Lambda_t)^{-1/2} \E \left[\det\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t\right) 1_{{\cal N}}\left(-\nabla^2 Y(\partial t_i, \partial t_j)_t \right) \biggl \vert \partial Y_t=0, Y_t=y\right] e^{-\frac{1}{2}(y-\mu_t)^2} \; e^{-\frac{1}{2}g(\nabla \mu_t,\nabla \mu_t)}
\end{aligned}
$$

\section{Taylor series on a Riemannian manifold}

%\\\\\\\ protect merge

We will want to expand $\rho(y,t)$ in some form of Taylor
expansion. On a manifold, using a Taylor expansion in a given
coordinate system can obscure things in different coordinate
systems. On a Riemannian manifold, it is simplest to derive a Taylor expansion
at $t$ in some small ball $B(t^*,r)$ by considering the geodesic connecting them.

Define $\log_{t^*}(t)$ to be the inverse of the exponential map's restriction to $T_{t^*}\mc{T}$:
$$
\exp_{\mc{T}}(t^*, \log_{t^*}(t)) = t
$$
and let $\gamma = \gamma_t:[0, \|\log_{t^*}(t)\|]$ be the geodesic connecting $t^*$ to $t$. The map $\log_{t^*}(t)$ will be well defined
on small balls around $t^*$.

For a smooth function $f:\mc{T} \rightarrow \R$ consider the map $\bar{f} = f \circ \gamma: [0, \|\log_{t^*}(t)\|] \rightarrow \R$.
The value $\bar{f}$ can be expressed using the usual one-dimensional Taylor's theorem. Up to order $k$ with a Taylor remainder it would be
$$
 \left|\bar{f}(r) - \sum_{j=0}^k \frac{r^j}{j!} \frac{d^j\bar{f}}{dr^j}(0) \right| \leq  \frac{r^{k+1}}{(k+1)!}\frac{d^{k+1}f}{dr^{k+1}} (\bar{r}(r))
$$
where $\bar{r} \in [0, r]$. For a H\"older type remainder for $f \in C^{k+\alpha}$ we would have
$$
 \left|\bar{f}(r) - \sum_{j=0}^k \frac{r^j}{j!} \frac{d^j\bar{f}}{dr^j}(0) \right| \leq  C_{\alpha}(\gamma) r^{\alpha}.
$$

 The key to extend this argument based on a geodesic to a more familiar form of Taylor series (and a natural one for Riemannian manifolds)
 is to note that along a geodesic, the derivatives implicated above will be expressible in terms of the $k$-th order covariant
 derivatives of $f$.

\subsection{Covariant differentiation}

The covariant derivatives of a function $f$ are tensor fields on $\mc{T}$, which means they behave nicely
as functions of $t$.\footnote{Specifically multilinearly as a module over smooth functions on $\mc{T}$. This,
in turn means they are easy to relate between different coordinate systems at any $t$.}
Let $X_i$ denote vector fields, however many as needed, with $\nabla_XY$ denoting
the covariant derivative of the vector field $Y$ with respect to $X$. The covariant
derivatives of a function are defined recursively:
$$
\begin{aligned}
  \nabla^1 f(X_1)_t &= g(\nabla f, X_1)_t \\
  &= X_{1,t}f \\
  \nabla^2 f(X_2, X_1)_t &= X_{2,t}(\nabla^1 f(X_1)) - \nabla^1 f(\nabla_{X_2}X_1)_t \\
  \nabla^k f(X_k, X_{k-1}, \dots, X_1)_t &= X_{k,t}(\nabla^{k-1} f(X_{k-1}, \dots, X_1)) -
  \nabla^{k-1} f(\nabla_{X_k}X_{k-1}, X_{k-2}, \dots, X_1)_t - \\
&  \qquad   \nabla^{k-1} f(X_{k-1}, \nabla_{X_k}X_{k-2}, \dots, X_1)_t - \\
  & \qquad \nabla^{k-1} f(X_{k-1}, \dots, \nabla_{X_k}X_1)_t.
  \end{aligned}
$$

In terms of symmetry, we note that beyond second order, covariant derivatives of a function are not symmetric. We can however project these derivatives
(or any tensor) 
onto the symmetric tensors. We denote this projection with a superscript
$$
A^{\mathrm{Sym}}(X_1, \dots, X_k) = \frac{1}{k!} \sum_{\sigma \in S_k} A(X_{\sigma(1)}, \dots, X_{\sigma(k)}).
$$

\begin{lemma}
\label{lem:geodesic}
  For $r \in [0, \|\log_{t^*}(t)\|]$ the following holds:
  $$
\frac{d^j\bar{f}}{dr^j}(r) = \nabla^j f_{\gamma(r)}(\dot{\gamma}(r), \dots, \dot{\gamma}(r)) =  \nabla^j f_{\gamma(r)}^{\mathrm{Sym}}(\dot{\gamma}(r), \dots, \dot{\gamma}(r))
  $$
  \end{lemma}
The proof of this is by induction starting at $j=2$ combined with the fact that $(\nabla_{\dot{\gamma}}\dot{\gamma})_{\gamma(r)}=0$ for $r \in [0, \|\log_{t^*}(t)\|]$.

We can now state a coordinate-free version of Taylor's theorem.
\begin{theorem}[Taylor's theorem]
\label{thm:taylor}
  Suppose $f \in C^{k+\alpha}(\mc{T})$ for $\alpha \in (0, 1]$ then, for $t \in B_{t^*}(r)$ with $r$ sufficiently small such that $\log_{t^*}$ is well-defined
    over $B_{t^*}(r)$. Then,
  \begin{equation}
    \label{eq:taylor:theorem}
\begin{aligned}
  \lefteqn{    \left|f(t) - \sum_{j=0}^k \frac{1}{j!} \nabla^j f^{\mathrm{Sym}}_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)) \right| } \\
  & \qquad =     \left|f(t) - \sum_{j=0}^k \frac{1}{j!} \nabla^j f_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)) \right| \\
  & \qquad \leq C(f,r) \|\log_{t^*}(t)\|^{k+\alpha}.
\end{aligned}
    \end{equation}
The constant $C(f, r)$ can be taken to be (up to universal constants)
\begin{equation}
  C(f,r) = \begin{cases} \frac{1}{(k+1)!}\sup_{t \in B(t^*,r)} \|\nabla^{k+1} f^{\mathrm{Sym}}_t\| & \alpha=1 \\
    \inf_{E_{t^*}}  C_{\alpha,E_{t^*}}(\bar{f}^{E_{t^*}}, r) & \alpha \in (0, 1)
\end{cases}
\end{equation}
  with $E_{t^*}$ denoting a choice of orthonormal basis for $T_{t^*}\mc{T}$ with corresponding
  normal coordinates $\varphi^{E_{t^*}}:\mc{T} \rightarrow \R^d$ at $t^*$. The quantity $C_{\alpha,E_{t^*}}(\bar{f}^{E_{t^*}}, r)$ is a coordinate-based H\"older constant.
  That is, if we define $\bar{f}^{E_{t^*}} = f\circ (\varphi^{E_{t^*}})^{-1}: \R^d \rightarrow \R$, then for each choice $E_{t^*}$ we can pick a H\"older constant:
  $$
\left| \bar{f}^{E_{t^*}}(h) - \sum_{j=0}^k \frac{1}{j!} \nabla^j \bar{f}^{E_{t^*}}_0(h, \dots, h) \right| \leq \|h\|^{\alpha} C_{\alpha, E_{t^*}}(\bar{f}^{E_{t^*}}, r) \ \forall \; h \in \varphi^{E_{t^*}}(B(t^*,r)).
  $$
  \end{theorem}

\subsection{Taylor's theorem for tensor fields}

Covariant derivatives of a tensor field are similary defined. Suppose $A$ is a $k$-th order (covariant) tensor field
$$
A_t:\otimes^k T_t\mc{T} \to \R
$$
then 
$$
\begin{aligned}
  \nabla^1 A(X_{k+1}, X_k, \dots, X_1)_t &= X_{k+1,t}(A(X_k, \dots, X_1)) - A(\nabla_{X_{k+1}}X_k, \dots, X_1)  \\
  &  \qquad  A(X_{k}, \nabla_{X_{k+1}}X_{k-1}, \dots, X_1)_t - \\
  & \qquad A(X_{k}, \dots, \nabla_{X_{k+1}}X_1)_t.
\end{aligned}
$$
Viewing $\nabla^{k} f$ as a tensor field, we see that $\nabla ( \nabla^k f)(X)$, to be read as the covariant derivative of the tensor $\nabla^k f$ in the direction
$X$, would be a tensor field of order $k$ satisfying
$$
\nabla(\nabla^k f)(Y)(X_1, \dots, X_k) = (\nabla^{k+1} f)(Y, X_1, \dots, X_k).
$$
In other words $\nabla (\nabla^k f) = \nabla^{k+1} f$.

We conclude by writing out a corresponding version of Taylor's theorem for tensor fields. For functions, we simply applied
the usual Taylor theorem on a curve. Repeated differentiation of a function is the same
as repeated application of the connection on functions. That is,
$$
\frac{d^k \bar{f}}{dr^k}\biggl|_r = \bar{\nabla}_{\dot{\gamma}} \left(\bar{\nabla}_{\dot{\gamma}}\left(\dots \left(\bar{\nabla}_{\dot{\gamma}}f\right) \right)\right)
$$
where $(\bar{\nabla}_{X}f)$ is  the function $t \mapsto \nabla f(X)_t$. We will write repeated applications of the connection\footnote{We will not use this notion
of repeated application of the connection elsewhere. It is used here to differentiate it from covariant differentiation.}as
$$
\bar{\nabla}^k_{(X_k, \dots, X_1)} f = \bar{\nabla}_{X_k}\left(\bar{\nabla}_{X_{k-q}} \left( \dots \left(\bar{\nabla}_{X_1}f\right)\right)\right).
$$
Importantly, the above expression is not tensorial in the $X_i$'s.

This repeated application of the connection is the most
natural way to differentiate a tensor of the form
$$
\sum_{s \in S} A_s \otimes_{j \in s} U_j
$$
for some subsets $s$ of fixed size, say $k$ for some set of vector fields ${\cal U} = \{U_1, \dots\}$. Indeed, this is how one would proceed in the Euclidean case. In the Euclidean case, of course one can express the tensor in some fixed basis, which is
not possible in general for a Riemmanian manifold.

Let's consider the simplest case, a 2-tensor of the form
$$
A = \sum_{i,j} a_{ij} U_i \otimes U_j.
$$
Now, consider directions we might want to differentiate this tensor. Even in the Euclidean case, the set of vector fields ${\cal U}$ may not be the standard
coordinate vector fields.

In the Euclidean case, we would likely look to differentiate $a_{ij}$ as well as $U_i$ and $U_j$ in the standard coordinate vector fields.
This differentiation corresponds exactly to repeated applications of the (Euclidean) connection to the terms $a_{ij}, U_i, U_j$.
After having
differentiated sufficiently we would expect to have some form of Taylor's theorem to evaluate the remainder.

Hence, for a tensor $A$, let's consider, for $r \in [0, \|\log_{t^*}(t)\|]$ the term
$$
A(r) - \sum_{j=0}^k \bar{\nabla}^k_{(\dot{\gamma}(r), \dots, \dot{\gamma}(r))}A(r).
$$
The following is a straightforward generalization of Lemma \cite{lem:geodesic}.
\begin{lemma}
$$
\bar{\nabla}^k_{(\dot{\gamma}(r), \dots, \dot{\gamma}(r))}A= \nabla^k A_{\gamma(r)}\left(\dot{\gamma}(r), \dots, \dot{\gamma}(r)\right).
$$
Or, repeated applications of the connection to a tensor along the geodesic computes covariant derivatives of the tensor. 
\end{lemma}
The proof is again inductive, starting with the identity $\nabla^2 A(X, Y) = (\nabla_X \nabla_Y - \nabla_{\nabla_XY})A$ and applying the geodesic condition.

In order to state our version of Taylor's thereom, it is useful to define a partial symmetrization of a $k$-tensor. For $q \leq k$, define the first $q$ symmetrization
$$
A^{\mathrm{Sym,q}}(X_k,\dots, X_1) = \sum_{\sigma \in S(q)} A(X_{k+1-\sigma(1)}, \dots, A_{k+1-\sigma(q)}, X_{k-q}, \dots, X_1).
$$

\begin{theorem}[Taylor's theorem for tensors]
\label{thm:taylor:tensor}
  Suppose $A \in C^{k+\alpha}(\mc{T})$ is a tensor for $\alpha \in (0, 1]$ then, for $t \in B_{t^*}(r)$ with $r$ sufficiently small such that $\log_{t^*}$ is well-defined
    over $B_{t^*}(r)$. Then,
  \begin{equation}
    \label{eq:taylor:theorem}
\begin{aligned}
  \lefteqn{    \left|A(t) - \sum_{j=0}^k \frac{1}{j!} \nabla^j A^{\mathrm{Sym},j}_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)) \right| } \\
  & \qquad =     \left|A(t) - \sum_{j=0}^k \frac{1}{j!} \nabla^j A_{t^*}(\log_{t^*}(t), \dots, \log_{t^*}(t)) \right| \\
  & \qquad \leq C(f,r) \|\log_{t^*}(t)\|^{k+\alpha}.
\end{aligned}
    \end{equation}
The constant $C(f, r)$ is similarly defined to Theorem \ref{thm:taylor}.
  \end{theorem}

%\\\\\\\\\\\\\\\

\section{Coordinate-free versions of (10),(11),(14),(15)}

In a given coordinate system, $H_t(v)$ is a (coordinate-dependent) matrix but it can similarly
be thought of as a tensor field. With variance constant and equal to 1, it is
$$
\begin{aligned}
  H(v)(X, W)_t &= \E[-\nabla^2 Y(X, W)_t | \partial Y_t=0, Y_t=v] \\
  &= \E[\nabla^2 Y(X, W)_t | \partial Y_t=0] \\
  &=- \nabla^2 \mu(X, W)_t + (v-\mu_t)g(X, W)_t
\end{aligned}
$$

In the general case, it is given by (correcting the error of the display between (12.2.11) and (12.2.12) in RFG along with non-zero mean):
$$
\begin{aligned}
  H(v)(X, W)_t &= \E[-\nabla^2 Y(X, W)_t | \partial Y_t=0, Y_t=v] \\
  &= -\nabla^2 \mu(X, W)_t - \frac{(v-\bar{\mu}_t)}{\text{\Var}(\epsilon_t|\partial \epsilon_t)} \Cov(\nabla^2 \epsilon(X, W)_t, \epsilon_t).
\end{aligned}
$$
with
$$
\bar{\mu}_t = \E[Y_t|\partial Y_t=0] = \mu_t - \Cov(\epsilon_t,\partial \epsilon_t) \Cov(\partial \epsilon_t)^{-1} \partial \mu_t.
$$
In the general case, note that $\nabla^2 \epsilon(X, W)_t$ is independent of $\partial \epsilon_t$ hence
$$
\Cov(\nabla^2 \epsilon(X, W)_t, \epsilon_t | \partial \epsilon_t) = \Cov(\nabla^2 \epsilon(X, W)_t, \epsilon_t).
$$

In either case, we can define
$$
\delta(t,v) = \inf_{X_t: g_t(X_t,X_t)=1} H(v)_t(X_t,X_t)
$$
as a coordinate-free measure of curvature.

{\bf Let's stick with the variance constant and equal to 1 moving forward.}

Lemma 1 is essentially unchanged though relevant bounds could be made coordinate free if desired.
The $D_k$ terms actually can be computed in terms of the Riemannian curvature tensor using
(12.2.13) and Lemma 12.3.1 of RFG.

\end{document}
